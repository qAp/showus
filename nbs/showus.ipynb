{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unlimited-assault",
   "metadata": {
    "papermill": {
     "duration": 0.035672,
     "end_time": "2021-05-17T00:33:34.356908",
     "exception": false,
     "start_time": "2021-05-17T00:33:34.321236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acoustic-checklist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:33:34.426792Z",
     "iopub.status.busy": "2021-05-17T00:33:34.425678Z",
     "iopub.status.idle": "2021-05-17T00:33:34.428936Z",
     "shell.execute_reply": "2021-05-17T00:33:34.429473Z"
    },
    "papermill": {
     "duration": 0.039722,
     "end_time": "2021-05-17T00:33:34.429714",
     "exception": false,
     "start_time": "2021-05-17T00:33:34.389992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#default_exp showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-greece",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-05-17T00:33:34.500687Z",
     "iopub.status.busy": "2021-05-17T00:33:34.500085Z",
     "iopub.status.idle": "2021-05-17T00:34:13.587780Z",
     "shell.execute_reply": "2021-05-17T00:34:13.587238Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 39.122843,
     "end_time": "2021-05-17T00:34:13.587979",
     "exception": false,
     "start_time": "2021-05-17T00:33:34.465136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\r\n",
      "Installing collected packages: fsspec\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 0.8.7\r\n",
      "    Uninstalling fsspec-0.8.7:\r\n",
      "      Successfully uninstalled fsspec-0.8.7\r\n",
      "Successfully installed fsspec-2021.4.0\r\n",
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.4.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.59.0\r\n",
      "    Uninstalling tqdm-4.59.0:\r\n",
      "      Successfully uninstalled tqdm-4.59.0\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Installing collected packages: tokenizers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.10.2\r\n",
      "    Uninstalling tokenizers-0.10.2:\r\n",
      "      Successfully uninstalled tokenizers-0.10.2\r\n",
      "Successfully installed tokenizers-0.10.1\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.45)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2021.3.17)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.5.1\r\n",
      "    Uninstalling transformers-4.5.1:\r\n",
      "      Successfully uninstalled transformers-4.5.1\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\n",
    "! pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n",
    "! pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n",
    "! pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n",
    "! pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "endangered-disaster",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:13.680620Z",
     "iopub.status.busy": "2021-05-17T00:34:13.679945Z",
     "iopub.status.idle": "2021-05-17T00:34:14.787987Z",
     "shell.execute_reply": "2021-05-17T00:34:14.787353Z"
    },
    "papermill": {
     "duration": 1.156729,
     "end_time": "2021-05-17T00:34:14.788138",
     "exception": false,
     "start_time": "2021-05-17T00:34:13.631409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers, seqeval\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loving-local",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:14.884065Z",
     "iopub.status.busy": "2021-05-17T00:34:14.883327Z",
     "iopub.status.idle": "2021-05-17T00:34:15.608760Z",
     "shell.execute_reply": "2021-05-17T00:34:15.607993Z"
    },
    "papermill": {
     "duration": 0.777516,
     "end_time": "2021-05-17T00:34:15.608939",
     "exception": false,
     "start_time": "2021-05-17T00:34:14.831423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/coleridge-packages/my_seqeval.py ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-passage",
   "metadata": {
    "papermill": {
     "duration": 0.042391,
     "end_time": "2021-05-17T00:34:15.694430",
     "exception": false,
     "start_time": "2021-05-17T00:34:15.652039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "third-exploration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:15.785452Z",
     "iopub.status.busy": "2021-05-17T00:34:15.784751Z",
     "iopub.status.idle": "2021-05-17T00:34:15.787481Z",
     "shell.execute_reply": "2021-05-17T00:34:15.787928Z"
    },
    "papermill": {
     "duration": 0.050698,
     "end_time": "2021-05-17T00:34:15.788098",
     "exception": false,
     "start_time": "2021-05-17T00:34:15.737400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "Path.ls = lambda pth: list(pth.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-atlas",
   "metadata": {
    "papermill": {
     "duration": 0.043065,
     "end_time": "2021-05-17T00:34:15.874187",
     "exception": false,
     "start_time": "2021-05-17T00:34:15.831122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "overhead-verse",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:15.967121Z",
     "iopub.status.busy": "2021-05-17T00:34:15.966466Z",
     "iopub.status.idle": "2021-05-17T00:34:15.969615Z",
     "shell.execute_reply": "2021-05-17T00:34:15.968982Z"
    },
    "papermill": {
     "duration": 0.05174,
     "end_time": "2021-05-17T00:34:15.969749",
     "exception": false,
     "start_time": "2021-05-17T00:34:15.918009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_train_meta(pth, group_id=True):\n",
    "    df = pd.read_csv(pth)\n",
    "    if group_id:\n",
    "        df = df.groupby('Id').agg({'pub_title': 'first', 'dataset_title': '|'.join, \n",
    "                                   'dataset_label': '|'.join, 'cleaned_label': '|'.join}).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "confirmed-somewhere",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:16.064781Z",
     "iopub.status.busy": "2021-05-17T00:34:16.064094Z",
     "iopub.status.idle": "2021-05-17T00:34:16.964527Z",
     "shell.execute_reply": "2021-05-17T00:34:16.965291Z"
    },
    "papermill": {
     "duration": 0.952196,
     "end_time": "2021-05-17T00:34:16.965584",
     "exception": false,
     "start_time": "2021-05-17T00:34:16.013388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14316 19661\n",
      "['Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " 'Beginning Postsecondary Students Longitudinal Study|Education Longitudinal Study|Beginning Postsecondary Students'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Beginning Postsecondary Student|Beginning Postsecondary Students']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "df = load_train_meta(pth, group_id=True)\n",
    "df_nogroup = load_train_meta(pth, group_id=False)\n",
    "print(len(df), len(df_nogroup))\n",
    "dup_ids = df_nogroup[df_nogroup.Id.duplicated()].Id.unique()\n",
    "print(df[df.Id.isin(dup_ids)].dataset_label.values[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dramatic-bradford",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:17.061274Z",
     "iopub.status.busy": "2021-05-17T00:34:17.060279Z",
     "iopub.status.idle": "2021-05-17T00:34:17.062706Z",
     "shell.execute_reply": "2021-05-17T00:34:17.063189Z"
    },
    "papermill": {
     "duration": 0.052804,
     "end_time": "2021-05-17T00:34:17.063368",
     "exception": false,
     "start_time": "2021-05-17T00:34:17.010564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_papers(dir_json, paper_ids):\n",
    "    '''\n",
    "    Load papers into a dictionary.\n",
    "    \n",
    "    `papers`: \n",
    "        {''}\n",
    "    '''\n",
    "    \n",
    "    papers = {}\n",
    "    for paper_id in paper_ids:\n",
    "        with open(f'{dir_json}/{paper_id}.json', 'r') as f:\n",
    "            paper = json.load(f)\n",
    "            papers[paper_id] = paper\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "moving-function",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:17.158852Z",
     "iopub.status.busy": "2021-05-17T00:34:17.157683Z",
     "iopub.status.idle": "2021-05-17T00:34:17.280987Z",
     "shell.execute_reply": "2021-05-17T00:34:17.280309Z"
    },
    "papermill": {
     "duration": 0.172812,
     "end_time": "2021-05-17T00:34:17.281139",
     "exception": false,
     "start_time": "2021-05-17T00:34:17.108327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'section_title': 'Abstract', 'text': \"Abstract Past research has shown that locus of control plays an important role in a wide range of behaviors, such as academic achievement and positive social behaviors. However, little is known about whether locus of control plays the same role in minority adolescents' peer relationships. The current study examined ethnic differences in the associations between locus of control and peer relationships in early adolescence using samples from the Early Childhood Longitudinal Study (ECLS-K: 5,612 Caucasian, 1,562 Hispanic, 507 Asian, and 908 AfricanAmerican adolescents) and the National Education Longitudinal Study (NELS: 8,484 Caucasian, 1,604 Hispanic, and 860 Asian, and 1,228 African American adolescents).\\nGender was approximately evenly split in both samples. The results from the two datasets were highly consistent. Significant interactions between ethnicity and locus of control indicated that having a more internal locus of control was particularly important for Caucasian students' peer relationships (ECLS-K) and social status (NELS), but less so for Asian, Hispanic, and African American students. Our findings suggest that the role of locus of control in peer relationship is contingent upon culture.\"}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id.unique()[:10])\n",
    "print(type(papers))\n",
    "print(\n",
    "    papers[ random.choice(list(papers.keys())) ][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specific-dallas",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:17.376381Z",
     "iopub.status.busy": "2021-05-17T00:34:17.375663Z",
     "iopub.status.idle": "2021-05-17T00:34:17.379350Z",
     "shell.execute_reply": "2021-05-17T00:34:17.378690Z"
    },
    "papermill": {
     "duration": 0.052766,
     "end_time": "2021-05-17T00:34:17.379498",
     "exception": false,
     "start_time": "2021-05-17T00:34:17.326732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_sample_text(jpth):\n",
    "    sections = json.loads(jpth.read_text())\n",
    "    text = '\\n'.join(section['text'] for section in sections)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "contained-myanmar",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:17.474916Z",
     "iopub.status.busy": "2021-05-17T00:34:17.474262Z",
     "iopub.status.idle": "2021-05-17T00:34:17.826366Z",
     "shell.execute_reply": "2021-05-17T00:34:17.827067Z"
    },
    "papermill": {
     "duration": 0.402209,
     "end_time": "2021-05-17T00:34:17.827281",
     "exception": false,
     "start_time": "2021-05-17T00:34:17.425072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s. ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data. Creating a U.S. crosswalk to this system has been a goal of the National Center for Education Statistics and the Office of Research since the late 197,,s, when the National Institute of Education (the predecessor agency to the Office of Educational Research and Improvement) began exploring the idea. The design and implementation of a workable crosswalk, however, awaited the advent of changes to the Classification of Instructional Programs (CIP) system. The 1990 revision of the CIP system laid the foundation for a workable international crosswalk. Adoption of the National Education Goals set global consciousness and international educational comparisons firml\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "print(load_sample_text(jpths_trn[0])[:1_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-african",
   "metadata": {
    "papermill": {
     "duration": 0.045228,
     "end_time": "2021-05-17T00:34:17.917664",
     "exception": false,
     "start_time": "2021-05-17T00:34:17.872436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "authentic-reasoning",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:18.015938Z",
     "iopub.status.busy": "2021-05-17T00:34:18.014841Z",
     "iopub.status.idle": "2021-05-17T00:34:18.017306Z",
     "shell.execute_reply": "2021-05-17T00:34:18.017915Z"
    },
    "papermill": {
     "duration": 0.055421,
     "end_time": "2021-05-17T00:34:18.018091",
     "exception": false,
     "start_time": "2021-05-17T00:34:17.962670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_training_text(txt, lower=False, total_clean=False):\n",
    "    \"\"\"\n",
    "    similar to the default clean_text function but without lowercasing.\n",
    "    \"\"\"\n",
    "    txt = str(txt).lower() if lower else str(txt)\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt).strip()\n",
    "    if total_clean:\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "confused-psychology",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:18.113306Z",
     "iopub.status.busy": "2021-05-17T00:34:18.112366Z",
     "iopub.status.idle": "2021-05-17T00:34:18.119302Z",
     "shell.execute_reply": "2021-05-17T00:34:18.119810Z"
    },
    "papermill": {
     "duration": 0.056353,
     "end_time": "2021-05-17T00:34:18.120030",
     "exception": false,
     "start_time": "2021-05-17T00:34:18.063677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle This competition awards 90 000\n",
      "hopkld 7 11 002\n"
     ]
    }
   ],
   "source": [
    "print(clean_training_text('@kaggle This competition awards $90,000!!!!.'))\n",
    "print(clean_training_text('HoPKLd + 7 ! 11,002', total_clean=True, lower=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "verbal-heavy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:18.217657Z",
     "iopub.status.busy": "2021-05-17T00:34:18.217032Z",
     "iopub.status.idle": "2021-05-17T00:34:18.219020Z",
     "shell.execute_reply": "2021-05-17T00:34:18.219467Z"
    },
    "papermill": {
     "duration": 0.054358,
     "end_time": "2021-05-17T00:34:18.219634",
     "exception": false,
     "start_time": "2021-05-17T00:34:18.165276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def shorten_sentences(sentences, max_length=64, overlap=20):\n",
    "    '''\n",
    "    Args:\n",
    "        sentences (list): List of sentences.\n",
    "        max_length (int): Maximum number of words allowed for each sentence.\n",
    "        overlap (int): If a sentence exceeds `max_length`, we split it to multiple sentences with \n",
    "            this amount of overlapping.\n",
    "    '''\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > max_length:\n",
    "            for p in range(0, len(words), max_length - overlap):\n",
    "                short_sentences.append(' '.join(words[p:p+max_length]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "broke-abraham",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:18.314171Z",
     "iopub.status.busy": "2021-05-17T00:34:18.313321Z",
     "iopub.status.idle": "2021-05-17T00:34:18.477811Z",
     "shell.execute_reply": "2021-05-17T00:34:18.478559Z"
    },
    "papermill": {
     "duration": 0.213374,
     "end_time": "2021-05-17T00:34:18.478775",
     "exception": false,
     "start_time": "2021-05-17T00:34:18.265401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s', ' ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data']\n",
      "\n",
      "After: ['The International Standard Classification of Education, known by its acronym', 'its acronym ISCED, was developed by the United Nations Educational,', 'Nations Educational, Scientific, and Cultural Organization during the late 1960s', 'late 1960s and 1970s', 'ISCED was implemented in 1976 and is the recognized international', 'recognized international standard for reporting and interpreting education program data', 'program data']\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "sentences = load_sample_text(jpths_trn[0]).split('.')[:2]\n",
    "short_sentences = shorten_sentences(sentences, max_length=10, overlap=2)\n",
    "print('Before:', sentences)\n",
    "print()\n",
    "print('After:', short_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "differential-credits",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:18.578008Z",
     "iopub.status.busy": "2021-05-17T00:34:18.577353Z",
     "iopub.status.idle": "2021-05-17T00:34:18.582084Z",
     "shell.execute_reply": "2021-05-17T00:34:18.582566Z"
    },
    "papermill": {
     "duration": 0.055454,
     "end_time": "2021-05-17T00:34:18.582722",
     "exception": false,
     "start_time": "2021-05-17T00:34:18.527268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def find_sublist(big_list, small_list):\n",
    "    all_positions = []\n",
    "    for i in range(len(big_list) - len(small_list) + 1):\n",
    "        if small_list == big_list[i:i+len(small_list)]:\n",
    "            all_positions.append(i)\n",
    "    \n",
    "    return all_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "isolated-tanzania",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:18.679272Z",
     "iopub.status.busy": "2021-05-17T00:34:18.678634Z",
     "iopub.status.idle": "2021-05-17T00:34:18.687213Z",
     "shell.execute_reply": "2021-05-17T00:34:18.687706Z"
    },
    "papermill": {
     "duration": 0.058133,
     "end_time": "2021-05-17T00:34:18.687891",
     "exception": false,
     "start_time": "2021-05-17T00:34:18.629758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list = ['If', 'the', 'thing', 'above', 'is', 'below', 'that', 'thing', 'which', 'is',\n",
    "            'not', 'as', 'high', 'up', 'on', 'the', 'thing', 'above', 'when', 'it', 'is', \n",
    "            'underneath', 'them.']\n",
    "small_list = ['the', 'thing', 'above']\n",
    "\n",
    "find_sublist(big_list, small_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "absolute-buffer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:18.784325Z",
     "iopub.status.busy": "2021-05-17T00:34:18.783689Z",
     "iopub.status.idle": "2021-05-17T00:34:18.790920Z",
     "shell.execute_reply": "2021-05-17T00:34:18.791528Z"
    },
    "papermill": {
     "duration": 0.057478,
     "end_time": "2021-05-17T00:34:18.791854",
     "exception": false,
     "start_time": "2021-05-17T00:34:18.734376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tag_sentence(sentence, labels): \n",
    "    '''\n",
    "    requirement: both sentence and labels are already cleaned\n",
    "    '''\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n",
    "                                  for label in labels): # positive sample\n",
    "        nes = ['O'] * len(sentence_words)\n",
    "        for label in labels:\n",
    "            label_words = label.split()\n",
    "\n",
    "            all_pos = find_sublist(sentence_words, label_words)\n",
    "            for pos in all_pos:\n",
    "                nes[pos] = 'B'\n",
    "                for i in range(pos+1, pos+len(label_words)):\n",
    "                    nes[i] = 'I'\n",
    "\n",
    "        return True, list(zip(sentence_words, nes))\n",
    "        \n",
    "    else: # negative sample\n",
    "        nes = ['O'] * len(sentence_words)\n",
    "        return False, list(zip(sentence_words, nes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "formal-administration",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:18.897133Z",
     "iopub.status.busy": "2021-05-17T00:34:18.896135Z",
     "iopub.status.idle": "2021-05-17T00:34:18.903014Z",
     "shell.execute_reply": "2021-05-17T00:34:18.903923Z"
    },
    "papermill": {
     "duration": 0.057689,
     "end_time": "2021-05-17T00:34:18.904237",
     "exception": false,
     "start_time": "2021-05-17T00:34:18.846548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A label is found in the sentence: True\n",
      "(token, tag) pairs:\n",
      "[('The', 'B'), ('International', 'I'), ('Standard', 'O'), ('Classification', 'O'), ('of', 'O'), ('Education', 'O'), ('known', 'O'), ('by', 'O'), ('its', 'O'), ('acronym', 'O'), ('ISCED', 'O'), ('was', 'O'), ('developed', 'O'), ('by', 'O'), ('the', 'O'), ('United', 'B'), ('Nations', 'I'), ('Educational', 'I'), ('Scientific', 'O'), ('and', 'O'), ('Cultural', 'B'), ('Organization', 'I'), ('during', 'O'), ('the', 'O'), ('late', 'O'), ('1960s', 'O'), ('and', 'O'), ('1970s', 'O')]\n"
     ]
    }
   ],
   "source": [
    "sentence = (\"The International Standard Classification of Education, known by its acronym ISCED, \"\n",
    "            \"was developed by the United Nations Educational, \"\n",
    "            \"Scientific, and Cultural Organization during the late 1960s and 1970s\")\n",
    "labels = ['The International', 'Cultural Organization', 'United Nations Educational']\n",
    "\n",
    "sentence = clean_training_text(sentence)\n",
    "labels = [clean_training_text(label) for label in labels]\n",
    "found_any, token_tags = tag_sentence(sentence, labels)\n",
    "\n",
    "print('A label is found in the sentence:', found_any)\n",
    "print('(token, tag) pairs:')\n",
    "print(token_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "moderate-forest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:19.024408Z",
     "iopub.status.busy": "2021-05-17T00:34:19.023611Z",
     "iopub.status.idle": "2021-05-17T00:34:19.038402Z",
     "shell.execute_reply": "2021-05-17T00:34:19.037740Z"
    },
    "papermill": {
     "duration": 0.07926,
     "end_time": "2021-05-17T00:34:19.038557",
     "exception": false,
     "start_time": "2021-05-17T00:34:18.959297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_data(papers, df=None, shuffle=True):\n",
    "    '''\n",
    "    Args:\n",
    "        papers (dict): Like that returned by `load_papers`.\n",
    "        df (pd.DataFrame): Competition's train.csv or a subset of it.\n",
    "    '''\n",
    "    cnt_pos, cnt_neg = 0, 0 \n",
    "    ner_data = []\n",
    "\n",
    "    tqdm._instances.clear()\n",
    "    pbar = tqdm(total=len(df))\n",
    "    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():\n",
    "        paper = papers[id]\n",
    "\n",
    "        labels = dataset_label.split('|')\n",
    "        labels = [clean_training_text(label) for label in labels]\n",
    "\n",
    "        sentences = set([clean_training_text(sentence) for section in paper \n",
    "                     for sentence in section['text'].split('.')])\n",
    "        sentences = shorten_sentences(sentences) \n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "\n",
    "        # positive sample\n",
    "        for sentence in sentences:\n",
    "            is_positive, tags = tag_sentence(sentence, labels)\n",
    "            if is_positive:\n",
    "                cnt_pos += 1\n",
    "                ner_data.append(tags)\n",
    "            elif any(word in sentence.lower() for word in ['data', 'study']): \n",
    "                ner_data.append(tags)\n",
    "                cnt_neg += 1\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n",
    "#         print(f\"\\rProcessing paper {i:05d} / {len(df)}. Training data size: {cnt_pos} positives + {cnt_neg} negatives\", \n",
    "#               flush=True, end='')\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(ner_data)\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "artistic-conflict",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:19.151271Z",
     "iopub.status.busy": "2021-05-17T00:34:19.150344Z",
     "iopub.status.idle": "2021-05-17T00:34:19.423226Z",
     "shell.execute_reply": "2021-05-17T00:34:19.422578Z"
    },
    "papermill": {
     "duration": 0.33517,
     "end_time": "2021-05-17T00:34:19.423372",
     "exception": false,
     "start_time": "2021-05-17T00:34:19.088202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 54 positives + 406 negatives: 100%|██████████| 20/20 [00:00<00:00, 132.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Study', 'O'), ('sample', 'O'), ('A', 'O'), ('nationally', 'O'), ('representative', 'O'), ('sample', 'O'), ('of', 'O'), ('eighth', 'O'), ('graders', 'O'), ('was', 'O'), ('first', 'O'), ('surveyed', 'O'), ('in', 'O'), ('the', 'O'), ('spring', 'O'), ('of', 'O'), ('1988', 'O')]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:20]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "ner_data = get_ner_data(papers, df, shuffle=False)\n",
    "print(ner_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "indonesian-arcade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:19.577274Z",
     "iopub.status.busy": "2021-05-17T00:34:19.576342Z",
     "iopub.status.idle": "2021-05-17T00:34:19.578260Z",
     "shell.execute_reply": "2021-05-17T00:34:19.579102Z"
    },
    "papermill": {
     "duration": 0.092833,
     "end_time": "2021-05-17T00:34:19.579325",
     "exception": false,
     "start_time": "2021-05-17T00:34:19.486492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def write_ner_json(ner_data, pth=Path('train_ner.json')):\n",
    "    with open(pth, 'w') as f:\n",
    "        for row in ner_data:\n",
    "            words, nes = list(zip(*row))\n",
    "            row_json = {'tokens' : words, 'tags' : nes}\n",
    "            json.dump(row_json, f)\n",
    "            f.write('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "rough-painting",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:19.698374Z",
     "iopub.status.busy": "2021-05-17T00:34:19.697449Z",
     "iopub.status.idle": "2021-05-17T00:34:20.433581Z",
     "shell.execute_reply": "2021-05-17T00:34:20.433022Z"
    },
    "papermill": {
     "duration": 0.79341,
     "end_time": "2021-05-17T00:34:20.433734",
     "exception": false,
     "start_time": "2021-05-17T00:34:19.640324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tokens\": [\"There\", \"is\", \"no\", \"dataset\", \"here\"], \"tags\": [\"O\", \"O\", \"O\", \"O\", \"O\"]}\r\n",
      "{\"tokens\": [\"Load\", \"the\", \"UN\", \"Trade\", \"Development\", \"into\", \"view\"], \"tags\": [\"O\", \"O\", \"B\", \"I\", \"I\", \"O\", \"O\"]}\r\n"
     ]
    }
   ],
   "source": [
    "ner_data = [\n",
    "    [('There', 'O'), ('is', 'O'), ('no', 'O'), ('dataset', 'O'), ('here', 'O')], \n",
    "    [('Load', 'O'), ('the', 'O'), ('UN', 'B'), ('Trade', 'I'), ('Development', 'I'), ('into', 'O'), ('view', 'O')]\n",
    "]\n",
    "write_ner_json(ner_data, pth=Path('/kaggle/tmp_ner.json'))\n",
    "! cat /kaggle/tmp_ner.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-coupon",
   "metadata": {
    "papermill": {
     "duration": 0.05481,
     "end_time": "2021-05-17T00:34:20.543497",
     "exception": false,
     "start_time": "2021-05-17T00:34:20.488687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "broad-trial",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:20.661755Z",
     "iopub.status.busy": "2021-05-17T00:34:20.661141Z",
     "iopub.status.idle": "2021-05-17T00:34:20.663134Z",
     "shell.execute_reply": "2021-05-17T00:34:20.663561Z"
    },
    "papermill": {
     "duration": 0.065138,
     "end_time": "2021-05-17T00:34:20.663727",
     "exception": false,
     "start_time": "2021-05-17T00:34:20.598589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def kaggle_run_ner(model_name_or_path='bert-base-cased', \n",
    "                   train_file='./train_ner.json', validation_file='./train_ner.json',\n",
    "                   num_train_epochs=1, per_device_train_batch_size=8, per_device_eval_batch_size=8,\n",
    "                   save_steps=15000, output_dir='./output', report_to='none', seed=123):\n",
    "    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --train_file {train_file} \\\n",
    "    --validation_file {validation_file} \\\n",
    "    --num_train_epochs {num_train_epochs} \\\n",
    "    --per_device_train_batch_size {per_device_train_batch_size} \\\n",
    "    --per_device_eval_batch_size {per_device_eval_batch_size} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --report_to {report_to} \\\n",
    "    --seed {seed} \\\n",
    "    --do_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "little-oasis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:34:20.776820Z",
     "iopub.status.busy": "2021-05-17T00:34:20.776232Z",
     "iopub.status.idle": "2021-05-17T00:35:22.564437Z",
     "shell.execute_reply": "2021-05-17T00:35:22.563415Z"
    },
    "papermill": {
     "duration": 61.845872,
     "end_time": "2021-05-17T00:35:22.564642",
     "exception": false,
     "start_time": "2021-05-17T00:34:20.718770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 5 positives + 26 negatives: 100%|██████████| 2/2 [00:00<00:00, 210.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-54249eba0b6572cf/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-54249eba0b6572cf/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 00:34:30,991 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7i4s06iq\r\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 437kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 00:34:31,265 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 00:34:31,265 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\r\n",
      "[INFO|configuration_utils.py:472] 2021-05-17 00:34:31,266 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\r\n",
      "[INFO|configuration_utils.py:508] 2021-05-17 00:34:31,267 >> Model config BertConfig {\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:472] 2021-05-17 00:34:31,540 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\r\n",
      "[INFO|configuration_utils.py:508] 2021-05-17 00:34:31,541 >> Model config BertConfig {\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 00:34:31,812 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8qiq1xty\r\n",
      "Downloading: 100%|████████████████████████████| 213k/213k [00:00<00:00, 843kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 00:34:32,341 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 00:34:32,341 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 00:34:32,614 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp27re7low\r\n",
      "Downloading: 100%|███████████████████████████| 436k/436k [00:00<00:00, 1.38MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 00:34:33,205 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 00:34:33,205 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 00:34:34,024 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpz4g1mgmh\r\n",
      "Downloading: 100%|███████████████████████████| 29.0/29.0 [00:00<00:00, 22.6kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 00:34:34,298 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 00:34:34,298 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 00:34:34,299 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 00:34:34,299 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 00:34:34,299 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 00:34:34,299 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 00:34:34,299 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 00:34:34,608 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpt9bcn76n\r\n",
      "Downloading: 100%|███████████████████████████| 436M/436M [00:21<00:00, 20.6MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 00:34:56,041 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 00:34:56,041 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\r\n",
      "[INFO|modeling_utils.py:1051] 2021-05-17 00:34:56,042 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\r\n",
      "[WARNING|modeling_utils.py:1159] 2021-05-17 00:35:00,269 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\r\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "[WARNING|modeling_utils.py:1170] 2021-05-17 00:35:00,269 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 57.52ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-05-17 00:35:00,497 >> The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags.\r\n",
      "[INFO|trainer.py:988] 2021-05-17 00:35:00,773 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:989] 2021-05-17 00:35:00,773 >>   Num examples = 31\r\n",
      "[INFO|trainer.py:990] 2021-05-17 00:35:00,773 >>   Num Epochs = 1\r\n",
      "[INFO|trainer.py:991] 2021-05-17 00:35:00,773 >>   Instantaneous batch size per device = 8\r\n",
      "[INFO|trainer.py:992] 2021-05-17 00:35:00,773 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\r\n",
      "[INFO|trainer.py:993] 2021-05-17 00:35:00,773 >>   Gradient Accumulation steps = 1\r\n",
      "[INFO|trainer.py:994] 2021-05-17 00:35:00,773 >>   Total optimization steps = 4\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:19<00:00,  5.03s/it][INFO|trainer.py:1171] 2021-05-17 00:35:20,730 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 19.9566, 'train_samples_per_second': 0.2, 'epoch': 1.0}\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:19<00:00,  4.99s/it]\r\n",
      "[INFO|trainer.py:1600] 2021-05-17 00:35:20,873 >> Saving model checkpoint to ./output\r\n",
      "[INFO|configuration_utils.py:318] 2021-05-17 00:35:20,874 >> Configuration saved in ./output/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-05-17 00:35:21,538 >> Model weights saved in ./output/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-05-17 00:35:21,538 >> tokenizer config file saved in ./output/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-05-17 00:35:21,539 >> Special tokens file saved in ./output/special_tokens_map.json\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-05-17 00:35:21,579 >> ***** train metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:35:21,579 >>   epoch                      =        1.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:35:21,579 >>   init_mem_cpu_alloc_delta   =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:35:21,579 >>   init_mem_cpu_peaked_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:35:21,579 >>   train_mem_cpu_alloc_delta  =     1760MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:35:21,579 >>   train_mem_cpu_peaked_delta =       84MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:35:21,579 >>   train_runtime              = 0:00:19.95\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:35:21,579 >>   train_samples              =         31\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:35:21,579 >>   train_samples_per_second   =        0.2\r\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:2]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', df.Id)\n",
    "ner_data = get_ner_data(papers, df)\n",
    "write_ner_json(ner_data, pth=Path('./train_ner.json'))\n",
    "kaggle_run_ner(save_steps=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-sugar",
   "metadata": {
    "papermill": {
     "duration": 0.119191,
     "end_time": "2021-05-17T00:35:22.808978",
     "exception": false,
     "start_time": "2021-05-17T00:35:22.689787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Literal matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "threatened-nothing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:35:23.048680Z",
     "iopub.status.busy": "2021-05-17T00:35:23.048008Z",
     "iopub.status.idle": "2021-05-17T00:35:23.050164Z",
     "shell.execute_reply": "2021-05-17T00:35:23.050630Z"
    },
    "papermill": {
     "duration": 0.12511,
     "end_time": "2021-05-17T00:35:23.050816",
     "exception": false,
     "start_time": "2021-05-17T00:35:22.925706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_knowledge_bank(pth):\n",
    "    '''\n",
    "    Args:\n",
    "        pth (str): Path to meta data like 'train.csv', which\n",
    "        needs to have columns: 'dataset_title', 'dataset_label', and 'cleaned_label'.\n",
    "        \n",
    "    Returns:\n",
    "        all_labels (set): All possible strings associated with a dataset from the meta data.\n",
    "    '''\n",
    "    df = load_train_meta(pth, group_id=False)\n",
    "    all_labels = set()\n",
    "    for label_1, label_2, label_3 in df[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n",
    "        all_labels.add(str(label_1).lower())\n",
    "        all_labels.add(str(label_2).lower())\n",
    "        all_labels.add(str(label_3).lower())\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "liquid-province",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:35:23.288586Z",
     "iopub.status.busy": "2021-05-17T00:35:23.287894Z",
     "iopub.status.idle": "2021-05-17T00:35:23.418800Z",
     "shell.execute_reply": "2021-05-17T00:35:23.419343Z"
    },
    "papermill": {
     "duration": 0.250491,
     "end_time": "2021-05-17T00:35:23.419522",
     "exception": false,
     "start_time": "2021-05-17T00:35:23.169031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "['2019 ncov complete genome sequences', '2019 ncov genome sequence', '2019 ncov genome sequences', '2019-ncov complete genome sequences', '2019-ncov genome sequence', '2019-ncov genome sequences', 'adni', 'advanced national seismic system (anss) comprehensive catalog (comcat)', 'advanced national seismic system anss comprehensive catalog comcat ', 'advanced national seismic system comprehensive catalog']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "print(len(all_labels))\n",
    "print(sorted(all_labels)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cooperative-blair",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:35:23.666287Z",
     "iopub.status.busy": "2021-05-17T00:35:23.665654Z",
     "iopub.status.idle": "2021-05-17T00:35:23.671307Z",
     "shell.execute_reply": "2021-05-17T00:35:23.671807Z"
    },
    "papermill": {
     "duration": 0.126627,
     "end_time": "2021-05-17T00:35:23.671999",
     "exception": false,
     "start_time": "2021-05-17T00:35:23.545372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def literal_match(paper, all_labels):\n",
    "    '''\n",
    "    Args:\n",
    "        paper ()\n",
    "    '''\n",
    "    text_1 = '. '.join(section['text'] for section in paper).lower()\n",
    "    text_2 = clean_training_text(text_1, lower=True, total_clean=True)\n",
    "    \n",
    "    labels = set()\n",
    "    for label in all_labels:\n",
    "        if label in text_1 or label in text_2:\n",
    "            labels.add(clean_training_text(label, lower=True, total_clean=True))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "organic-study",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:35:23.909017Z",
     "iopub.status.busy": "2021-05-17T00:35:23.908346Z",
     "iopub.status.idle": "2021-05-17T00:35:24.216723Z",
     "shell.execute_reply": "2021-05-17T00:35:24.217242Z"
    },
    "papermill": {
     "duration": 0.427963,
     "end_time": "2021-05-17T00:35:24.217412",
     "exception": false,
     "start_time": "2021-05-17T00:35:23.789449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alzheimer s disease neuroimaging initiative adni|adni',\n",
       " 'nces common core of data|trends in international mathematics and science study|common core of data',\n",
       " 'noaa storm surge inundation|sea lake and overland surges from hurricanes|slosh model',\n",
       " 'rural urban continuum codes']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n",
    "\n",
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "\n",
    "literal_preds = []\n",
    "for paper_id in sample_submission.Id:\n",
    "    paper = papers[paper_id]\n",
    "    literal_preds.append('|'.join(literal_match(paper, all_labels)))\n",
    "    \n",
    "literal_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-richardson",
   "metadata": {
    "papermill": {
     "duration": 0.116813,
     "end_time": "2021-05-17T00:35:24.451896",
     "exception": false,
     "start_time": "2021-05-17T00:35:24.335083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bert model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "genetic-yield",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:35:24.689794Z",
     "iopub.status.busy": "2021-05-17T00:35:24.689158Z",
     "iopub.status.idle": "2021-05-17T00:35:24.697723Z",
     "shell.execute_reply": "2021-05-17T00:35:24.698268Z"
    },
    "papermill": {
     "duration": 0.128383,
     "end_time": "2021-05-17T00:35:24.698448",
     "exception": false,
     "start_time": "2021-05-17T00:35:24.570065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_inference_data(papers, sample_submission):\n",
    "    '''\n",
    "    Args:\n",
    "        papers (dict): Each list in this dictionary consists of the section of a paper.\n",
    "        sample_submission (pd.DataFrame): Competition 'sample_submission.csv'.\n",
    "    Returns:\n",
    "        test_rows (list): Each dict in this list is of the form: \n",
    "            {'tokens': ['goat', 'win', ...], 'tags': ['O', 'O', ...]}\n",
    "            and represents a sentence.  \n",
    "        paper_length (list): Number of sentences in each paper.\n",
    "    '''\n",
    "    test_rows = [] # test data in NER format\n",
    "    paper_length = [] # store the number of sentences each paper has\n",
    "\n",
    "    for paper_id in sample_submission['Id']:\n",
    "        # load paper\n",
    "        paper = papers[paper_id]\n",
    "\n",
    "        # extract sentences\n",
    "        sentences = [clean_training_text(sentence) for section in paper \n",
    "                     for sentence in section['text'].split('.')\n",
    "                    ]\n",
    "        sentences = shorten_sentences(sentences) # make sentences short\n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "\n",
    "        # collect all sentences in json\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split()\n",
    "            dummy_tags = ['O']*len(sentence_words)\n",
    "            test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n",
    "\n",
    "        # track which sentence belongs to which data point\n",
    "        paper_length.append(len(sentences))\n",
    "\n",
    "    print(f'total number of sentences: {len(test_rows)}')\n",
    "    return test_rows, paper_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "clinical-charter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:35:24.937633Z",
     "iopub.status.busy": "2021-05-17T00:35:24.937024Z",
     "iopub.status.idle": "2021-05-17T00:35:25.025308Z",
     "shell.execute_reply": "2021-05-17T00:35:25.024582Z"
    },
    "papermill": {
     "duration": 0.209377,
     "end_time": "2021-05-17T00:35:25.025453",
     "exception": false,
     "start_time": "2021-05-17T00:35:24.816076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sentences: 367\n",
      "[{'tokens': ['A', 'recent', 'large', 'genomewide', 'association', 'study', 'GWAS', 'reported', 'a', 'genome', 'wide', 'significant', 'locus', 'for', 'years', 'of', 'education', 'which', 'subsequently', 'demonstrated', 'association', 'to', 'general', 'cognitive', 'ability', 'g', 'in', 'overlapping', 'cohorts'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'tokens': ['The', 'current', 'study', 'was', 'designed', 'to', 'test', 'whether', 'GWAS', 'hits', 'for', 'educational', 'attainment', 'are', 'involved', 'in', 'general', 'cognitive', 'ability', 'in', 'an', 'independent', 'large', 'scale', 'collection', 'of', 'cohorts'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'tokens': ['We', 'next', 'conducted', 'meta', 'analyses', 'with', '24', '189', 'individuals', 'with', 'neurocognitive', 'data', 'from', 'the', 'educational', 'attainment', 'studies', 'and', 'then', 'with', '53', '188', 'largely', 'independent', 'individuals', 'from', 'a', 'recent', 'GWAS', 'of', 'cognition'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
      "[34, 151, 98, 84]\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test', sample_submission.Id)\n",
    "test_rows, paper_length = get_ner_inference_data(papers, sample_submission)\n",
    "print(test_rows[:3])\n",
    "print(paper_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "annoying-dining",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:35:25.266152Z",
     "iopub.status.busy": "2021-05-17T00:35:25.265422Z",
     "iopub.status.idle": "2021-05-17T00:35:25.284039Z",
     "shell.execute_reply": "2021-05-17T00:35:25.283418Z"
    },
    "papermill": {
     "duration": 0.140674,
     "end_time": "2021-05-17T00:35:25.284184",
     "exception": false,
     "start_time": "2021-05-17T00:35:25.143510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def kaggle_run_ner_predict(model_name_or_path='/kaggle/input/coleridge-bert-models/output', \n",
    "                           train_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n",
    "                           validation_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n",
    "                           test_file='./input_data/test_ner_input.json', \n",
    "                           output_dir='./pred'):\n",
    "    '''\n",
    "    Args:\n",
    "        test_file (Path, str): Path to json file in which each row represents an input\n",
    "            sample to the model (representing a sentence in this context).  Each row\n",
    "            is a dictionary of the form:\n",
    "            {'tokens': ['hi', 'there', ...], 'tags': ['O', 'O', ...]}\n",
    "        output_dir (Path, str): Path to the directory in which prediction results are saved.\n",
    "    '''\n",
    "    os.environ[\"MODEL_PATH\"] = f\"{model_name_or_path}\"\n",
    "    os.environ[\"TRAIN_FILE\"] = f\"{train_file}\"\n",
    "    os.environ[\"VALIDATION_FILE\"] = f\"{validation_file}\"\n",
    "    os.environ[\"TEST_FILE\"] = f\"{test_file}\"\n",
    "    os.environ[\"OUTPUT_DIR\"] = f\"{output_dir}\"\n",
    "    \n",
    "    ! python /kaggle/input/kaggle-ner-utils/kaggle_run_ner.py \\\n",
    "    --model_name_or_path \"$MODEL_PATH\" \\\n",
    "    --validation_file \"$VALIDATION_FILE\" \\\n",
    "    --train_file \"$TRAIN_FILE\" \\\n",
    "    --test_file \"$TEST_FILE\" \\\n",
    "    --output_dir \"$OUTPUT_DIR\" \\\n",
    "    --report_to 'none' \\\n",
    "    --seed 123 \\\n",
    "    --do_predict\n",
    "\n",
    "def run_inference(test_rows, predict_batch=64_000, \n",
    "                  model_name_or_path='/kaggle/input/coleridge-bert-models/output', \n",
    "                  train_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n",
    "                  validation_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n",
    "                  test_file='./input_data/test_ner_input.json', \n",
    "                  output_dir='./pred'):\n",
    "    '''\n",
    "    '''\n",
    "    test_file = Path(test_file)\n",
    "    test_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    bert_outputs = []\n",
    "    for batch_begin in range(0, len(test_rows), predict_batch):\n",
    "        # write data rows to input file\n",
    "        with open(test_file, 'w') as f:\n",
    "            for row in test_rows[batch_begin:batch_begin + predict_batch]:\n",
    "                json.dump(row, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        # remove output dir\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)\n",
    "\n",
    "        # do predict\n",
    "        kaggle_run_ner_predict(\n",
    "            model_name_or_path=model_name_or_path, \n",
    "            train_file=train_file, validation_file=validation_file, test_file=test_file, \n",
    "            output_dir=output_dir)\n",
    "\n",
    "        # read predictions\n",
    "        with open(f'{output_dir}/test_predictions.txt') as f:\n",
    "            this_preds = f.read().split('\\n')[:-1]\n",
    "            bert_outputs += [pred.split() for pred in this_preds]\n",
    "    return bert_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dated-truth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:35:25.545437Z",
     "iopub.status.busy": "2021-05-17T00:35:25.543762Z",
     "iopub.status.idle": "2021-05-17T00:36:32.111300Z",
     "shell.execute_reply": "2021-05-17T00:36:32.110485Z"
    },
    "papermill": {
     "duration": 66.706354,
     "end_time": "2021-05-17T00:36:32.111573",
     "exception": false,
     "start_time": "2021-05-17T00:35:25.405219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-8d9ad0a7cb78afc8/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-8d9ad0a7cb78afc8/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\r\n",
      "[INFO|configuration_utils.py:470] 2021-05-17 00:35:31,088 >> loading configuration file /kaggle/working/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-05-17 00:35:31,089 >> Model config BertConfig {\r\n",
      "  \"_name_or_path\": \"bert-base-cased\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:470] 2021-05-17 00:35:31,089 >> loading configuration file /kaggle/working/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-05-17 00:35:31,090 >> Model config BertConfig {\r\n",
      "  \"_name_or_path\": \"bert-base-cased\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-05-17 00:35:31,090 >> Didn't find file /kaggle/working/output/tokenizer.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-05-17 00:35:31,090 >> Didn't find file /kaggle/working/output/added_tokens.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 00:35:31,090 >> loading file /kaggle/working/output/vocab.txt\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 00:35:31,090 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 00:35:31,090 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 00:35:31,090 >> loading file /kaggle/working/output/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 00:35:31,090 >> loading file /kaggle/working/output/tokenizer_config.json\r\n",
      "[INFO|modeling_utils.py:1049] 2021-05-17 00:35:31,151 >> loading weights file /kaggle/working/output/pytorch_model.bin\r\n",
      "[INFO|modeling_utils.py:1167] 2021-05-17 00:35:34,855 >> All model checkpoint weights were used when initializing BertForTokenClassification.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:1176] 2021-05-17 00:35:34,855 >> All the weights of BertForTokenClassification were initialized from the model checkpoint at /kaggle/working/output/.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 11.95ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-05-17 00:35:35,417 >> The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags.\r\n",
      "[INFO|trainer.py:1817] 2021-05-17 00:35:35,418 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:1818] 2021-05-17 00:35:35,418 >>   Num examples = 367\r\n",
      "[INFO|trainer.py:1819] 2021-05-17 00:35:35,418 >>   Batch size = 8\r\n",
      "100%|███████████████████████████████████████████| 46/46 [00:54<00:00,  1.16s/it]/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\r\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\r\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\r\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\r\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\r\n",
      "  avg = a.mean(axis)\r\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\r\n",
      "  ret = ret.dtype.type(ret / rcount)\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-05-17 00:36:31,228 >> ***** test metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,228 >>   init_mem_cpu_alloc_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,228 >>   init_mem_cpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,229 >>   test_accuracy             =        1.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,229 >>   test_f1                   =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,229 >>   test_loss                 =     0.2176\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,229 >>   test_mem_cpu_alloc_delta  =        8MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,229 >>   test_mem_cpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,229 >>   test_precision            =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,229 >>   test_recall               =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,229 >>   test_runtime              = 0:00:55.63\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 00:36:31,229 >>   test_samples_per_second   =      6.597\r\n",
      "100%|███████████████████████████████████████████| 46/46 [00:54<00:00,  1.19s/it]\r\n"
     ]
    }
   ],
   "source": [
    "predict_batch = 64_000 \n",
    "\n",
    "model_name_or_path = '/kaggle/working/output/' #'/kaggle/input/coleridge-bert-models/output'\n",
    "test_file = './input_data/test_ner_input.json'\n",
    "train_file = 'train_ner.json' #'/kaggle/input/coleridge-bert-models/train_ner.json'\n",
    "validation_file = 'train_ner.json' #'/kaggle/input/coleridge-bert-models/train_ner.json'\n",
    "output_dir = './pred'\n",
    "\n",
    "bert_outputs = run_inference(test_rows, predict_batch=predict_batch, \n",
    "                             model_name_or_path=model_name_or_path, \n",
    "                             test_file=test_file, train_file=train_file, validation_file=validation_file,\n",
    "                             output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "lasting-dressing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:32.396091Z",
     "iopub.status.busy": "2021-05-17T00:36:32.390493Z",
     "iopub.status.idle": "2021-05-17T00:36:33.130893Z",
     "shell.execute_reply": "2021-05-17T00:36:33.130164Z"
    },
    "papermill": {
     "duration": 0.882903,
     "end_time": "2021-05-17T00:36:33.131054",
     "exception": false,
     "start_time": "2021-05-17T00:36:32.248151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pred/test_predictions.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls {output_dir}/test_predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "classified-photography",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:33.413875Z",
     "iopub.status.busy": "2021-05-17T00:36:33.412505Z",
     "iopub.status.idle": "2021-05-17T00:36:33.417039Z",
     "shell.execute_reply": "2021-05-17T00:36:33.416410Z"
    },
    "papermill": {
     "duration": 0.149839,
     "end_time": "2021-05-17T00:36:33.417177",
     "exception": false,
     "start_time": "2021-05-17T00:36:33.267338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bert_dataset_labels(test_rows, paper_length, bert_outputs):\n",
    "    '''\n",
    "    Returns:\n",
    "        bert_dataset_labels (list): Each element is a set consisting of labels predicted\n",
    "            by the model.\n",
    "    '''\n",
    "    test_sentences = [row['tokens'] for row in test_rows]\n",
    "    \n",
    "    bert_dataset_labels = [] # store all dataset labels for each publication\n",
    "\n",
    "    for length in paper_length:\n",
    "        labels = set()\n",
    "        for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n",
    "            curr_phrase = ''\n",
    "            for word, tag in zip(sentence, pred):\n",
    "                if tag == 'B': # start a new phrase\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "                    curr_phrase = word\n",
    "                elif tag == 'I' and curr_phrase: # continue the phrase\n",
    "                    curr_phrase += ' ' + word\n",
    "                else: # end last phrase (if any)\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "            # check if the label is the suffix of the sentence\n",
    "            if curr_phrase:\n",
    "                labels.add(curr_phrase)\n",
    "                curr_phrase = ''\n",
    "\n",
    "        # record dataset labels for this publication\n",
    "        bert_dataset_labels.append(labels)\n",
    "\n",
    "        del test_sentences[:length], bert_outputs[:length]\n",
    "        \n",
    "    return bert_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "rolled-productivity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:33.702234Z",
     "iopub.status.busy": "2021-05-17T00:36:33.701568Z",
     "iopub.status.idle": "2021-05-17T00:36:33.704373Z",
     "shell.execute_reply": "2021-05-17T00:36:33.703720Z"
    },
    "papermill": {
     "duration": 0.147657,
     "end_time": "2021-05-17T00:36:33.704510",
     "exception": false,
     "start_time": "2021-05-17T00:36:33.556853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = ['They do not present all the features', \n",
    "             'Despite the pretraining on the Tigers EcoNAX dataset',\n",
    "             'Weirdly there has been lots of studies based on WGS Equality Definitiveness Dataset']\n",
    "paper_length = [2, 1]\n",
    "test_rows = [{'tokens': sentence.split(), 'tags': len(sentence.split()) * ['O']} \n",
    "             for sentence in sentences]\n",
    "bert_outputs = [['O', 'O', 'O', 'B', 'I', 'I', 'O'],\n",
    "                ['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I'],\n",
    "                ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I']]\n",
    "\n",
    "for i, row in enumerate(test_rows):\n",
    "    assert len(row['tokens']) == len(row['tags']) == len(bert_outputs[i])\n",
    "\n",
    "bert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "other-howard",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:33.980295Z",
     "iopub.status.busy": "2021-05-17T00:36:33.979619Z",
     "iopub.status.idle": "2021-05-17T00:36:33.982401Z",
     "shell.execute_reply": "2021-05-17T00:36:33.982986Z"
    },
    "papermill": {
     "duration": 0.143819,
     "end_time": "2021-05-17T00:36:33.983156",
     "exception": false,
     "start_time": "2021-05-17T00:36:33.839337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Tigers EcoNAX dataset', 'present all the'},\n",
       " {'WGS Equality Definitiveness Dataset'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "structured-filing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:34.259338Z",
     "iopub.status.busy": "2021-05-17T00:36:34.258689Z",
     "iopub.status.idle": "2021-05-17T00:36:34.263581Z",
     "shell.execute_reply": "2021-05-17T00:36:34.264117Z"
    },
    "papermill": {
     "duration": 0.144118,
     "end_time": "2021-05-17T00:36:34.264285",
     "exception": false,
     "start_time": "2021-05-17T00:36:34.120167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def jaccard_similarity(s1, s2):\n",
    "    l1 = set(s1.split(\" \"))\n",
    "    l2 = set(s2.split(\" \"))\n",
    "    intersection = len(list(l1.intersection(l2)))\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "mobile-commissioner",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:34.539055Z",
     "iopub.status.busy": "2021-05-17T00:36:34.538446Z",
     "iopub.status.idle": "2021-05-17T00:36:34.543930Z",
     "shell.execute_reply": "2021-05-17T00:36:34.544503Z"
    },
    "papermill": {
     "duration": 0.144238,
     "end_time": "2021-05-17T00:36:34.544668",
     "exception": false,
     "start_time": "2021-05-17T00:36:34.400430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity('USGS Frog Counts Data', 'USGA Croc Counts Data') == 1 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "enclosed-boxing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:34.820320Z",
     "iopub.status.busy": "2021-05-17T00:36:34.819695Z",
     "iopub.status.idle": "2021-05-17T00:36:34.826618Z",
     "shell.execute_reply": "2021-05-17T00:36:34.826128Z"
    },
    "papermill": {
     "duration": 0.145442,
     "end_time": "2021-05-17T00:36:34.826759",
     "exception": false,
     "start_time": "2021-05-17T00:36:34.681317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_bert_labels(bert_dataset_labels):\n",
    "    '''\n",
    "    When several labels for a paper are too similar, keep just one of them.\n",
    "    '''\n",
    "    filtered_bert_labels = []\n",
    "\n",
    "    for labels in bert_dataset_labels:\n",
    "        filtered = []\n",
    "\n",
    "        for label in sorted(labels, key=len):\n",
    "            label = clean_training_text(label, lower=True)\n",
    "            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n",
    "                filtered.append(label)\n",
    "\n",
    "        filtered_bert_labels.append('|'.join(filtered))\n",
    "    return filtered_bert_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "regional-adrian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:35.107709Z",
     "iopub.status.busy": "2021-05-17T00:36:35.107066Z",
     "iopub.status.idle": "2021-05-17T00:36:35.109715Z",
     "shell.execute_reply": "2021-05-17T00:36:35.110392Z"
    },
    "papermill": {
     "duration": 0.145382,
     "end_time": "2021-05-17T00:36:35.110559",
     "exception": false,
     "start_time": "2021-05-17T00:36:34.965177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moma artists|housing market|moma artists catalogue',\n",
       " 'deep sea rock salts|rhs fertiliser index']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset_labels = [{'moma artists catalogue', 'moma artists', 'housing market'},\n",
    "                       {'rhs flowers fertiliser index', 'deep sea rock salts', 'rhs fertiliser index'}]\n",
    "\n",
    "filter_bert_labels(bert_dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-zealand",
   "metadata": {
    "papermill": {
     "duration": 0.139062,
     "end_time": "2021-05-17T00:36:35.387586",
     "exception": false,
     "start_time": "2021-05-17T00:36:35.248524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overall prediction for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "radical-music",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:35.668653Z",
     "iopub.status.busy": "2021-05-17T00:36:35.668032Z",
     "iopub.status.idle": "2021-05-17T00:36:35.672557Z",
     "shell.execute_reply": "2021-05-17T00:36:35.673106Z"
    },
    "papermill": {
     "duration": 0.146122,
     "end_time": "2021-05-17T00:36:35.673284",
     "exception": false,
     "start_time": "2021-05-17T00:36:35.527162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def combine_matching_and_bert(literal_preds, filtererd_bert_labels):\n",
    "    final_predictions = []\n",
    "    for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n",
    "        if literal_match:\n",
    "            final_predictions.append(literal_match)\n",
    "        else:\n",
    "            final_predictions.append(bert_pred)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "appreciated-interpretation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T00:36:35.952172Z",
     "iopub.status.busy": "2021-05-17T00:36:35.951477Z",
     "iopub.status.idle": "2021-05-17T00:36:35.956361Z",
     "shell.execute_reply": "2021-05-17T00:36:35.956889Z"
    },
    "papermill": {
     "duration": 0.145662,
     "end_time": "2021-05-17T00:36:35.957062",
     "exception": false,
     "start_time": "2021-05-17T00:36:35.811400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mongolian racing cars|reallife headphones',\n",
       " 'hifi dataset|headphones collection data']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_preds = ['mongolian racing cars|reallife headphones', '']\n",
    "filtered_bert_labels = ['data|dataset', 'hifi dataset|headphones collection data']\n",
    "combine_matching_and_bert(literal_preds, filtered_bert_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-comparative",
   "metadata": {
    "papermill": {
     "duration": 0.138966,
     "end_time": "2021-05-17T00:36:36.234652",
     "exception": false,
     "start_time": "2021-05-17T00:36:36.095686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reference\n",
    "- https://www.kaggle.com/tungmphung/pytorch-bert-for-named-entity-recognition/notebook\n",
    "- https://www.kaggle.com/tungmphung/coleridge-matching-bert-ner/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-poison",
   "metadata": {
    "papermill": {
     "duration": 0.138191,
     "end_time": "2021-05-17T00:36:36.510541",
     "exception": false,
     "start_time": "2021-05-17T00:36:36.372350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 191.945002,
   "end_time": "2021-05-17T00:36:38.410221",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-17T00:33:26.465219",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
