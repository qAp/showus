{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# showus"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#default_exp showus"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "! pip install /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\n! pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n! pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n! pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n! pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\nimport os, shutil\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport itertools\nfrom functools import partial\nimport re\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport transformers, seqeval\nfrom transformers import AutoTokenizer, DataCollatorForTokenClassification\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_dataset, ClassLabel, load_metric\n\nimport matplotlib.pyplot as plt"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Utilities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\nPath.ls = lambda pth: list(pth.iterdir())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Data I/O"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_train_meta(pth, group_id=True):\n    df = pd.read_csv(pth)\n    if group_id:\n        df = df.groupby('Id').agg({'pub_title': 'first', 'dataset_title': '|'.join, \n                                   'dataset_label': '|'.join, 'cleaned_label': '|'.join}).reset_index()\n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\ndf = load_train_meta(pth, group_id=True)\ndf_nogroup = load_train_meta(pth, group_id=False)\nprint(len(df), len(df_nogroup))\ndup_ids = df_nogroup[df_nogroup.Id.duplicated()].Id.unique()\nprint(df[df.Id.isin(dup_ids)].dataset_label.values[-10:])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_papers(dir_json, paper_ids):\n    '''\n    Load papers into a dictionary.\n    \n    `papers`: \n        {''}\n    '''\n    \n    papers = {}\n    for paper_id in paper_ids:\n        with open(f'{dir_json}/{paper_id}.json', 'r') as f:\n            paper = json.load(f)\n            papers[paper_id] = paper\n    return papers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[-10:]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\nprint(type(papers))\nprint(\n    papers[ np.random.choice(df.Id.values) ][0]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_sample_text(jpth):\n    sections = json.loads(jpth.read_text())\n    text = '\\n'.join(section['text'] for section in sections)\n    return text"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\nprint(load_sample_text(jpths_trn[0])[:1_000])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Data processing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef clean_training_text(txt, lower=False, total_clean=False):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    txt = str(txt).lower() if lower else str(txt)\n    txt = re.sub('[^A-Za-z0-9]+', ' ', txt).strip()\n    if total_clean:\n        txt = re.sub(' +', ' ', txt)\n    return txt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(clean_training_text('@kaggle This competition awards $90,000!!!!.'))\nprint(clean_training_text('HoPKLd + 7 ! 11,002', total_clean=True, lower=True))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef shorten_sentences(sentences, max_length=64, overlap=20):\n    '''\n    Args:\n        sentences (list): List of sentences.\n        max_length (int): Maximum number of words allowed for each sentence.\n        overlap (int): If a sentence exceeds `max_length`, we split it to multiple sentences with \n            this amount of overlapping.\n    '''\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > max_length:\n            for p in range(0, len(words), max_length - overlap):\n                short_sentences.append(' '.join(words[p:p+max_length]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\nsentences = load_sample_text(jpths_trn[0]).split('.')[:2]\nshort_sentences = shorten_sentences(sentences, max_length=10, overlap=2)\nprint('Before:', sentences)\nprint()\nprint('After:', short_sentences)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef find_sublist(big_list, small_list):\n    all_positions = []\n    for i in range(len(big_list) - len(small_list) + 1):\n        if small_list == big_list[i:i+len(small_list)]:\n            all_positions.append(i)\n    \n    return all_positions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "big_list = ['If', 'the', 'thing', 'above', 'is', 'below', 'that', 'thing', 'which', 'is',\n            'not', 'as', 'high', 'up', 'on', 'the', 'thing', 'above', 'when', 'it', 'is', \n            'underneath', 'them.']\nsmall_list = ['the', 'thing', 'above']\n\nfind_sublist(big_list, small_list)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Named Entity Recognition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_ner_classlabel():\n    '''\n    Labels for named entity recognition.\n        'O': Token not part of a phrase that mentions a dataset.\n        'I': Intermediate token of a phrase mentioning a dataset.\n        'B': First token of a phrase mentioning a dataset.\n    '''\n    return ClassLabel(names=['O', 'I', 'B'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "classlabel = get_ner_classlabel()\nprint(classlabel)\nprint(classlabel.str2int(['I', 'O', 'B']), classlabel.str2int('I'))\nprint(classlabel.int2str(2), classlabel.int2str([2, 1, 0]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef tag_sentence(sentence, labels, classlabel=None): \n    '''\n    requirement: both sentence and labels are already cleaned\n    '''\n    sentence_words = sentence.split()\n    \n    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n                                  for label in labels): # positive sample\n        nes = [classlabel.str2int('O')] * len(sentence_words)\n        for label in labels:\n            label_words = label.split()\n\n            all_pos = find_sublist(sentence_words, label_words)\n            for pos in all_pos:\n                nes[pos] = classlabel.str2int('B')\n                for i in range(pos+1, pos+len(label_words)):\n                    nes[i] = classlabel.str2int('I')\n\n        return True, list(zip(sentence_words, nes))\n        \n    else: # negative sample\n        nes = [classlabel.str2int('O')] * len(sentence_words)\n        return False, list(zip(sentence_words, nes))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sentence = (\"The International Standard Classification of Education, known by its acronym ISCED, \"\n            \"was developed by the United Nations Educational, \"\n            \"Scientific, and Cultural Organization during the late 1960s and 1970s\")\nlabels = ['The International', 'Cultural Organization', 'United Nations Educational']\n\nsentence = clean_training_text(sentence)\nlabels = [clean_training_text(label) for label in labels]\nclasslabel = get_ner_classlabel()\nfound_any, token_tags = tag_sentence(sentence, labels, classlabel=classlabel)\n\nprint('A label is found in the sentence:', found_any)\nprint('(token, tag) pairs:')\nprint(token_tags)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef extract_sentences(paper, sentence_definition='sentence'):\n    if sentence_definition == 'sentence':\n        sentences = set(clean_training_text(sentence) \n                        for sec in paper for sentence in sec['text'].split('.') if sec['text'])\n    elif sentence_definition == 'section':\n        sentences = set(clean_training_text(sec['section_title'] + '\\n' + sec['text']) \n                        for sec in paper if sec['text'])\n    return sentences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[100:110]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', df.Id)\npaper = papers[df.Id.iloc[3]]\nprint('..... Sentence definition = normal sentence')\nsentences = extract_sentences(paper, sentence_definition='sentence')\nprint(len(sentences), list(sentences)[:2], end='\\n\\n')\nprint('..... Sentence definition = paper section')\nsentences = extract_sentences(paper, sentence_definition='section')\nprint(len(sentences), list(sentences)[:2][:1_000], end='\\n\\n')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_paper_ner_data(paper, labels, classlabel=None,\n                       sentence_definition='sentence', max_length=64, overlap=20):\n    '''\n    Get NER data for a single paper.\n    '''\n    labels = [clean_training_text(label) for label in labels]\n    sentences = extract_sentences(paper, sentence_definition=sentence_definition)\n    sentences = shorten_sentences(sentences, max_length=max_length, overlap=overlap) \n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n\n    cnt_pos, cnt_neg, ner_data = 0, 0, []\n    for sentence in sentences:\n        is_positive, tags = tag_sentence(sentence, labels, classlabel=classlabel)\n        if is_positive:\n            cnt_pos += 1\n            ner_data.append(tags)\n        elif any(word in sentence.lower() for word in ['data', 'study']): \n            ner_data.append(tags)\n            cnt_neg += 1    \n    return cnt_pos, cnt_neg, ner_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[230:240]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\nclasslabel = get_ner_classlabel()\n\nidx = 8\npaper = papers[df.Id.iloc[idx]]\nlabels = df.dataset_label.iloc[idx].split('|')\ncnt_pos, cnt_neg, ner_data = get_paper_ner_data(paper, labels, classlabel=classlabel,\n                                                sentence_definition='section', max_length=512, overlap=20)\nprint(cnt_pos, cnt_neg)\nprint([len(sec) for sec in ner_data])\nprint(ner_data[-2])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_ner_data(papers, df=None, classlabel=None, shuffle=True, \n                 sentence_definition='sentence', max_length=64, overlap=20):\n    '''\n    Get NER data for a list of papers.\n    \n    Args:\n        papers (dict): Like that returned by `load_papers`.\n        df (pd.DataFrame): Competition's train.csv or a subset of it.\n    Returns:\n        cnt_pos (int): Number of samples (or 'sentences') that are tagged or partly\n            tagged as datasets.\n        cnt_neg (int): Number of samples (or 'sentences') that are not tagged\n            or partly tagged as datasets.\n        ner_data (list): List of samples, or 'sentences'. Each element is of the form:\n            [('There', 0), ('has', 0), ('been', 0), ...]\n    '''\n    cnt_pos, cnt_neg = 0, 0 \n    ner_data = []\n\n    tqdm._instances.clear()\n    pbar = tqdm(total=len(df))\n    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():\n        paper = papers[id]\n        labels = dataset_label.split('|')\n                \n        cnt_pos_, cnt_neg_, ner_data_ = get_paper_ner_data(\n            paper, labels, classlabel=classlabel, \n            sentence_definition=sentence_definition, max_length=max_length, overlap=overlap)\n        cnt_pos += cnt_pos_\n        cnt_neg += cnt_neg_\n        ner_data.extend(ner_data_)\n\n        pbar.update(1)\n        pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n\n    if shuffle:\n        random.shuffle(ner_data)\n    return cnt_pos, cnt_neg, ner_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\ndf = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:10]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\nclasslabel = get_ner_classlabel()\ncnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False,\n                                          sentence_definition='sentence', max_length=64, overlap=20)\nprint(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\nprint(ner_data[250])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\ndf = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:10]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\nclasslabel = get_ner_classlabel()\ncnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False, \n                                          sentence_definition='section', max_length=512, overlap=20)\nprint(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\nprint(ner_data[3])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef write_ner_json(ner_data, pth=Path('train_ner.json')):\n    '''\n    Save NER data to json file.\n    '''\n    with open(pth, 'w') as f:\n        for row in ner_data:\n            words, nes = list(zip(*row))\n            row_json = {'tokens' : words, 'ner_tags' : nes}\n            json.dump(row_json, f)\n            f.write('\\n')    "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "ner_data = [\n    [('There', 0), ('is', 0), ('no', 0), ('dataset', 0), ('here', 0)], \n    [('Load', 0), ('the', 0), ('UN', 2), ('Trade', 1), ('Development', 1), ('into', 0), ('view', 0)]\n]\nwrite_ner_json(ner_data, pth=Path('/kaggle/tmp_ner.json'))\n! cat /kaggle/tmp_ner.json"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_ner_datasets(data_files=None):\n    '''\n    Load NER data in json files to a `datasets` object.  In addition,\n    Append the NER ClassLabel for the `ner_tags` feature.\n    '''\n    datasets = load_dataset('json', data_files=data_files)\n    classlabel = get_ner_classlabel()\n    for split, dataset in datasets.items():\n        dataset.features['ner_tags'].feature = classlabel\n    return datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\nprint()\nprint(datasets['valid'].features)\nprint(datasets['train'][1])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef create_tokenizer(model_checkpoint='distilbert-base-cased'):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n    return tokenizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "tokenizer = create_tokenizer(model_checkpoint='distilbert-base-cased')\nprint(\n    tokenizer(\"A smattering of people descended from the midday boat on Monday.\"))\nprint()\nprint(\n    tokenizer(\"Giglio boasts several pristine bays with crystal clear water\".split(), is_split_into_words=True)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sentence = \"A smattering of people descended from the midday boat on Monday.\".split()\ntokenized_sentence = tokenizer(sentence, is_split_into_words=True)\n\nprint(sentence)\nprint(tokenized_sentence.word_ids())\nprint(tokenizer.convert_ids_to_tokens(tokenized_sentence['input_ids']))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find a suitable original sample length, given a max model input length of 512\n\nmax_input_length = 512\nmax_length, overlap = 360, 20\n\ndf = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True)\ndf = df.sample(100).reset_index(drop=True)\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', df.Id)\nclasslabel = get_ner_classlabel()\ncnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, \n                                          sentence_definition='section', max_length=max_length, overlap=overlap)\nwrite_ner_json(ner_data, pth='train_ner.json')\n\ndatasets = load_ner_datasets(data_files={'train':'train_ner.json'})\n\ntokenizer = create_tokenizer(model_checkpoint='distilbert-base-cased')\n\ntokenized_input = tokenizer(datasets['train']['tokens'], truncation=False, is_split_into_words=True)\n\noriginal_sample_lengths = np.array([len(tokens) for tokens in datasets['train']['tokens']])\ntokenized_sample_lengths = np.array([len(input_ids) for input_ids in tokenized_input['input_ids']])\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(original_sample_lengths, label='original lengths', linestyle='none', marker='o')\nax.plot(tokenized_sample_lengths, label='tokenized lengths', linestyle='none', marker='x')\nax.hlines(y=max_input_length, xmin=0, xmax=len(original_sample_lengths), color='red')\nax.set_ylabel('length')\nax.set_xlabel('sample index')\nax.set_title('section lengths')\nax.legend()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef tokenize_and_align_labels(examples, tokenizer=None, label_all_tokens=True):\n    '''\n    Adds a new field called 'labels' that are the NER tags to the tokenized input.\n    \n    Args:\n        tokenizer (transformers.AutoTokenizer): Tokenizer.\n        examples (datasets.arrow_dataset.Dataset): Dataset.\n        label_all_tokens (bool): If True, all sub-tokens are given the same tag as the \n            first sub-token, otherwise all but the first sub-token are given the tag\n            -100.\n    '''\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    word_ids_all = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(label[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n        word_ids_all.append(word_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    tokenized_inputs['word_ids'] = word_ids_all\n    return tokenized_inputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\ntokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\n\n! cat /kaggle/tmp_ner.json\n\nprint()\nprint(tokenize_and_align_labels(datasets['train'][:], tokenizer, label_all_tokens=True), end='\\n\\n')\n\ntokenized_datasets = datasets.map(\n    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\nprint(tokenized_datasets['valid'][:])\nprint(tokenized_datasets['train'].features)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save and load `datasets.dataset_dict.DatasetDict`\ntokenized_datasets.save_to_disk('testsave_datasets')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(datasets.load_from_disk('testsave_datasets')['train']['input_ids'])\nprint(tokenized_datasets['train']['input_ids'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "metric = load_metric('seqeval')\n\npredictions = np.array([['O', 'O', 'B', 'I', 'I', 'O']])\nreferences = [['O', 'O', 'B', 'I', 'I', 'O']]\nprint(metric.compute(predictions=predictions, references=references))\n\npredictions = [['O', 'O', 'B', 'I', 'I', 'O']]\nreferences = [['B', 'I', 'I', 'O', 'O', 'O']]\nprint(metric.compute(predictions=predictions, references=references))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef jaccard_similarity(s1, s2):\n    l1 = set(s1.split(\" \"))\n    l2 = set(s2.split(\" \"))\n    intersection = len(list(l1.intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "jaccard_similarity('USGS Frog Counts Data', 'USGA Croc Counts Data') == 1 / 3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef remove_nonoriginal_outputs(outputs, word_ids):\n    '''\n    Remove elements that correspond to special tokens or subtokens,\n    retaining only those elements that correspond to a word in original\n    text.\n    \n    Args:\n        outputs (np.array): \n        \n    Returns:\n        outputs (list)\n    '''\n    assert len(outputs) == len(word_ids)\n    idxs = [[word_id.index(i) for i in set(word_id) if i is not None] \n            for word_id in word_ids]\n    outputs = [output[idx].tolist() for output, idx in zip(outputs, idxs)]\n    for output in outputs:\n        assert -100 not in output\n    return outputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "classlabel = get_ner_classlabel()\n\npredictions = np.random.randn(2, 8, classlabel.num_classes)\npredictions = np.argmax(predictions, axis=2)\n\nlabel_ids = np.array([[-100, 0, 0,    2, 1,    2,    1, -100],\n                      [-100, 2, 1, -100, 0, -100, -100, -100]])\n\nword_ids = [[None, 0, 0, 1, 2, None],\n            [None, 0, 1, 1, 2, 2, None]]\n\ntrue_predictions = remove_nonoriginal_outputs(predictions, word_ids)\ntrue_label_ids   = remove_nonoriginal_outputs(label_ids,   word_ids)\n\nprint('predictions')\nprint('BEFORE:')\nprint(predictions)\nprint('AFTER:')\nprint(true_predictions)\nprint(60 * '=')\nprint('label_ids')\nprint('BEFORE:')\nprint(label_ids)\nprint('AFTER:')\nprint(true_label_ids)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef compute_metrics(p, metric=None, word_ids=None, label_list=None):\n    '''\n    1. Remove predicted and ground-truth class ids of special and sub tokens.\n    2. Convert class ids to class labels. (int ---> str)\n    3. Compute metric.\n    \n    Args:\n        p (tuple): 2-tuple consisting of model prediction and ground-truth\n            labels.  These will contain elements corresponding to special \n            tokens and sub-tokens.\n        word_ids (list): Word IDs from the tokenizer's output, indicating\n            which original word each sub-token belongs to.\n    '''\n    predictions, label_ids = p\n    predictions = predictions.argmax(axis=2)\n\n    true_predictions = remove_nonoriginal_outputs(predictions, word_ids)\n    true_label_ids = remove_nonoriginal_outputs(label_ids, word_ids)\n#     true_predictions = [[p for p, l, in zip(pred, label) if l != -100] \n#                         for pred, label in zip(predictions, label_ids)]\n#     true_label_ids   = [[l for l in label if l != -100] for label in label_ids]\n    \n    true_predictions = [[label_list[p] for p in pred] for pred in true_predictions]\n    true_labels = [[label_list[i] for i in label_id] for label_id in true_label_ids]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "batch_size = 2\nmax_example_length = 6\n\npredictions = np.random.randn(batch_size, max_example_length, classlabel.num_classes)\nlabel_ids = np.random.randint(low=0, high=classlabel.num_classes, \n                              size=(batch_size, max_example_length), dtype=np.int16)\nword_ids = [[None, 0, 0, 1, 2, None], \n            [None, 0, 1, None]]\n\nprint(predictions.argmax(axis=2))\nprint(label_ids)\np = (predictions, label_ids)\nmetric = load_metric('seqeval')\ncompute_metrics(p, metric=metric, label_list=classlabel.names, word_ids=word_ids)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## NER training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_meta = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:3]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', train_meta.Id)\n\nvalid_cutoff = int(.50 * len(train_meta))\nvalid_meta = train_meta.iloc[:valid_cutoff].reset_index(drop=True)\ntrain_meta = train_meta.iloc[valid_cutoff:].reset_index(drop=True)\n\nclasslabel = get_ner_classlabel()\ntrain_cnt_pos, train_cnt_neg, train_ner_data = get_ner_data(\n    papers, df=train_meta, classlabel=classlabel, sentence_definition='section', max_length=360, overlap=20)\nvalid_cnt_pos, valid_cnt_neg, valid_ner_data = get_ner_data(\n    papers, df=valid_meta, classlabel=classlabel, sentence_definition='section', max_length=360, overlap=20)\nprint(f'Train.  Positive count: {train_cnt_pos}.  Negative count: {train_cnt_neg}.')\nprint(f'Valid.  Positive count: {valid_cnt_pos}.  Negative count: {valid_cnt_neg}.')\n\nwrite_ner_json(train_ner_data, pth='train_ner.json')\nwrite_ner_json(valid_ner_data, pth='valid_ner.json')\ndatasets = load_ner_datasets(data_files={'train':'train_ner.json', 'valid':'valid_ner.json'})\n\nmodel_checkpoint = 'distilbert-base-cased'\ntokenizer = create_tokenizer(model_checkpoint)\ntokenized_datasets = datasets.map(\n    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\n\ntokenized_datasets.save_to_disk(f'datasetdict_{model_checkpoint}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_checkpoint = 'distilbert-base-cased'\ntokenizer = create_tokenizer(model_checkpoint)\ntokenized_datasets = datasets.load_from_disk(f'datasetdict_{model_checkpoint}')\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=classlabel.num_classes)\n\nmetric = load_metric('seqeval')\nword_ids = tokenized_datasets['valid']['word_ids']\ncompute_metrics_ = partial(compute_metrics, metric=metric, label_list=classlabel.names, word_ids=word_ids)\n\nargs = TrainingArguments(output_dir='test_training', num_train_epochs=2, \n                         learning_rate=2e-5, weight_decay=0.01,\n                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n                         save_strategy='epoch', save_total_limit=6)\n\ntrainer = Trainer(model=model, args=args, \n                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n                  data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics_)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "trainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "! ls -lrt test_training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "args = TrainingArguments(output_dir='test_training', num_train_epochs=4, \n                         learning_rate=2e-5, weight_decay=0.01,\n                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n                         save_strategy='epoch', save_total_limit=6)\n\ntrainer = Trainer(model=model, args=args, \n                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n                  data_collator=data_collator, tokenizer=tokenizer, \n                  compute_metrics=compute_metrics_)\ntrainer.train(resume_from_checkpoint='/kaggle/working/test_training/checkpoint-4/')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "trainer.evaluate()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## NER inference\n\n**Turn off the Internet here**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_ner_inference_data(papers, sample_submission, classlabel=None, \n                           sentence_definition='sentence', max_length=64, overlap=20):\n    '''\n    Args:\n        papers (dict): Each list in this dictionary consists of the section of a paper.\n        sample_submission (pd.DataFrame): Competition 'sample_submission.csv'.\n    Returns:\n        test_rows (list): Each list in this list is of the form: \n             [('goat', 0), ('win', 0), ...] and represents a sentence.  \n        paper_length (list): Number of sentences in each paper.\n    '''\n    test_rows = [] \n    paper_length = [] \n\n    for paper_id in sample_submission['Id']:\n        paper = papers[paper_id]\n\n        sentences = extract_sentences(paper, sentence_definition=sentence_definition)\n        sentences = shorten_sentences(sentences, max_length=max_length, overlap=overlap)\n        sentences = [sentence for sentence in sentences if len(sentence) > 10] \n        sentences = [sentence for sentence in sentences \n                     if any(word in sentence.lower() for word in ['data', 'study'])]\n\n        for sentence in sentences:\n            sentence_words = sentence.split()\n            dummy_tags = [classlabel.str2int('O')]*len(sentence_words)\n            test_rows.append(list(zip(sentence_words, dummy_tags)))\n\n        paper_length.append(len(sentences))\n\n    print(f'total number of \"sentences\": {len(test_rows)}')\n    return test_rows, paper_length"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test', sample_submission.Id)\nclasslabel = get_ner_classlabel()\ntest_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel, \n                                                 sentence_definition='section', max_length=300, overlap=100)\nprint(test_rows[1])\nprint(paper_length)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef ner_predict(pth=None, tokenizer=None, model=None, metric=None):\n    classlabel = get_ner_classlabel()\n    datasets = load_ner_datasets(data_files={'test':pth})\n    print('Tokenizing testset...')\n    tokenized_datasets = datasets.map(\n        partial(tokenize_and_align_labels,tokenizer=tokenizer, label_all_tokens=True), \n        batched=True) \n\n    print('Creating data collator...')\n    data_collator = DataCollatorForTokenClassification(tokenizer)\n    \n    print('Creating (dummy) training arguments...')\n    args = TrainingArguments(output_dir='test_ner', num_train_epochs=3, \n                             learning_rate=2e-5, weight_decay=0.01,\n                             per_device_train_batch_size=16, per_device_eval_batch_size=16,\n                             evaluation_strategy='epoch', logging_steps=4, report_to='none', \n                             save_strategy='epoch', save_total_limit=6)\n\n    print('Creating trainer...')\n    word_ids = tokenized_datasets['test']['word_ids']\n    compute_metrics_ = partial(compute_metrics, metric=metric, label_list=classlabel.names, word_ids=word_ids)\n    trainer = Trainer(model=model, args=args, \n                      train_dataset=tokenized_datasets['test'], eval_dataset=tokenized_datasets['test'], \n                      data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics_)\n\n    print('Predicting on test samples...')\n    predictions, label_ids, _ = trainer.predict(tokenized_datasets['test'])\n    predictions = predictions.argmax(axis=2)\n    predictions = remove_nonoriginal_outputs(predictions, word_ids)\n    label_ids   = remove_nonoriginal_outputs(label_ids, word_ids)\n    return predictions, label_ids"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# This shows where to look for the cached metric `seqeval`.\n# metric = load_metric('/root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/seqeval.py')\n\n# Exporting the cached metric \n\n# %cd /root/.cache\n# ! zip -r huggingface_cache.zip huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/\n# %cd "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_checkpoint = 'test_training/checkpoint-4/'\n\ntokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n# metric = load_metric('seqeval')\nmetric = load_metric('/root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/seqeval.py')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "samples = ['''Archaeologists estimate the carvings are between 4,000 and 5,000 years old''', \n           ('''I could see that I was looking at a deer stag upside down, '''\n            '''and as I continued looking around, more animals appeared on the rock,” he said.''')]\ntest_rows = [list(zip(sample.split(), len(sample.split()) * [0])) for sample in samples]\nwrite_ner_json(test_rows, pth='test_ner.json')\n\npredictions, label_ids = ner_predict(pth='test_ner.json', tokenizer=tokenizer, model=model, metric=metric)\nfor i in range(len(predictions)):\n    print(f'Sample {i}:', len(predictions[i]), len(label_ids[i]), len(samples[i].split()))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\n\ndef batched_ner_predict(pth, tokenizer=None, model=model, metric=metric, \n                        batch_size=16):\n    '''\n    Do inference on dataset in batches.\n    '''\n    lines = open(pth, mode='r').readlines()\n    \n    pth_tmp = 'ner_predict_tmp.json'\n    predictions, label_ids = [], []\n    for ib in range(0, len(lines), batch_size):\n        with open(pth_tmp, mode='w') as f:\n            f.writelines(lines[ ib: ib + batch_size ])\n\n        predictions_, label_ids_ = ner_predict(\n            pth_tmp, tokenizer=tokenizer, model=model, metric=metric)\n        predictions.extend(predictions_)\n        label_ids.extend(label_ids_)\n    return predictions, label_ids"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "samples = ['''Archaeologists estimate the carvings are between 4,000 and 5,000 years old''', \n           ('''I could see that I was looking at a deer stag upside down, '''\n            '''and as I continued looking around, more animals appeared on the rock,” he said.'''),\n           '''The RNN model we are about to build has LSTM cells as basic hidden units.''', \n           '''YouTube series, the Crooner Sessions. Now he gets his musical pals together in real life for ''']\ntest_rows = [list(zip(sample.split(), len(sample.split()) * [0])) for sample in samples]\nwrite_ner_json(test_rows, pth='test_ner.json')\n\npredictions, label_ids = batched_ner_predict(\n    'test_ner.json', tokenizer=tokenizer, model=model, metric=metric, batch_size=2)\n    \nfor i in range(len(predictions)):\n    print(f'Sample {i}:', len(predictions[i]), len(label_ids[i]), len(samples[i].split()))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_paper_dataset_labels(pth, paper_length, predictions):\n    '''\n    Args:\n        pth (Path, str): Path to json file containing NER data.  Each row is \n            of form: {'tokens': ['Studying', 'human'], 'ner_tags': [0, 0, ...]}.\n    \n    Returns:\n        paper_dataset_labels (list): Each element is a set consisting of labels predicted\n            by the model.\n    '''\n    test_sentences = [json.loads(sample)['tokens'] for sample in open(pth).readlines()]\n    \n    paper_dataset_labels = [] # store all dataset labels for each publication\n    for ipaper in range(len(paper_length)):\n        istart = sum(paper_length[:ipaper])\n        iend = istart + paper_length[ipaper]\n        \n        labels = set()\n        for sentence, pred in zip(test_sentences[istart:iend], predictions[istart:iend]):\n            curr_phrase = ''\n            for word, tag in zip(sentence, pred):\n                if tag == 'B': # start a new phrase\n                    if curr_phrase:\n                        labels.add(curr_phrase)\n                        curr_phrase = ''\n                    curr_phrase = word\n                elif tag == 'I' and curr_phrase: # continue the phrase\n                    curr_phrase += ' ' + word\n                else: # end last phrase (if any)\n                    if curr_phrase:\n                        labels.add(curr_phrase)\n                        curr_phrase = ''\n            # check if the label is the suffix of the sentence\n            if curr_phrase:\n                labels.add(curr_phrase)\n                curr_phrase = ''\n\n        # record dataset labels for this publication\n        paper_dataset_labels.append(labels)\n\n    return paper_dataset_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sentences = ['They do not present all the features', \n             'Despite the pretraining on the Tigers EcoNAX dataset',\n             'Weirdly there has been lots of studies based on WGS Equality Definitiveness Dataset']\npaper_length = [2, 1]\ntest_rows = [[(word, 0) for word in sentence.split()] for sentence in sentences]\npredictions = [['O', 'O', 'O', 'B', 'I', 'I', 'O'],\n               ['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I'],\n               ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I']]\nfor i, row in enumerate(test_rows):\n    assert len(row) == len(predictions[i])\n\nwrite_ner_json(test_rows, pth='test_ner.json')\n\npaper_dataset_labels = get_paper_dataset_labels('test_ner.json', paper_length, predictions)\nprint(paper_dataset_labels)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef filter_dataset_labels(paper_dataset_labels):\n    '''\n    When several labels for a paper are too similar, keep just one of them.\n    '''\n    filtered_dataset_labels = []\n\n    for labels in paper_dataset_labels:\n        filtered = []\n\n        for label in sorted(labels, key=len):\n            label = clean_training_text(label, lower=True)\n            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n                filtered.append(label)\n\n        filtered_dataset_labels.append('|'.join(filtered))\n    return filtered_dataset_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "paper_dataset_labels = [{'moma artists catalogue', 'moma artists', 'housing market'},\n                       {'rhs flowers fertiliser index', 'deep sea rock salts', 'rhs fertiliser index'}]\n\nfilter_dataset_labels(paper_dataset_labels)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Literal matching"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef create_knowledge_bank(pth):\n    '''\n    Args:\n        pth (str): Path to meta data like 'train.csv', which\n        needs to have columns: 'dataset_title', 'dataset_label', and 'cleaned_label'.\n        \n    Returns:\n        all_labels (set): All possible strings associated with a dataset from the meta data.\n    '''\n    df = load_train_meta(pth, group_id=False)\n    all_labels = set()\n    for label_1, label_2, label_3 in df[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n        all_labels.add(str(label_1).lower())\n        all_labels.add(str(label_2).lower())\n        all_labels.add(str(label_3).lower())\n    return all_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\nall_labels = create_knowledge_bank(pth)\nprint(len(all_labels))\nprint(sorted(all_labels)[:10])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef literal_match(paper, all_labels):\n    '''\n    Args:\n        paper ()\n    '''\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = clean_training_text(text_1, lower=True, total_clean=True)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_training_text(label, lower=True, total_clean=True))\n    return labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n\npth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\nall_labels = create_knowledge_bank(pth)\n\nliteral_preds = []\nfor paper_id in sample_submission.Id:\n    paper = papers[paper_id]\n    literal_preds.append('|'.join(literal_match(paper, all_labels)))\n    \nliteral_preds"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Overall prediction for submission"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef combine_matching_and_model(literal_preds, filtererd_dataset_labels):\n    '''\n    For a given sentence, if there's a literal match, use that as the final\n    prediction for the sentence.  If there isn't a literal match,\n    use what the model predicts.\n    '''\n    final_predictions = []\n    for literal_match, model_pred in zip(literal_preds, filtered_dataset_labels):\n        if literal_match:\n            final_predictions.append(literal_match)\n        else:\n            final_predictions.append(model_pred)\n    return final_predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "literal_preds = ['mongolian racing cars|reallife headphones', '']\nfiltered_dataset_labels = ['data|dataset', 'hifi dataset|headphones collection data']\ncombine_matching_and_model(literal_preds, filtered_dataset_labels)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Inference script"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "! cp ../input/huggingface-cache/huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/seqeval.py .\n\nmodel_checkpoint = '/kaggle/input/showusdata-distilbert-base-cased-ner/ner_training_results/checkpoint-72822'\n\nprint('Preparing NER inference data...')\nsample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\ntest_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel,\n                                                 sentence_definition='section', max_length=300, overlap=100)\nwrite_ner_json(test_rows, pth='test_ner.json')\n\nprint('Loading model, tokenizer, and metric...')\ntokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\nmetric = load_metric('seqeval.py')\n\nprint('Predicting on each sentence...')\npredictions, label_ids = ner_predict('test_ner.json', tokenizer=tokenizer, model=model, metric=metric)\npredictions = [[classlabel.int2str(p) for p in pred] for pred in predictions]\nlabel_ids   = [[classlabel.int2str(l) for l in label] for label in label_ids]\n\nprint('Getting predicted labels for each article...')\npaper_dataset_labels = get_paper_dataset_labels('test_ner.json', paper_length, predictions)\n\nprint('Keeping just one of labels that are too similar to each other...')\nfiltered_dataset_labels = filter_dataset_labels(paper_dataset_labels)\n\nsample_submission['PredictionString'] = filtered_dataset_labels\n\nsample_submission.to_csv('submission.csv', index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "! cat submission.csv"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Error analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_checkpoint = '/kaggle/input/showusdata-distilbert-base-cased-ner/ner_training_results/checkpoint-72822'\npth_train_json = '/kaggle/input/showus-data-ner-jsons/train_ner.json'\npth_valid_json = '/kaggle/input/showus-data-ner-jsons/valid_ner.json'\n\n\nclasslabel = get_ner_classlabel()\n\nner_data_train = open(pth_train_json).readlines()[:10]\nner_data_valid = open(pth_valid_json).readlines()[:10]\nner_data_train = [json.loads(sample) for sample in ner_data_train]\nner_data_valid = [json.loads(sample) for sample in ner_data_valid]\nner_data_train = [list(zip(sample['tokens'], sample['ner_tags'])) for sample in ner_data_train]\nner_data_valid = [list(zip(sample['tokens'], sample['ner_tags'])) for sample in ner_data_valid]\nwrite_ner_json(ner_data_train, pth='train_ner.json')\nwrite_ner_json(ner_data_valid, pth='valid_ner.json')\n\ntokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=classlabel.num_classes)\nmetric = load_metric('seqeval')\n\npredictions, label_ids = ner_predict(pth='valid_ner.json', tokenizer=tokenizer, model=model, metric=metric)\npredictions = [[classlabel.int2str(p) for p in pred] for pred in predictions]\nlabel_ids   = [[classlabel.int2str(l) for l in label] for label in label_ids]\n\npaper_dataset_labels = get_paper_dataset_labels('valid_ner.json', len(predictions) * [1], predictions)\ngt_paper_dataset_labels = get_paper_dataset_labels('valid_ner.json', len(label_ids) * [1], label_ids)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Predicted labels:')\nprint(paper_dataset_labels)\nprint('Ground-truth labels:')\nprint(gt_paper_dataset_labels)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "metric.compute(predictions=predictions, references=label_ids)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Reference\n- https://www.kaggle.com/tungmphung/pytorch-bert-for-named-entity-recognition/notebook\n- https://www.kaggle.com/tungmphung/coleridge-matching-bert-ner/notebook\n- https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n- https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin\n- https://huggingface.co/docs/datasets/loading_metrics.html\n- [Python strings and memory](https://rushter.com/blog/python-strings-and-memory/)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
