{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# showus"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#default_exp showus"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Processing /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\nInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 0.8.7\n    Uninstalling fsspec-0.8.7:\n      Successfully uninstalled fsspec-0.8.7\nSuccessfully installed fsspec-2021.4.0\nLooking in links: file:///kaggle/input/coleridge-packages/packages/datasets\nProcessing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\nProcessing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\nRequirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nProcessing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.4.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\nProcessing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\nInstalling collected packages: tqdm, xxhash, huggingface-hub, datasets\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.59.0\n    Uninstalling tqdm-4.59.0:\n      Successfully uninstalled tqdm-4.59.0\nSuccessfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\nProcessing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\nProcessing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\nInstalling collected packages: tokenizers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.10.2\n    Uninstalling tokenizers-0.10.2:\n      Successfully uninstalled tokenizers-0.10.2\nSuccessfully installed tokenizers-0.10.1\nProcessing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.45)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2021.3.17)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.4)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (4.0.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.5.1\n    Uninstalling transformers-4.5.1:\n      Successfully uninstalled transformers-4.5.1\nSuccessfully installed transformers-4.5.0.dev0\n"
    }
   ],
   "source": "! pip install /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\n! pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n! pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n! pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n! pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\nimport os, shutil\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport itertools\nfrom functools import partial\nimport re\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport transformers, seqeval\nfrom transformers import AutoTokenizer, DataCollatorForTokenClassification\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_dataset, ClassLabel, load_metric"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Utilities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\nPath.ls = lambda pth: list(pth.iterdir())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Data I/O"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_train_meta(pth, group_id=True):\n    df = pd.read_csv(pth)\n    if group_id:\n        df = df.groupby('Id').agg({'pub_title': 'first', 'dataset_title': '|'.join, \n                                   'dataset_label': '|'.join, 'cleaned_label': '|'.join}).reset_index()\n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "14316 19661\n['Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n 'Beginning Postsecondary Students Longitudinal Study|Education Longitudinal Study|Beginning Postsecondary Students'\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n 'Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n 'Beginning Postsecondary Student|Beginning Postsecondary Students']\n"
    }
   ],
   "source": "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\ndf = load_train_meta(pth, group_id=True)\ndf_nogroup = load_train_meta(pth, group_id=False)\nprint(len(df), len(df_nogroup))\ndup_ids = df_nogroup[df_nogroup.Id.duplicated()].Id.unique()\nprint(df[df.Id.isin(dup_ids)].dataset_label.values[-10:])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_papers(dir_json, paper_ids):\n    '''\n    Load papers into a dictionary.\n    \n    `papers`: \n        {''}\n    '''\n    \n    papers = {}\n    for paper_id in paper_ids:\n        with open(f'{dir_json}/{paper_id}.json', 'r') as f:\n            paper = json.load(f)\n            papers[paper_id] = paper\n    return papers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'dict'>\n{'section_title': 'Abstract', 'text': \"This article investigates an important factor in student achievement-parental involvement. Using data from the National Education Longitudinal Study (NELS), we estimate a value-added education production function that includes parental effort as an input. Parental effort equations are also estimated as a function of child, parent, household, and school characteristics. Our results suggest that parental effort has a strong positive effect on achievement that is large relative to the effect of school resources and is not captured by family background variables. Parents appear to reduce their effort in response to increased school resources, suggesting potential ''crowding out'' of school resources.\"}\n"
    }
   ],
   "source": "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id.unique()[:10])\nprint(type(papers))\nprint(\n    papers[ random.choice(list(papers.keys())) ][0]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_sample_text(jpth):\n    sections = json.loads(jpth.read_text())\n    text = '\\n'.join(section['text'] for section in sections)\n    return text"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s. ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data. Creating a U.S. crosswalk to this system has been a goal of the National Center for Education Statistics and the Office of Research since the late 197,,s, when the National Institute of Education (the predecessor agency to the Office of Educational Research and Improvement) began exploring the idea. The design and implementation of a workable crosswalk, however, awaited the advent of changes to the Classification of Instructional Programs (CIP) system. The 1990 revision of the CIP system laid the foundation for a workable international crosswalk. Adoption of the National Education Goals set global consciousness and international educational comparisons firml\n"
    }
   ],
   "source": "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\nprint(load_sample_text(jpths_trn[0])[:1_000])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Data processing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef clean_training_text(txt, lower=False, total_clean=False):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    txt = str(txt).lower() if lower else str(txt)\n    txt = re.sub('[^A-Za-z0-9]+', ' ', txt).strip()\n    if total_clean:\n        txt = re.sub(' +', ' ', txt)\n    return txt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "kaggle This competition awards 90 000\nhopkld 7 11 002\n"
    }
   ],
   "source": "print(clean_training_text('@kaggle This competition awards $90,000!!!!.'))\nprint(clean_training_text('HoPKLd + 7 ! 11,002', total_clean=True, lower=True))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef shorten_sentences(sentences, max_length=64, overlap=20):\n    '''\n    Args:\n        sentences (list): List of sentences.\n        max_length (int): Maximum number of words allowed for each sentence.\n        overlap (int): If a sentence exceeds `max_length`, we split it to multiple sentences with \n            this amount of overlapping.\n    '''\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > max_length:\n            for p in range(0, len(words), max_length - overlap):\n                short_sentences.append(' '.join(words[p:p+max_length]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Before: ['The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s', ' ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data']\n\nAfter: ['The International Standard Classification of Education, known by its acronym', 'its acronym ISCED, was developed by the United Nations Educational,', 'Nations Educational, Scientific, and Cultural Organization during the late 1960s', 'late 1960s and 1970s', 'ISCED was implemented in 1976 and is the recognized international', 'recognized international standard for reporting and interpreting education program data', 'program data']\n"
    }
   ],
   "source": "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\nsentences = load_sample_text(jpths_trn[0]).split('.')[:2]\nshort_sentences = shorten_sentences(sentences, max_length=10, overlap=2)\nprint('Before:', sentences)\nprint()\nprint('After:', short_sentences)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef find_sublist(big_list, small_list):\n    all_positions = []\n    for i in range(len(big_list) - len(small_list) + 1):\n        if small_list == big_list[i:i+len(small_list)]:\n            all_positions.append(i)\n    \n    return all_positions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 15]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "big_list = ['If', 'the', 'thing', 'above', 'is', 'below', 'that', 'thing', 'which', 'is',\n            'not', 'as', 'high', 'up', 'on', 'the', 'thing', 'above', 'when', 'it', 'is', \n            'underneath', 'them.']\nsmall_list = ['the', 'thing', 'above']\n\nfind_sublist(big_list, small_list)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Named Entity Recognition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_ner_classlabel():\n    '''\n    Labels for named entity recognition.\n        'O': Token not part of a phrase that mentions a dataset.\n        'I': Intermediate token of a phrase mentioning a dataset.\n        'B': First token of a phrase mentioning a dataset.\n    '''\n    return ClassLabel(names=['O', 'I', 'B'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None)\n[1, 0, 2] 1\nB ['B', 'I', 'O']\n"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\nprint(classlabel)\nprint(classlabel.str2int(['I', 'O', 'B']), classlabel.str2int('I'))\nprint(classlabel.int2str(2), classlabel.int2str([2, 1, 0]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef tag_sentence(sentence, labels, classlabel=None): \n    '''\n    requirement: both sentence and labels are already cleaned\n    '''\n    sentence_words = sentence.split()\n    \n    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n                                  for label in labels): # positive sample\n        nes = [classlabel.str2int('O')] * len(sentence_words)\n        for label in labels:\n            label_words = label.split()\n\n            all_pos = find_sublist(sentence_words, label_words)\n            for pos in all_pos:\n                nes[pos] = classlabel.str2int('B')\n                for i in range(pos+1, pos+len(label_words)):\n                    nes[i] = classlabel.str2int('I')\n\n        return True, list(zip(sentence_words, nes))\n        \n    else: # negative sample\n        nes = [classlabel.str2int('O')] * len(sentence_words)\n        return False, list(zip(sentence_words, nes))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "A label is found in the sentence: True\n(token, tag) pairs:\n[('The', 2), ('International', 1), ('Standard', 0), ('Classification', 0), ('of', 0), ('Education', 0), ('known', 0), ('by', 0), ('its', 0), ('acronym', 0), ('ISCED', 0), ('was', 0), ('developed', 0), ('by', 0), ('the', 0), ('United', 2), ('Nations', 1), ('Educational', 1), ('Scientific', 0), ('and', 0), ('Cultural', 2), ('Organization', 1), ('during', 0), ('the', 0), ('late', 0), ('1960s', 0), ('and', 0), ('1970s', 0)]\n"
    }
   ],
   "source": "sentence = (\"The International Standard Classification of Education, known by its acronym ISCED, \"\n            \"was developed by the United Nations Educational, \"\n            \"Scientific, and Cultural Organization during the late 1960s and 1970s\")\nlabels = ['The International', 'Cultural Organization', 'United Nations Educational']\n\nsentence = clean_training_text(sentence)\nlabels = [clean_training_text(label) for label in labels]\nclasslabel = get_ner_classlabel()\nfound_any, token_tags = tag_sentence(sentence, labels, classlabel=classlabel)\n\nprint('A label is found in the sentence:', found_any)\nprint('(token, tag) pairs:')\nprint(token_tags)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_ner_data(papers, df=None, classlabel=None, shuffle=True):\n    '''\n    Args:\n        papers (dict): Like that returned by `load_papers`.\n        df (pd.DataFrame): Competition's train.csv or a subset of it.\n    '''\n    cnt_pos, cnt_neg = 0, 0 \n    ner_data = []\n\n    tqdm._instances.clear()\n    pbar = tqdm(total=len(df))\n    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():\n        paper = papers[id]\n\n        labels = dataset_label.split('|')\n        labels = [clean_training_text(label) for label in labels]\n\n        sentences = set([clean_training_text(sentence) for section in paper \n                     for sentence in section['text'].split('.')])\n        sentences = shorten_sentences(sentences) \n        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n\n        # positive sample\n        for sentence in sentences:\n            is_positive, tags = tag_sentence(sentence, labels, classlabel=classlabel)\n            if is_positive:\n                cnt_pos += 1\n                ner_data.append(tags)\n            elif any(word in sentence.lower() for word in ['data', 'study']): \n                ner_data.append(tags)\n                cnt_neg += 1\n\n        pbar.update(1)\n        pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n#         print(f\"\\rProcessing paper {i:05d} / {len(df)}. Training data size: {cnt_pos} positives + {cnt_neg} negatives\", \n#               flush=True, end='')\n\n    if shuffle:\n        random.shuffle(ner_data)\n    return cnt_pos, cnt_neg, ner_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Training data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [00:00<00:00, 110.41it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Postive count: 168.   Negative count: 2574\n[('These', 0), ('works', 0), ('include', 0), ('Coleman', 0), ('and', 0), ('Hoffer', 0), ('s', 0), ('findings', 0), ('regarding', 0), ('the', 0), ('social', 0), ('capital', 0), ('and', 0), ('functional', 0), ('community', 0), ('of', 0), ('Catholic', 0), ('schools', 0), ('Chubb', 0), ('and', 0), ('Moe', 0), ('s', 0), ('conclusions', 0), ('regarding', 0), ('the', 0), ('climate', 0), ('of', 0), ('collegiality', 0), ('and', 0), ('empowerment', 0), ('in', 0), ('Catholic', 0), ('schools', 0), ('the', 0), ('Rand', 0), ('study', 0), ('s', 0), ('focus', 0), ('on', 0), ('clarity', 0), ('of', 0), ('mission', 0), ('and', 0), ('local', 0), ('ownership', 0), ('by', 0), ('educators', 0), ('students', 0), ('and', 0), ('parents', 0), ('and', 0), ('Bryk', 0), ('Lee', 0), ('and', 0), ('Holland', 0), ('s', 0), ('emphasis', 0), ('on', 0), ('Catholic', 0), ('schools', 0), ('as', 0), ('voluntary', 0), ('communities', 0)]\nCPU times: user 951 ms, sys: 72 ms, total: 1.02 s\nWall time: 1.36 s\n"
    }
   ],
   "source": "%%time\ndf = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:100]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\nclasslabel = get_ner_classlabel()\ncnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False)\nprint(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\nprint(ner_data[250])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef write_ner_json(ner_data, pth=Path('train_ner.json')):\n    with open(pth, 'w') as f:\n        for row in ner_data:\n            words, nes = list(zip(*row))\n            row_json = {'tokens' : words, 'ner_tags' : nes}\n            json.dump(row_json, f)\n            f.write('\\n')    "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{\"tokens\": [\"There\", \"is\", \"no\", \"dataset\", \"here\"], \"ner_tags\": [0, 0, 0, 0, 0]}\n{\"tokens\": [\"Load\", \"the\", \"UN\", \"Trade\", \"Development\", \"into\", \"view\"], \"ner_tags\": [0, 0, 2, 1, 1, 0, 0]}\n"
    }
   ],
   "source": "ner_data = [\n    [('There', 0), ('is', 0), ('no', 0), ('dataset', 0), ('here', 0)], \n    [('Load', 0), ('the', 0), ('UN', 2), ('Trade', 1), ('Development', 1), ('into', 0), ('view', 0)]\n]\nwrite_ner_json(ner_data, pth=Path('/kaggle/tmp_ner.json'))\n! cat /kaggle/tmp_ner.json"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_ner_datasets(data_files=None):\n    datasets = load_dataset('json', data_files=data_files)\n    classlabel = get_ner_classlabel()\n    for split, dataset in datasets.items():\n        dataset.features['ner_tags'].feature = classlabel\n    return datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-32fb78d820344d9f/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-32fb78d820344d9f/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None)}\n\n{'tokens': ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view'], 'ner_tags': [0, 0, 2, 1, 1, 0, 0]}\n"
    }
   ],
   "source": "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\nprint(datasets['valid'].features)\nprint()\nprint(datasets['train'][1])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Training data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [00:19<00:00, 110.41it/s]"
    }
   ],
   "source": "#export\ndef create_tokenizer(model_checkpoint='bert-base-cased'):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n    return tokenizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33594e40c7ee414d8c33072d40d8caa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26125953d85c4c7398d78bf0a1dfbb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a956de59f643eebf9437b604974a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd78cb21c424b1281e2d90f3b0efd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n{'input_ids': [101, 138, 188, 21943, 9930, 1104, 1234, 9026, 1121, 1103, 2286, 6194, 3499, 1113, 6356, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n{'input_ids': [101, 144, 6512, 9436, 24372, 1317, 185, 12937, 2042, 15520, 1114, 8626, 2330, 1447, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
    }
   ],
   "source": "tokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\nprint(\n    tokenizer(\"A smattering of people descended from the midday boat on Monday.\"))\nprint()\nprint(\n    tokenizer(\"Giglio boasts several pristine bays with crystal clear water\".split(), is_split_into_words=True)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef tokenize_and_align_labels(examples, tokenizer=None, label_all_tokens=True):\n    '''\n    Adds a new field called 'labels' that are the NER tags to the tokenized input.\n    \n    Args:\n        tokenizer (transformers.AutoTokenizer): Tokenizer.\n        examples (datasets.arrow_dataset.Dataset): Dataset.\n        label_all_tokens (bool): If True, all sub-tokens are given the same tag as the \n            first sub-token, otherwise all but the first sub-token are given the tag\n            -100.\n    '''\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(label[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]]}\n\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a747ce3d075495dac423aaac185aa90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d3a8bd2a5f40fa83a4091b3bc0ba10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'ner_tags': [[0, 0, 0, 0, 0], [0, 0, 2, 1, 1, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tokens': [['There', 'is', 'no', 'dataset', 'here'], ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view']]}\n\n{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'ner_tags': [[0, 0, 0, 0, 0], [0, 0, 2, 1, 1, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tokens': [['There', 'is', 'no', 'dataset', 'here'], ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view']]}\n{'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
    }
   ],
   "source": "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\ntokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\n\nprint(tokenize_and_align_labels(datasets['train'][:], tokenizer, label_all_tokens=True), end='\\n\\n')\n\ntokenized_datasets = datasets.map(\n    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\n\nprint(tokenized_datasets['train'][:], end='\\n\\n')\nprint(tokenized_datasets['valid'][:])\nprint(tokenized_datasets['train'].features)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd88270702754651a773dc062ca29770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961.0, style=ProgressStyle(description…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "text/plain": "{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n 'overall_precision': 0.0,\n 'overall_recall': 0.0,\n 'overall_f1': 0.0,\n 'overall_accuracy': 0.3333333333333333}"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "metric = load_metric('seqeval')\n\npredictions = [['O', 'O', 'B', 'I', 'I', 'O']]\nreferences = [['B', 'I', 'B', 'O', 'O', 'O']]\nmetric.compute(predictions=predictions, references=references)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef jaccard_similarity(s1, s2):\n    l1 = set(s1.split(\" \"))\n    l2 = set(s2.split(\" \"))\n    intersection = len(list(l1.intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "jaccard_similarity('USGS Frog Counts Data', 'USGA Croc Counts Data') == 1 / 3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef compute_metrics(p, metric=None, label_list=None):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'precision': 0.17699115044247787,\n 'recall': 0.19801980198019803,\n 'f1': 0.18691588785046728,\n 'accuracy': 0.2708333333333333}"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\n\nbatch_size = 8\nmax_sentence_length = 30\n\npredictions = np.random.randn(batch_size, max_sentence_length, classlabel.num_classes)\nlabel_ids = np.random.randint(low=0, high=classlabel.num_classes, \n                              size=(batch_size, max_sentence_length), dtype=np.int16)\n\np = (predictions, label_ids)\nmetric = load_metric('seqeval')\ncompute_metrics(p, metric=metric, label_list=classlabel.names)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## NER training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Training data size: 12 positives + 100 negatives: 100%|██████████| 2/2 [00:00<00:00, 37.69it/s] \nTraining data size: 2 positives + 51 negatives: 100%|██████████| 2/2 [00:00<00:00, 62.60it/s] "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train.  Positive count: 12.  Negative count: 100.\nValid.  Positive count: 2.  Negative count: 51.\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\ntrain_meta = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:4]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', train_meta.Id)\n\nvalid_cutoff = int(.50 * len(train_meta))\nvalid_meta = train_meta.iloc[:valid_cutoff].reset_index(drop=True)\ntrain_meta = train_meta.iloc[valid_cutoff:].reset_index(drop=True)\n\ntrain_cnt_pos, train_cnt_neg, train_ner_data = get_ner_data(papers, df=train_meta, classlabel=classlabel)\nvalid_cnt_pos, valid_cnt_neg, valid_ner_data = get_ner_data(papers, df=valid_meta, classlabel=classlabel)\nprint(f'Train.  Positive count: {train_cnt_pos}.  Negative count: {train_cnt_neg}.')\nprint(f'Valid.  Positive count: {valid_cnt_pos}.  Negative count: {valid_cnt_neg}.')\n\nwrite_ner_json(train_ner_data, pth='train_ner.json')\nwrite_ner_json(valid_ner_data, pth='valid_ner.json')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-fa9b3b0ccda83360/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-fa9b3b0ccda83360/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7572e8d78d0e4b4a9f97d74e0eceb105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb328d563eb74f558cd7c74acfcea1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9868f83852c4ebaba7338bd28cce00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cad1eadf0a42e2b76c257bb1b432e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b4714da30e472b8474198203f7e12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101bb2822e0f4f9f9df0b38dabe936d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb19b612dced4f57bce6eb8132445e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descri…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [01:24<00:00,  1.18it/s] \n"
    }
   ],
   "source": "datasets = load_ner_datasets(data_files={'train':'train_ner.json', 'valid':'valid_ner.json'})\n\nmodel_checkpoint = 'distilbert-base-cased'\ntokenizer = create_tokenizer(model_checkpoint)\ntokenized_datasets = datasets.map(\n    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=classlabel.num_classes)\n\nmetric = load_metric('seqeval')\n\nargs = TrainingArguments(output_dir='test_ner', num_train_epochs=2, \n                         learning_rate=2e-5, weight_decay=0.01,\n                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n                         save_strategy='epoch', save_total_limit=6)\n\ntrainer = Trainer(model=model, args=args, \n                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n                  data_collator=data_collator, tokenizer=tokenizer, \n                  compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14/14 01:15, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.750600</td>\n      <td>0.311518</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.988406</td>\n      <td>4.726900</td>\n      <td>11.212000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.198500</td>\n      <td>0.177066</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.988406</td>\n      <td>4.631100</td>\n      <td>11.444000</td>\n    </tr>\n  </tbody>\n</table><p>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=14, training_loss=0.4010348490306309, metrics={'train_runtime': 80.5559, 'train_samples_per_second': 0.174, 'total_flos': 5338542317472.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 919347200, 'train_mem_cpu_peaked_delta': 839835648})"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "trainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "total 8\ndrwxr-xr-x 2 root root 4096 May 25 04:36 checkpoint-7\ndrwxr-xr-x 2 root root 4096 May 25 04:37 checkpoint-14\n"
    }
   ],
   "source": "! ls -lrt test_ner"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 01:13, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>3</td>\n      <td>0.114600</td>\n      <td>0.096661</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.988406</td>\n      <td>4.651500</td>\n      <td>11.394000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.096200</td>\n      <td>0.085153</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.988406</td>\n      <td>4.495100</td>\n      <td>11.791000</td>\n    </tr>\n  </tbody>\n</table><p>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=28, training_loss=0.04629874921270779, metrics={'train_runtime': 78.1778, 'train_samples_per_second': 0.358, 'total_flos': 10564430752512.0, 'epoch': 4.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1171316736, 'train_mem_cpu_peaked_delta': 633868288})"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "args = TrainingArguments(output_dir='test_ner', num_train_epochs=4, \n                         learning_rate=2e-5, weight_decay=0.01,\n                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n                         save_strategy='epoch', save_total_limit=6)\n\ntrainer = Trainer(model=model, args=args, \n                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n                  data_collator=data_collator, tokenizer=tokenizer, \n                  compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))\ntrainer.train(resume_from_checkpoint='/kaggle/working/test_ner/checkpoint-14/')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='8' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:17]\n    </div>\n    ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.08515263348817825,\n 'eval_precision': 0.0,\n 'eval_recall': 0.0,\n 'eval_f1': 0.0,\n 'eval_accuracy': 0.9884057971014493,\n 'eval_runtime': 4.6291,\n 'eval_samples_per_second': 11.449,\n 'epoch': 4.0,\n 'eval_mem_cpu_alloc_delta': 32768,\n 'eval_mem_cpu_peaked_delta': 0}"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "trainer.evaluate()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 4}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.9884057971014493}\n"
    }
   ],
   "source": "predictions, label_ids, _ = trainer.predict(tokenized_datasets['valid'])\npredictions = np.argmax(predictions, axis=2)\n\ntrue_predictions = [[classlabel.names[p] for p, i in zip(prediction, label_id) if i != -100]\n                    for prediction, label_id in zip(predictions, label_ids)]\n\ntrue_labels = [[classlabel.names[i] for i in label_id if i != -100] for label_id in label_ids]\n\nprint(true_predictions[0])\nprint(true_labels[0])\n\nresults = metric.compute(predictions=true_predictions, references=true_labels)\nprint(results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## NER inference\n\n**Turn off the Internet here**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_ner_inference_data(papers, sample_submission, classlabel=None):\n    '''\n    Args:\n        papers (dict): Each list in this dictionary consists of the section of a paper.\n        sample_submission (pd.DataFrame): Competition 'sample_submission.csv'.\n    Returns:\n        test_rows (list): Each list in this list is of the form: \n             [('goat', 0), ('win', 0), ...] and represents a sentence.  \n        paper_length (list): Number of sentences in each paper.\n    '''\n    test_rows = [] # test data in NER format\n    paper_length = [] # store the number of sentences each paper has\n\n    for paper_id in sample_submission['Id']:\n        # load paper\n        paper = papers[paper_id]\n\n        # extract sentences\n        sentences = [clean_training_text(sentence) for section in paper \n                     for sentence in section['text'].split('.')\n                    ]\n        sentences = shorten_sentences(sentences) # make sentences short\n        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n\n        # collect all sentences in json\n        for sentence in sentences:\n            sentence_words = sentence.split()\n            dummy_tags = [classlabel.str2int('O')]*len(sentence_words)\n            test_rows.append(list(zip(sentence_words, dummy_tags)))\n#             test_rows.append({'tokens' : sentence_words, 'ner_tags' : dummy_tags})\n\n        # track which sentence belongs to which data point\n        paper_length.append(len(sentences))\n\n    print(f'total number of sentences: {len(test_rows)}')\n    return test_rows, paper_length"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "total number of sentences: 367\n[[('A', 0), ('recent', 0), ('large', 0), ('genomewide', 0), ('association', 0), ('study', 0), ('GWAS', 0), ('reported', 0), ('a', 0), ('genome', 0), ('wide', 0), ('significant', 0), ('locus', 0), ('for', 0), ('years', 0), ('of', 0), ('education', 0), ('which', 0), ('subsequently', 0), ('demonstrated', 0), ('association', 0), ('to', 0), ('general', 0), ('cognitive', 0), ('ability', 0), ('g', 0), ('in', 0), ('overlapping', 0), ('cohorts', 0)], [('The', 0), ('current', 0), ('study', 0), ('was', 0), ('designed', 0), ('to', 0), ('test', 0), ('whether', 0), ('GWAS', 0), ('hits', 0), ('for', 0), ('educational', 0), ('attainment', 0), ('are', 0), ('involved', 0), ('in', 0), ('general', 0), ('cognitive', 0), ('ability', 0), ('in', 0), ('an', 0), ('independent', 0), ('large', 0), ('scale', 0), ('collection', 0), ('of', 0), ('cohorts', 0)], [('We', 0), ('next', 0), ('conducted', 0), ('meta', 0), ('analyses', 0), ('with', 0), ('24', 0), ('189', 0), ('individuals', 0), ('with', 0), ('neurocognitive', 0), ('data', 0), ('from', 0), ('the', 0), ('educational', 0), ('attainment', 0), ('studies', 0), ('and', 0), ('then', 0), ('with', 0), ('53', 0), ('188', 0), ('largely', 0), ('independent', 0), ('individuals', 0), ('from', 0), ('a', 0), ('recent', 0), ('GWAS', 0), ('of', 0), ('cognition', 0)]]\n[34, 151, 98, 84]\n"
    }
   ],
   "source": "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test', sample_submission.Id)\nclasslabel = get_ner_classlabel()\ntest_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel)\nprint(test_rows[:3])\nprint(paper_length)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef ner_predict(test_rows, tokenizer=None, model=None, metric=None):\n    classlabel = get_ner_classlabel()\n    \n    write_ner_json(test_rows, pth='test_ner.json')\n    datasets = load_ner_datasets(data_files={'test':'test_ner.json'})\n    print('Tokenizing testset...')\n    tokenized_datasets = datasets.map(\n        partial(tokenize_and_align_labels,tokenizer=tokenizer, label_all_tokens=True), \n        batched=True) \n\n    print('Creating data collator...')\n    data_collator = DataCollatorForTokenClassification(tokenizer)\n    print('Creating (dummy) training arguments...')\n    args = TrainingArguments(output_dir='test_ner', num_train_epochs=3, \n                             learning_rate=2e-5, weight_decay=0.01,\n                             per_device_train_batch_size=16, per_device_eval_batch_size=16,\n                             evaluation_strategy='epoch', logging_steps=4, report_to='none', \n                             save_strategy='epoch', save_total_limit=6)\n\n    print('Creating trainer...')\n    trainer = Trainer(model=model, args=args, \n                      train_dataset=tokenized_datasets['test'], eval_dataset=tokenized_datasets['test'], \n                      data_collator=data_collator, tokenizer=tokenizer, \n                      compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))\n\n    print('Predicting on test samples...')\n    predictions, label_ids, _ = trainer.predict(tokenized_datasets['test'])\n    predictions = predictions.argmax(axis=2)\n    true_predictions = [\n        [p for p, i in zip(prediction, label_id) if i != -100]\n        for prediction, label_id in zip(predictions, label_ids)]\n\n    return true_predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_checkpoint = 'test_ner/checkpoint-28/'\n\ntokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\nmetric = load_metric('seqeval')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-f1c6cdee082b7a5a/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f1c6cdee082b7a5a/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\nTokenizing testset...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86a18e3cf394082b21d3890e3219cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nCreating data collator...\nCreating (dummy) training arguments...\nCreating trainer...\nPredicting on test samples...\n"
    },
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [23/23 00:29]\n    </div>\n    ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n  avg = a.mean(axis)\n/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
    }
   ],
   "source": "bert_outputs = ner_predict(test_rows, tokenizer=tokenizer, model=model, metric=metric)\nprint(bert_outputs[0])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# predict_batch = 100\n\n# bert_outputs = []\n# for i in range(0, len(test_rows), predict_batch):\n#     print(f'\\rPredicting on samples {i} to {i + predict_batch}...', flush=True)\n#     b = ner_predict(test_rows[i:i + predict_batch], model=model, tokenizer=tokenizer, metric=metric)\n#     bert_outputs.append(b)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# def kaggle_run_ner_predict(model_name_or_path='/kaggle/input/coleridge-bert-models/output', \n#                            train_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n#                            validation_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n#                            test_file='./input_data/test_ner_input.json', \n#                            output_dir='./pred'):\n#     '''\n#     Args:\n#         test_file (Path, str): Path to json file in which each row represents an input\n#             sample to the model (representing a sentence in this context).  Each row\n#             is a dictionary of the form:\n#             {'tokens': ['hi', 'there', ...], 'tags': ['O', 'O', ...]}\n#         output_dir (Path, str): Path to the directory in which prediction results are saved.\n#     '''\n#     os.environ[\"MODEL_PATH\"] = f\"{model_name_or_path}\"\n#     os.environ[\"TRAIN_FILE\"] = f\"{train_file}\"\n#     os.environ[\"VALIDATION_FILE\"] = f\"{validation_file}\"\n#     os.environ[\"TEST_FILE\"] = f\"{test_file}\"\n#     os.environ[\"OUTPUT_DIR\"] = f\"{output_dir}\"\n    \n#     ! python /kaggle/input/kaggle-ner-utils/kaggle_run_ner.py \\\n#     --model_name_or_path \"$MODEL_PATH\" \\\n#     --validation_file \"$VALIDATION_FILE\" \\\n#     --train_file \"$TRAIN_FILE\" \\\n#     --test_file \"$TEST_FILE\" \\\n#     --output_dir \"$OUTPUT_DIR\" \\\n#     --report_to 'none' \\\n#     --seed 123 \\\n#     --do_predict\n\n# def run_inference(test_rows, predict_batch=64_000, \n#                   model_name_or_path='/kaggle/input/coleridge-bert-models/output', \n#                   train_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n#                   validation_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n#                   test_file='./input_data/test_ner_input.json', \n#                   output_dir='./pred'):\n#     '''\n#     '''\n#     test_file = Path(test_file)\n#     test_file.parent.mkdir(exist_ok=True, parents=True)\n    \n#     bert_outputs = []\n#     for batch_begin in range(0, len(test_rows), predict_batch):\n#         # write data rows to input file\n#         with open(test_file, 'w') as f:\n#             for row in test_rows[batch_begin:batch_begin + predict_batch]:\n#                 json.dump(row, f)\n#                 f.write('\\n')\n\n#         # remove output dir\n#         if os.path.exists(output_dir):\n#             shutil.rmtree(output_dir)\n\n#         # do predict\n#         kaggle_run_ner_predict(\n#             model_name_or_path=model_name_or_path, \n#             train_file=train_file, validation_file=validation_file, test_file=test_file, \n#             output_dir=output_dir)\n\n#         # read predictions\n#         with open(f'{output_dir}/test_predictions.txt') as f:\n#             this_preds = f.read().split('\\n')[:-1]\n#             bert_outputs += [pred.split() for pred in this_preds]\n#     return bert_outputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# predict_batch = 64_000 \n\n# model_name_or_path = '/kaggle/working/output/' #'/kaggle/input/coleridge-bert-models/output'\n# test_file = './input_data/test_ner_input.json'\n# train_file = 'train_ner.json' #'/kaggle/input/coleridge-bert-models/train_ner.json'\n# validation_file = 'train_ner.json' #'/kaggle/input/coleridge-bert-models/train_ner.json'\n# output_dir = './pred'\n\n# bert_outputs = run_inference(test_rows, predict_batch=predict_batch, \n#                              model_name_or_path=model_name_or_path, \n#                              test_file=test_file, train_file=train_file, validation_file=validation_file,\n#                              output_dir=output_dir)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=None):\n    '''\n    Returns:\n        bert_dataset_labels (list): Each element is a set consisting of labels predicted\n            by the model.\n    '''\n    test_sentences = [list(zip(*row))[0] for row in test_rows]\n#     test_sentences = [row['tokens'] for row in test_rows]\n    \n    bert_dataset_labels = [] # store all dataset labels for each publication\n\n    for length in paper_length:\n        labels = set()\n        for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n            curr_phrase = ''\n            for word, tag in zip(sentence, pred):\n                if tag == classlabel.str2int('B'): # start a new phrase\n                    if curr_phrase:\n                        labels.add(curr_phrase)\n                        curr_phrase = ''\n                    curr_phrase = word\n                elif tag == classlabel.str2int('I') and curr_phrase: # continue the phrase\n                    curr_phrase += ' ' + word\n                else: # end last phrase (if any)\n                    if curr_phrase:\n                        labels.add(curr_phrase)\n                        curr_phrase = ''\n            # check if the label is the suffix of the sentence\n            if curr_phrase:\n                labels.add(curr_phrase)\n                curr_phrase = ''\n\n        # record dataset labels for this publication\n        bert_dataset_labels.append(labels)\n\n        del test_sentences[:length], bert_outputs[:length]\n        \n    return bert_dataset_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[{'present all the', 'Tigers EcoNAX dataset'}, {'WGS Equality Definitiveness Dataset'}]\n"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\nsentences = ['They do not present all the features', \n             'Despite the pretraining on the Tigers EcoNAX dataset',\n             'Weirdly there has been lots of studies based on WGS Equality Definitiveness Dataset']\npaper_length = [2, 1]\ntest_rows = [[(word, 0) for word in sentence.split()] for sentence in sentences]\nbert_outputs = [[0, 0, 0, 2, 1, 1, 0],\n                [0, 0, 0, 0, 0, 2, 1, 1],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1]]\nfor i, row in enumerate(test_rows):\n    assert len(row) == len(bert_outputs[i])\n\nbert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=classlabel)\nprint(bert_dataset_labels)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef filter_bert_labels(bert_dataset_labels):\n    '''\n    When several labels for a paper are too similar, keep just one of them.\n    '''\n    filtered_bert_labels = []\n\n    for labels in bert_dataset_labels:\n        filtered = []\n\n        for label in sorted(labels, key=len):\n            label = clean_training_text(label, lower=True)\n            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n                filtered.append(label)\n\n        filtered_bert_labels.append('|'.join(filtered))\n    return filtered_bert_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['moma artists|housing market|moma artists catalogue',\n 'deep sea rock salts|rhs fertiliser index']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "bert_dataset_labels = [{'moma artists catalogue', 'moma artists', 'housing market'},\n                       {'rhs flowers fertiliser index', 'deep sea rock salts', 'rhs fertiliser index'}]\n\nfilter_bert_labels(bert_dataset_labels)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Literal matching"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef create_knowledge_bank(pth):\n    '''\n    Args:\n        pth (str): Path to meta data like 'train.csv', which\n        needs to have columns: 'dataset_title', 'dataset_label', and 'cleaned_label'.\n        \n    Returns:\n        all_labels (set): All possible strings associated with a dataset from the meta data.\n    '''\n    df = load_train_meta(pth, group_id=False)\n    all_labels = set()\n    for label_1, label_2, label_3 in df[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n        all_labels.add(str(label_1).lower())\n        all_labels.add(str(label_2).lower())\n        all_labels.add(str(label_3).lower())\n    return all_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "180\n['2019 ncov complete genome sequences', '2019 ncov genome sequence', '2019 ncov genome sequences', '2019-ncov complete genome sequences', '2019-ncov genome sequence', '2019-ncov genome sequences', 'adni', 'advanced national seismic system (anss) comprehensive catalog (comcat)', 'advanced national seismic system anss comprehensive catalog comcat ', 'advanced national seismic system comprehensive catalog']\n"
    }
   ],
   "source": "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\nall_labels = create_knowledge_bank(pth)\nprint(len(all_labels))\nprint(sorted(all_labels)[:10])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef literal_match(paper, all_labels):\n    '''\n    Args:\n        paper ()\n    '''\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = clean_training_text(text_1, lower=True, total_clean=True)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_training_text(label, lower=True, total_clean=True))\n    return labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['adni|alzheimer s disease neuroimaging initiative adni',\n 'trends in international mathematics and science study|common core of data|nces common core of data',\n 'slosh model|noaa storm surge inundation|sea lake and overland surges from hurricanes',\n 'rural urban continuum codes']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n\npth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\nall_labels = create_knowledge_bank(pth)\n\nliteral_preds = []\nfor paper_id in sample_submission.Id:\n    paper = papers[paper_id]\n    literal_preds.append('|'.join(literal_match(paper, all_labels)))\n    \nliteral_preds"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Overall prediction for submission"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef combine_matching_and_bert(literal_preds, filtererd_bert_labels):\n    '''\n    For a given sentence, if there's a literal match, use that as the final\n    prediction for the sentence.  If there isn't a literal match,\n    use what the model predicts.\n    '''\n    final_predictions = []\n    for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n        if literal_match:\n            final_predictions.append(literal_match)\n        else:\n            final_predictions.append(bert_pred)\n    return final_predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['mongolian racing cars|reallife headphones',\n 'hifi dataset|headphones collection data']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "literal_preds = ['mongolian racing cars|reallife headphones', '']\nfiltered_bert_labels = ['data|dataset', 'hifi dataset|headphones collection data']\ncombine_matching_and_bert(literal_preds, filtered_bert_labels)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Preparing NER inference data...\ntotal number of sentences: 367\nLoading model, tokenizer, and metric...\nPredicting on each sentence...\nDownloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-e4660346708e43b6/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e4660346708e43b6/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\nTokenizing testset...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05adc860268c4d6db4a735a17f80c34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nCreating data collator...\nCreating (dummy) training arguments...\nCreating trainer...\nPredicting on test samples...\n"
    },
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [23/23 00:30]\n    </div>\n    ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Getting predicted labels for each article...\nKeep just one of labels that are too similar...\n"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\n\nprint('Preparing NER inference data...')\nsample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\ntest_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel)\n\nprint('Loading model, tokenizer, and metric...')\nmodel_checkpoint = 'test_ner/checkpoint-28/'\ntokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\nmetric = load_metric('seqeval')\n\nprint('Predicting on each sentence...')\nbert_outputs = ner_predict(test_rows, tokenizer=tokenizer, model=model, metric=metric)\n\nprint('Getting predicted labels for each article...')\nbert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=classlabel)\n\nprint('Keeping just one of labels that are too similar...')\nfiltered_bert_labels = filter_bert_labels(bert_dataset_labels)\n\nsample_submission['PredictionString'] = filtered_bert_labels\n\nsample_submission.to_csv('submission.csv', index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                     Id PredictionString\n0  2100032a-7c33-4bff-97ef-690822c43466                 \n1  2f392438-e215-4169-bebf-21ac4ff253e1                 \n2  3f316b38-1a24-45a9-8d8c-4e05a42257c6                 \n3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60                 "
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sample_submission.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Reference\n- https://www.kaggle.com/tungmphung/pytorch-bert-for-named-entity-recognition/notebook\n- https://www.kaggle.com/tungmphung/coleridge-matching-bert-ner/notebook\n- https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n- https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
