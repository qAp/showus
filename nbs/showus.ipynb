{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "entitled-analyst",
   "metadata": {
    "tags": []
   },
   "source": [
    "# showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-final",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#default_exp showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-advice",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\r\n",
      "Installing collected packages: fsspec\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 0.8.7\r\n",
      "    Uninstalling fsspec-0.8.7:\r\n",
      "      Successfully uninstalled fsspec-0.8.7\r\n",
      "Successfully installed fsspec-2021.4.0\r\n",
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.59.0\r\n",
      "    Uninstalling tqdm-4.59.0:\r\n",
      "      Successfully uninstalled tqdm-4.59.0\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Installing collected packages: tokenizers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.10.2\r\n",
      "    Uninstalling tokenizers-0.10.2:\r\n",
      "      Successfully uninstalled tokenizers-0.10.2\r\n",
      "Successfully installed tokenizers-0.10.1\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.45)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2021.3.17)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.5.1\r\n",
      "    Uninstalling transformers-4.5.1:\r\n",
      "      Successfully uninstalled transformers-4.5.1\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\n",
    "! pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n",
    "! pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n",
    "! pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n",
    "! pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-fitting",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from functools import partial\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers, seqeval\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset, ClassLabel, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-massachusetts",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-responsibility",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "Path.ls = lambda pth: list(pth.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-tumor",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-angola",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_train_meta(pth, group_id=True):\n",
    "    df = pd.read_csv(pth)\n",
    "    if group_id:\n",
    "        df = df.groupby('Id').agg({'pub_title': 'first', 'dataset_title': '|'.join, \n",
    "                                   'dataset_label': '|'.join, 'cleaned_label': '|'.join}).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-election",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14316 19661\n",
      "['Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " 'Beginning Postsecondary Students Longitudinal Study|Education Longitudinal Study|Beginning Postsecondary Students'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Beginning Postsecondary Student|Beginning Postsecondary Students']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "df = load_train_meta(pth, group_id=True)\n",
    "df_nogroup = load_train_meta(pth, group_id=False)\n",
    "print(len(df), len(df_nogroup))\n",
    "dup_ids = df_nogroup[df_nogroup.Id.duplicated()].Id.unique()\n",
    "print(df[df.Id.isin(dup_ids)].dataset_label.values[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-disclosure",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_papers(dir_json, paper_ids):\n",
    "    '''\n",
    "    Load papers into a dictionary.\n",
    "    \n",
    "    `papers`: \n",
    "        {''}\n",
    "    '''\n",
    "    \n",
    "    papers = {}\n",
    "    for paper_id in paper_ids:\n",
    "        with open(f'{dir_json}/{paper_id}.json', 'r') as f:\n",
    "            paper = json.load(f)\n",
    "            papers[paper_id] = paper\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-expense",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'section_title': 'Abstract', 'text': 'Mathematics, as no other school subject, evokes conflicting emotions and contradictory attitudes -from \"the gate to a career\" and \"the queen of science\" to the widespread acceptance of mathematical ignorance in society. The process of studying mathematics requires systematic work and patience, as mathematical knowledge has a cumulative nature. In the case of mathematics education, some students abandon mathematics at quite early levels of education and begin to consider themselves \"humanists\", which results in serious consequences for future educational and career choices. In this paper, I propose a description of the process of escaping from mathematics in the context of students\\' perceptions of this subject, using the results of two studies -one qualitative and the other quantitative.'}\n"
     ]
    }
   ],
   "source": [
    "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[-10:]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "print(type(papers))\n",
    "print(\n",
    "    papers[ np.random.choice(df.Id.values) ][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-option",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.dataset_label.iloc[3]\n",
    "\n",
    "# paper = papers[df.Id.iloc[4]]\n",
    "# ['\\n'.join([section['section_title'], section['text']]).split() for section in paper if section['text']][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-endorsement",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_sample_text(jpth):\n",
    "    sections = json.loads(jpth.read_text())\n",
    "    text = '\\n'.join(section['text'] for section in sections)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-component",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s. ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data. Creating a U.S. crosswalk to this system has been a goal of the National Center for Education Statistics and the Office of Research since the late 197,,s, when the National Institute of Education (the predecessor agency to the Office of Educational Research and Improvement) began exploring the idea. The design and implementation of a workable crosswalk, however, awaited the advent of changes to the Classification of Instructional Programs (CIP) system. The 1990 revision of the CIP system laid the foundation for a workable international crosswalk. Adoption of the National Education Goals set global consciousness and international educational comparisons firml\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "print(load_sample_text(jpths_trn[0])[:1_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-provision",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-burner",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_training_text(txt, lower=False, total_clean=False):\n",
    "    \"\"\"\n",
    "    similar to the default clean_text function but without lowercasing.\n",
    "    \"\"\"\n",
    "    txt = str(txt).lower() if lower else str(txt)\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt).strip()\n",
    "    if total_clean:\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-tension",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle This competition awards 90 000\n",
      "hopkld 7 11 002\n"
     ]
    }
   ],
   "source": [
    "print(clean_training_text('@kaggle This competition awards $90,000!!!!.'))\n",
    "print(clean_training_text('HoPKLd + 7 ! 11,002', total_clean=True, lower=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-remainder",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def shorten_sentences(sentences, max_length=64, overlap=20):\n",
    "    '''\n",
    "    Args:\n",
    "        sentences (list): List of sentences.\n",
    "        max_length (int): Maximum number of words allowed for each sentence.\n",
    "        overlap (int): If a sentence exceeds `max_length`, we split it to multiple sentences with \n",
    "            this amount of overlapping.\n",
    "    '''\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > max_length:\n",
    "            for p in range(0, len(words), max_length - overlap):\n",
    "                short_sentences.append(' '.join(words[p:p+max_length]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-african",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s', ' ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data']\n",
      "\n",
      "After: ['The International Standard Classification of Education, known by its acronym', 'its acronym ISCED, was developed by the United Nations Educational,', 'Nations Educational, Scientific, and Cultural Organization during the late 1960s', 'late 1960s and 1970s', 'ISCED was implemented in 1976 and is the recognized international', 'recognized international standard for reporting and interpreting education program data', 'program data']\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "sentences = load_sample_text(jpths_trn[0]).split('.')[:2]\n",
    "short_sentences = shorten_sentences(sentences, max_length=10, overlap=2)\n",
    "print('Before:', sentences)\n",
    "print()\n",
    "print('After:', short_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-costs",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def find_sublist(big_list, small_list):\n",
    "    all_positions = []\n",
    "    for i in range(len(big_list) - len(small_list) + 1):\n",
    "        if small_list == big_list[i:i+len(small_list)]:\n",
    "            all_positions.append(i)\n",
    "    \n",
    "    return all_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-courage",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list = ['If', 'the', 'thing', 'above', 'is', 'below', 'that', 'thing', 'which', 'is',\n",
    "            'not', 'as', 'high', 'up', 'on', 'the', 'thing', 'above', 'when', 'it', 'is', \n",
    "            'underneath', 'them.']\n",
    "small_list = ['the', 'thing', 'above']\n",
    "\n",
    "find_sublist(big_list, small_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-beauty",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-advisory",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_classlabel():\n",
    "    '''\n",
    "    Labels for named entity recognition.\n",
    "        'O': Token not part of a phrase that mentions a dataset.\n",
    "        'I': Intermediate token of a phrase mentioning a dataset.\n",
    "        'B': First token of a phrase mentioning a dataset.\n",
    "    '''\n",
    "    return ClassLabel(names=['O', 'I', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-frequency",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None)\n",
      "[1, 0, 2] 1\n",
      "B ['B', 'I', 'O']\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "print(classlabel)\n",
    "print(classlabel.str2int(['I', 'O', 'B']), classlabel.str2int('I'))\n",
    "print(classlabel.int2str(2), classlabel.int2str([2, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-three",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tag_sentence(sentence, labels, classlabel=None): \n",
    "    '''\n",
    "    requirement: both sentence and labels are already cleaned\n",
    "    '''\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n",
    "                                  for label in labels): # positive sample\n",
    "        nes = [classlabel.str2int('O')] * len(sentence_words)\n",
    "        for label in labels:\n",
    "            label_words = label.split()\n",
    "\n",
    "            all_pos = find_sublist(sentence_words, label_words)\n",
    "            for pos in all_pos:\n",
    "                nes[pos] = classlabel.str2int('B')\n",
    "                for i in range(pos+1, pos+len(label_words)):\n",
    "                    nes[i] = classlabel.str2int('I')\n",
    "\n",
    "        return True, list(zip(sentence_words, nes))\n",
    "        \n",
    "    else: # negative sample\n",
    "        nes = [classlabel.str2int('O')] * len(sentence_words)\n",
    "        return False, list(zip(sentence_words, nes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-documentation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A label is found in the sentence: True\n",
      "(token, tag) pairs:\n",
      "[('The', 2), ('International', 1), ('Standard', 0), ('Classification', 0), ('of', 0), ('Education', 0), ('known', 0), ('by', 0), ('its', 0), ('acronym', 0), ('ISCED', 0), ('was', 0), ('developed', 0), ('by', 0), ('the', 0), ('United', 2), ('Nations', 1), ('Educational', 1), ('Scientific', 0), ('and', 0), ('Cultural', 2), ('Organization', 1), ('during', 0), ('the', 0), ('late', 0), ('1960s', 0), ('and', 0), ('1970s', 0)]\n"
     ]
    }
   ],
   "source": [
    "sentence = (\"The International Standard Classification of Education, known by its acronym ISCED, \"\n",
    "            \"was developed by the United Nations Educational, \"\n",
    "            \"Scientific, and Cultural Organization during the late 1960s and 1970s\")\n",
    "labels = ['The International', 'Cultural Organization', 'United Nations Educational']\n",
    "\n",
    "sentence = clean_training_text(sentence)\n",
    "labels = [clean_training_text(label) for label in labels]\n",
    "classlabel = get_ner_classlabel()\n",
    "found_any, token_tags = tag_sentence(sentence, labels, classlabel=classlabel)\n",
    "\n",
    "print('A label is found in the sentence:', found_any)\n",
    "print('(token, tag) pairs:')\n",
    "print(token_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-garlic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_sentences(paper, sentence_definition='sentence'):\n",
    "    if sentence_definition == 'sentence':\n",
    "        sentences = set(clean_training_text(sentence) \n",
    "                        for sec in paper for sentence in sec['text'].split('.') if sec['text'])\n",
    "    elif sentence_definition == 'section':\n",
    "        sentences = set(clean_training_text(sec['section_title'] + '\\n' + sec['text']) \n",
    "                        for sec in paper if sec['text'])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-contents",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence definition = normal sentence\n",
      "302 ['', 'Charter schools are public schools that are founded by community leaders and parents', 'Section 6 summarizes the main findings addresses limitations and offers suggestions for future research', 'g', 'Card and Krueger 1992 and further educational attainment e', 'One additional factor that might affect test performance is the degree of urbanization', 'We explore this question most appropriately addressed from a longitudinal perspective in the next two subsections', 'First we ignore all non charter school intra system choices', 'The term traditional is not meant to suggest that charter schools necessarily adopt an atypical curriculum', 'Finally Greene and Forster 2002 report gains to competition as measured by an index of the distance between traditional schools and charters']\n",
      "\n",
      "Sentence definition = paper section\n",
      "12 ['Cross section regressions We begin by exploring cross section regression models Beyond the potential inconsistency from the use of the lagged dependent variable as a regressor this first econometric specification has two possible limitations First the lagged performance composite score proxying for ability is measured with error because the score is an imperfect measure of underlying student quality charter school placement and thus distance from the traditional school to the nearest charter school may be a function of traditional school quality For example charters may locate in areas with aboveaverage student quality in order to skim the high performing students and appear to be very effective In this situation a cross section regression would indicate a beneficial effect of charter school competition because the average traditional school facing competition has higher quality students than the average school not facing competition Alternatively charters may originate in locales in which parents are dissatisfied with the performance of the traditional school If students in these areas have below average achievement competition would appear detrimental to traditional school performance because the average school facing competition has lower quality students than the average school not facing competition We examine the possibility that charter schools are endogenously placed in cross section regressions in which distance is instrumented by county level factors thought to influence the location of charter schools in North Carolina 10 Over all distance variable specifications Hausman tests of exogeneity are insignificant implying that the distance measure can be treated as exogenous Bettinger 1999 used distance to the nearest public university for which the governor a charter school proponent appoints the board as an instrument for charter school competition motivated by the fact that in Michigan universities issue the charters for charter schools Since school systems issue the charters in North Carolina there is little theoretical justification for using a similar instrument in our regressions 11 Moreover North Carolina charter schools are quite heterogeneous in focus and history including some schools that converted from traditional public or private administration Thus there is no single model that explains how charter schools are instituted in North Carolina Therefore the remainder of the paper assumes that charter school placement and thus distance to the nearest charter school is exogneous The cross sectional models estimate the effect of five different distance measures Two of these are parametric the number of kilometers to the nearest charter school and the log of this measure We also explore three binary indicators Two of these signal whether the traditional school is within 10 or 20 km respectively of the nearest charter school 12 The third is an indicator for whether a charter school is located in the county that year Since our hypothesis predicts that traditional school quality will decrease as the distance to the nearest charter increases the predicted coefficients are negative for parametric measures and positive for the indicators Table 5 summarizes cross sectional regression results A separate regression is estimated for each of the three sample years and for each of the five distance measures resulting in fifteen separate models Eight of the fifteen distance estimates are significantly different from zero and all have the expected sign In the first and third year all three distance indicators are significant In 1997 98 the magnitude of the charter effect is invariant to whether the charter is within 10 or 20 km while in 1999 2000 the magnitude is twice as great for the shorter distance This pattern as well as the larger coefficient for charter in county in 1997 98 is consistent with the dramatic growth of the charter system during these two years in 1999 2000 more traditional schools were likely to have charters within the county and charters within 20 km In these 2 years the only parametric measure that is significant is the log of distance in 1999 2000 The results for 1998 99 are the opposite since distance is significant but no other measures are 13 11 We estimated IV models analogous to those of Bettinger 1999 Although distance to the nearest public university passed instrument validity tests Hausman tests failed to reject its exogeneity in the test performance equation 12 Similar results are found for the five fifteen and twenty five kilometer indicators 13 Schools may experience large shifts in enrollment if nearby schools open or close To test for robustness we run the IV models on the subsample of schools with yearly changes of enrollment of less than five percent The results are in general more statistically and economically significant The relevant policy question however is what happens to the quality of traditional schools after the onset of charter school competition We explore this question most appropriately addressed from a longitudinal perspective in the next two subsections Table 6 presents results from the Arellano Bond 1991 panel IV models Again we instrument the once lagged score with the twice lagged score to account for measurement error Model 1 uses the natural log of the kilometers from the nearest charter school to account for the degree of competition The estimated elasticity of Model 1 is 003 which is statistically insignificant', 'Maximum likelihood models Finally we specify the maximum likelihood model of Equation 8 which accounts for the timeinvariant unobserved characteristics as well as the initial condition The endogeneity is incorporated by points of support one point of support assumes that there is no time invariant heterogeneity As the points of support increase the model estimates more support points of the heterogeneity If heterogeneity exists then the one point of support model will yield inconsistent results Table 7 shows the results which are similar to those obtained in the cross sectional and panel IV regressions 15 Two distinct patterns emerge First the estimated effect of competition falls as distance increases as expected with just one exception the estimated effect of being 15 km is smaller than that of the estimated effect of begin within 20 km of a charter Traditional schools within 5 kilometers of a charter experience a one percent achievement gain traditional schools within 25 kilometers experience only about half of that gain Second unobserved heterogeneity has very little impact on the distance indicator coefficient estimates Although specification tests reject the parsimonious one point model which assumes that there is no unobserved heterogeneity the quantitative effect on the estimates of moving to the four point model is inconsequential']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[100:110]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', df.Id)\n",
    "paper = papers[df.Id.iloc[3]]\n",
    "print('Sentence definition = normal sentence')\n",
    "sentences = extract_sentences(paper, sentence_definition='sentence')\n",
    "print(len(sentences), list(sentences)[:10], end='\\n\\n')\n",
    "print('Sentence definition = paper section')\n",
    "sentences = extract_sentences(paper, sentence_definition='section')\n",
    "print(len(sentences), list(sentences)[:2], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-decline",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_paper_ner_data(paper, labels, classlabel=None,\n",
    "                       sentence_definition='sentence', max_length=64, overlap=20):\n",
    "    '''\n",
    "    Get NER data for a paper.\n",
    "    '''\n",
    "    labels = [clean_training_text(label) for label in labels]\n",
    "    sentences = extract_sentences(paper, sentence_definition=sentence_definition)\n",
    "    sentences = shorten_sentences(sentences, max_length=max_length, overlap=overlap) \n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "\n",
    "    cnt_pos, cnt_neg, ner_data = 0, 0, []\n",
    "    for sentence in sentences:\n",
    "        is_positive, tags = tag_sentence(sentence, labels, classlabel=classlabel)\n",
    "        if is_positive:\n",
    "            cnt_pos += 1\n",
    "            ner_data.append(tags)\n",
    "        elif any(word in sentence.lower() for word in ['data', 'study']): \n",
    "            ner_data.append(tags)\n",
    "            cnt_neg += 1    \n",
    "    return cnt_pos, cnt_neg, ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-payroll",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1\n",
      "[101, 512, 512, 49, 303, 512, 512, 512, 512, 259]\n",
      "[('as', 0), ('potential', 0), ('benefits', 0), ('implement', 0), ('lifestyle', 0), ('changes', 0), ('planning', 0), ('for', 0), ('the', 0), ('future', 0), ('eligibility', 0), ('for', 0), ('intervention', 0), ('trials', 0), ('Finally', 0), ('respondents', 0), ('questioned', 0), ('the', 0), ('validity', 0), ('of', 0), ('amyloid', 0), ('imaging', 0), ('results', 0), ('and', 0), ('called', 0), ('for', 0), ('more', 0), ('longitudinal', 0), ('data', 0), ('especially', 0), ('in', 0), ('cognitively', 0), ('normal', 0), ('individuals', 0), ('regarding', 0), ('risk', 0), ('of', 0), ('development', 0), ('of', 0), ('AD', 0), ('dementia', 0), ('Other', 0), ('topics', 0), ('respondents', 0), ('cited', 0), ('in', 0), ('need', 0), ('of', 0), ('research', 0), ('were', 0), ('solving', 0), ('ongoing', 0), ('technical', 0), ('issues', 0), ('related', 0), ('to', 0), ('the', 0), ('validity', 0), ('of', 0), ('amyloid', 0), ('imaging', 0), ('data', 0), ('determining', 0), ('cut', 0), ('off', 0), ('scores', 0), ('for', 0), ('positive', 0), ('negative', 0), ('results', 0), ('and', 0), ('addressing', 0), ('concerns', 0), ('about', 0), ('the', 0), ('stability', 0), ('of', 0), ('florbetapir', 0), ('vs', 0), ('other', 0), ('amyloid', 0), ('imaging', 0), ('tracers', 0), ('Respondents', 0), ('noted', 0), ('that', 0), ('disclosure', 0), ('of', 0), ('amyloid', 0), ('imaging', 0), ('results', 0), ('might', 0), ('serve', 0), ('as', 0), ('a', 0), ('useful', 0), ('way', 0), ('to', 0), ('help', 0), ('with', 0), ('recruitment', 0), ('and', 0), ('retention', 0), ('in', 0), ('ADNI', 2), ('providing', 0), ('participants', 0), ('with', 0), ('something', 0), ('in', 0), ('return', 0), ('for', 0), ('their', 0), ('time', 0), ('and', 0), ('effort', 0), ('Other', 0), ('respondents', 0), ('were', 0), ('concerned', 0), ('that', 0), ('disclosing', 0), ('the', 0), ('results', 0), ('of', 0), ('amyloid', 0), ('imaging', 0), ('especially', 0), ('in', 0), ('cognitively', 0), ('normal', 0), ('participants', 0), ('might', 0), ('affect', 0), ('the', 0), ('neuropsychometric', 0), ('or', 0), ('affective', 0), ('data', 0), ('subsequently', 0), ('collected', 0), ('Table', 0), ('2', 0), ('Respondents', 0), ('who', 0), ('have', 0), ('direct', 0), ('contact', 0), ('with', 0), ('ADNI', 2), ('participants', 0), ('n', 0), ('5', 0), ('139', 0), ('report', 0), ('of', 0), ('the', 0), ('number', 0), ('of', 0), ('participants', 0), ('with', 0), ('normal', 0), ('cognition', 0), ('or', 0), ('MCI', 0), ('who', 0), ('have', 0), ('requested', 0), ('their', 0), ('ADNI', 2), ('amyloid', 0), ('imaging', 0), ('results', 0), ('Respondents', 0), ('ethical', 0), ('arguments', 0), ('generally', 0), ('invoked', 0), ('ideas', 0), ('of', 0), ('respect', 0), ('for', 0), ('autonomy', 0), ('conditioned', 0), ('on', 0), ('the', 0), ('participant', 0), ('s', 0), ('clinical', 0), ('status', 0), ('Most', 0), ('frequently', 0), ('they', 0), ('argued', 0), ('that', 0), ('disclosure', 0), ('is', 0), ('appropriate', 0), ('for', 0), ('participants', 0), ('with', 0), ('cognitive', 0), ('impairment', 0), ('MCI', 0), ('or', 0), ('AD', 0), ('dementia', 0), ('but', 0), ('not', 0), ('appropriate', 0), ('for', 0), ('participants', 0), ('with', 0), ('normal', 0), ('cognition', 0), ('Some', 0), ('argued', 0), ('that', 0), ('participants', 0), ('have', 0), ('a', 0), ('right', 0), ('to', 0), ('know', 0), ('instead', 0), ('of', 0), ('researchers', 0), ('paternalistically', 0), ('withholding', 0), ('information', 0), ('from', 0), ('participants', 0), ('whereas', 0), ('others', 0), ('explained', 0), ('that', 0), ('participants', 0), ('also', 0), ('have', 0), ('a', 0), ('right', 0), ('not', 0), ('to', 0), ('know', 0), ('their', 0), ('imaging', 0), ('results', 0), ('and', 0), ('such', 0), ('information', 0), ('should', 0), ('be', 0), ('disclosed', 0), ('only', 0), ('if', 0), ('specifically', 0), ('requested', 0), ('by', 0), ('participants', 0)]\n"
     ]
    }
   ],
   "source": [
    "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[230:240]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "\n",
    "idx = 8\n",
    "paper = papers[df.Id.iloc[idx]]\n",
    "labels = df.dataset_label.iloc[idx].split('|')\n",
    "cnt_pos, cnt_neg, ner_data = get_paper_ner_data(paper, labels, classlabel=classlabel,\n",
    "                                                sentence_definition='section', max_length=512, overlap=20)\n",
    "print(cnt_pos, cnt_neg)\n",
    "print([len(sec) for sec in ner_data])\n",
    "print(ner_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-tennessee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_data(papers, df=None, classlabel=None, shuffle=True, \n",
    "                 sentence_definition='sentence', max_length=64, overlap=20):\n",
    "    '''\n",
    "    Args:\n",
    "        papers (dict): Like that returned by `load_papers`.\n",
    "        df (pd.DataFrame): Competition's train.csv or a subset of it.\n",
    "    '''\n",
    "    cnt_pos, cnt_neg = 0, 0 \n",
    "    ner_data = []\n",
    "\n",
    "    tqdm._instances.clear()\n",
    "    pbar = tqdm(total=len(df))\n",
    "    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():\n",
    "        paper = papers[id]\n",
    "        labels = dataset_label.split('|')\n",
    "                \n",
    "        cnt_pos_, cnt_neg_, ner_data_ = get_paper_ner_data(\n",
    "            paper, labels, classlabel=classlabel, \n",
    "            sentence_definition=sentence_definition, max_length=max_length, overlap=overlap)\n",
    "        cnt_pos += cnt_pos_\n",
    "        cnt_neg += cnt_neg_\n",
    "        ner_data.extend(ner_data_)\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(ner_data)\n",
    "    return cnt_pos, cnt_neg, ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-korea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [00:00<00:00, 120.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive count: 168.   Negative count: 2574\n",
      "[('Likewise', 0), ('there', 0), ('is', 0), ('a', 0), ('familiarity', 0), ('with', 0), ('a', 0), ('variety', 0), ('of', 0), ('studies', 0), ('that', 0), ('provide', 0), ('specific', 0), ('data', 0), ('on', 0), ('the', 0), ('academic', 0), ('achievement', 0), ('of', 0), ('Catholic', 0), ('school', 0), ('students', 0)]\n",
      "CPU times: user 893 ms, sys: 47.9 ms, total: 941 ms\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:100]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "cnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False,\n",
    "                                          sentence_definition='sentence', max_length=64, overlap=20)\n",
    "print(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\n",
    "print(ner_data[250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-victorian",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 171 positives + 910 negatives: 100%|██████████| 100/100 [00:00<00:00, 136.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive count: 171.   Negative count: 910\n",
      "[('WWC', 0), ('Single', 0), ('Study', 0), ('Review', 0), ('What', 0), ('did', 0), ('the', 0), ('study', 0), ('find', 0), ('The', 0), ('study', 0), ('reported', 0), ('and', 0), ('the', 0), ('WWC', 0), ('confirmed', 0), ('that', 0), ('dual', 0), ('enrollment', 0), ('programs', 0), ('significantly', 0), ('increased', 0), ('the', 0), ('likelihood', 0), ('of', 0), ('attaining', 0), ('a', 0), ('any', 0), ('college', 0), ('degree', 0), ('and', 0), ('b', 0), ('a', 0), ('bachelor', 0), ('s', 0), ('degree', 0), ('The', 0), ('study', 0), ('reported', 0), ('on', 0), ('the', 0), ('impact', 0), ('of', 0), ('dual', 0), ('enrollment', 0), ('programs', 0), ('for', 0), ('first', 0), ('generation', 0), ('college', 0), ('students', 0), ('students', 0), ('whose', 0), ('parents', 0), ('had', 0), ('some', 0), ('college', 0), ('students', 0), ('whose', 0), ('parents', 0), ('had', 0), ('a', 0), ('Bachelor', 0), ('s', 0), ('degree', 0), ('and', 0), ('students', 0), ('with', 0), ('post', 0), ('Bachelor', 0), ('s', 0), ('degree', 0), ('parents', 0), ('Although', 0), ('point', 0), ('estimates', 0), ('differed', 0), ('for', 0), ('the', 0), ('subgroups', 0), ('of', 0), ('students', 0), ('with', 0), ('different', 0), ('parental', 0), ('educational', 0), ('backgrounds', 0), ('the', 0), ('subgroups', 0), ('were', 0), ('not', 0), ('significantly', 0), ('different', 0), ('from', 0), ('each', 0), ('other', 0), ('The', 0), ('overall', 0), ('impact', 0), ('of', 0), ('dual', 0), ('enrollment', 0), ('programs', 0), ('is', 0), ('therefore', 0), ('the', 0), ('WWC', 0), ('s', 0), ('best', 0), ('estimate', 0), ('of', 0), ('effectiveness', 0), ('for', 0), ('these', 0), ('subgroups', 0), ('see', 0), ('Appendix', 0), ('D', 0), ('for', 0), ('more', 0), ('information', 0), ('The', 0), ('study', 0), ('author', 0), ('also', 0), ('examined', 0), ('whether', 0), ('students', 0), ('who', 0), ('earned', 0), ('more', 0), ('dual', 0), ('enrollment', 0), ('credits', 0), ('prior', 0), ('to', 0), ('attending', 0), ('college', 0), ('achieved', 0), ('greater', 0), ('benefits', 0), ('from', 0), ('the', 0), ('dual', 0), ('enrollment', 0), ('programs', 0), ('The', 0), ('impact', 0), ('of', 0), ('dual', 0), ('enrollment', 0), ('was', 0), ('significantly', 0), ('different', 0), ('for', 0), ('students', 0), ('earning', 0), ('three', 0), ('dual', 0), ('enrollment', 0), ('credits', 0), ('versus', 0), ('those', 0), ('earning', 0), ('six', 0), ('or', 0), ('more', 0), ('credits', 0), ('Students', 0), ('who', 0), ('earned', 0), ('three', 0), ('credits', 0), ('i', 0), ('e', 0), ('had', 0), ('one', 0), ('dual', 0), ('enrollment', 0), ('course', 0), ('were', 0), ('not', 0), ('more', 0), ('likely', 0), ('to', 0), ('attain', 0), ('a', 0), ('college', 0), ('degree', 0), ('than', 0), ('comparison', 0), ('group', 0), ('students', 0), ('However', 0), ('students', 0), ('who', 0), ('earned', 0), ('six', 0), ('credits', 0), ('i', 0), ('e', 0), ('two', 0), ('courses', 0), ('and', 0), ('students', 0), ('who', 0), ('earned', 0), ('seven', 0), ('or', 0), ('more', 0), ('credits', 0), ('were', 0), ('significantly', 0), ('more', 0), ('likely', 0), ('to', 0), ('attain', 0), ('any', 0), ('college', 0), ('degree', 0), ('or', 0), ('a', 0), ('bachelor', 0), ('s', 0), ('degree', 0), ('than', 0), ('comparison', 0), ('students', 0), ('see', 0), ('Appendix', 0), ('D', 0), ('for', 0), ('more', 0), ('information', 0), ('A', 0), ('sample', 0), ('of', 0), ('these', 0), ('respondents', 0), ('was', 0), ('then', 0), ('resurveyed', 0), ('through', 0), ('four', 0), ('follow', 0), ('ups', 0), ('in', 0), ('1990', 0), ('1992', 0), ('1994', 0), ('and', 0), ('2000', 0), ('The', 0), ('fourth', 0), ('follow', 0), ('up', 0), ('2000', 0), ('was', 0), ('used', 0), ('for', 0), ('this', 0), ('study', 0), ('Appendix', 0), ('B', 0), ('Outcome', 0), ('measures', 0), ('for', 0), ('the', 0), ('degree', 0), ('attainment', 0), ('domain', 0), ('Study', 0), ('Notes', 0), ('A', 0), ('correction', 0), ('for', 0), ('multiple', 0), ('comparisons', 0), ('was', 0), ('needed', 0), ('but', 0), ('did', 0), ('not', 0), ('affect', 0), ('significance', 0), ('levels', 0), ('The', 0), ('p', 0), ('values', 0), ('presented', 0), ('here', 0), ('were', 0), ('reported', 0), ('in', 0), ('the', 0), ('original', 0), ('study', 0), ('The', 0), ('mean', 0), ('differences', 0), ('reported', 0), ('in', 0), ('the', 0), ('table', 0), ('are', 0), ('covariate', 0), ('adjusted', 0), ('mean', 0), ('differences', 0), ('and', 0), ('were', 0), ('taken', 0), ('from', 0)]\n",
      "CPU times: user 750 ms, sys: 81 ms, total: 831 ms\n",
      "Wall time: 841 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:100]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "cnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False, \n",
    "                                          sentence_definition='section', max_length=512, overlap=20)\n",
    "print(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\n",
    "print(ner_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-sugar",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def write_ner_json(ner_data, pth=Path('train_ner.json')):\n",
    "    with open(pth, 'w') as f:\n",
    "        for row in ner_data:\n",
    "            words, nes = list(zip(*row))\n",
    "            row_json = {'tokens' : words, 'ner_tags' : nes}\n",
    "            json.dump(row_json, f)\n",
    "            f.write('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-beatles",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tokens\": [\"There\", \"is\", \"no\", \"dataset\", \"here\"], \"ner_tags\": [0, 0, 0, 0, 0]}\r\n",
      "{\"tokens\": [\"Load\", \"the\", \"UN\", \"Trade\", \"Development\", \"into\", \"view\"], \"ner_tags\": [0, 0, 2, 1, 1, 0, 0]}\r\n"
     ]
    }
   ],
   "source": [
    "ner_data = [\n",
    "    [('There', 0), ('is', 0), ('no', 0), ('dataset', 0), ('here', 0)], \n",
    "    [('Load', 0), ('the', 0), ('UN', 2), ('Trade', 1), ('Development', 1), ('into', 0), ('view', 0)]\n",
    "]\n",
    "write_ner_json(ner_data, pth=Path('/kaggle/tmp_ner.json'))\n",
    "! cat /kaggle/tmp_ner.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-cooperation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_ner_datasets(data_files=None):\n",
    "    datasets = load_dataset('json', data_files=data_files)\n",
    "    classlabel = get_ner_classlabel()\n",
    "    for split, dataset in datasets.items():\n",
    "        dataset.features['ner_tags'].feature = classlabel\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-victim",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-5c75d4e550e0f5d5/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5223cfab131c4daead8037644404a66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08ac6163440435aab5db069fc343f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5c75d4e550e0f5d5/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None)}\n",
      "\n",
      "{'tokens': ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view'], 'ner_tags': [0, 0, 2, 1, 1, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\n",
    "print(datasets['valid'].features)\n",
    "print()\n",
    "print(datasets['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-aluminum",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_tokenizer(model_checkpoint='bert-base-cased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-range",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8ee92a33b04525938675f59a51b590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ec2508cbc4477481d87d9d77e737d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c1ff71de314de8b391286748ab0b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5d7128b7fa4e5da9e14f54bc1883ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'input_ids': [101, 138, 188, 21943, 9930, 1104, 1234, 9026, 1121, 1103, 2286, 6194, 3499, 1113, 6356, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "{'input_ids': [101, 144, 6512, 9436, 24372, 1317, 185, 12937, 2042, 15520, 1114, 8626, 2330, 1447, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\n",
    "print(\n",
    "    tokenizer(\"A smattering of people descended from the midday boat on Monday.\"))\n",
    "print()\n",
    "print(\n",
    "    tokenizer(\"Giglio boasts several pristine bays with crystal clear water\".split(), is_split_into_words=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-scientist",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_and_align_labels(examples, tokenizer=None, label_all_tokens=True):\n",
    "    '''\n",
    "    Adds a new field called 'labels' that are the NER tags to the tokenized input.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer.\n",
    "        examples (datasets.arrow_dataset.Dataset): Dataset.\n",
    "        label_all_tokens (bool): If True, all sub-tokens are given the same tag as the \n",
    "            first sub-token, otherwise all but the first sub-token are given the tag\n",
    "            -100.\n",
    "    '''\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-richards",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]]}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c38b7be5aa4744bda739bdcfe9922a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ac11fbdc404bfea1fde60d5dbcc3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'ner_tags': [[0, 0, 0, 0, 0], [0, 0, 2, 1, 1, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tokens': [['There', 'is', 'no', 'dataset', 'here'], ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view']]}\n",
      "\n",
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'ner_tags': [[0, 0, 0, 0, 0], [0, 0, 2, 1, 1, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tokens': [['There', 'is', 'no', 'dataset', 'here'], ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view']]}\n",
      "{'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\n",
    "tokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\n",
    "\n",
    "print(tokenize_and_align_labels(datasets['train'][:], tokenizer, label_all_tokens=True), end='\\n\\n')\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\n",
    "\n",
    "print(tokenized_datasets['train'][:], end='\\n\\n')\n",
    "print(tokenized_datasets['valid'][:])\n",
    "print(tokenized_datasets['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-terrorism",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a224ccfcd58e4827a7b06a14c8e8d476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.3333333333333333}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric('seqeval')\n",
    "\n",
    "predictions = [['O', 'O', 'B', 'I', 'I', 'O']]\n",
    "references = [['B', 'I', 'B', 'O', 'O', 'O']]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-richards",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def jaccard_similarity(s1, s2):\n",
    "    l1 = set(s1.split(\" \"))\n",
    "    l2 = set(s2.split(\" \"))\n",
    "    intersection = len(list(l1.intersection(l2)))\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-disease",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity('USGS Frog Counts Data', 'USGA Croc Counts Data') == 1 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-commission",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_metrics(p, metric=None, label_list=None):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-austin",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.24786324786324787,\n",
       " 'recall': 0.26605504587155965,\n",
       " 'f1': 0.25663716814159293,\n",
       " 'accuracy': 0.38333333333333336}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "\n",
    "batch_size = 8\n",
    "max_sentence_length = 30\n",
    "\n",
    "predictions = np.random.randn(batch_size, max_sentence_length, classlabel.num_classes)\n",
    "label_ids = np.random.randint(low=0, high=classlabel.num_classes, \n",
    "                              size=(batch_size, max_sentence_length), dtype=np.int16)\n",
    "\n",
    "p = (predictions, label_ids)\n",
    "metric = load_metric('seqeval')\n",
    "compute_metrics(p, metric=metric, label_list=classlabel.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-collar",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NER training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-complexity",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 2 positives + 23 negatives: 100%|██████████| 2/2 [00:00<00:00, 138.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train.  Positive count: 8.  Negative count: 24.\n",
      "Valid.  Positive count: 2.  Negative count: 23.\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "train_meta = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:4]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', train_meta.Id)\n",
    "\n",
    "valid_cutoff = int(.50 * len(train_meta))\n",
    "valid_meta = train_meta.iloc[:valid_cutoff].reset_index(drop=True)\n",
    "train_meta = train_meta.iloc[valid_cutoff:].reset_index(drop=True)\n",
    "\n",
    "train_cnt_pos, train_cnt_neg, train_ner_data = get_ner_data(\n",
    "    papers, df=train_meta, classlabel=classlabel, sentence_definition='section', max_length=512, overlap=20)\n",
    "valid_cnt_pos, valid_cnt_neg, valid_ner_data = get_ner_data(\n",
    "    papers, df=valid_meta, classlabel=classlabel, sentence_definition='section', max_length=512, overlap=20)\n",
    "print(f'Train.  Positive count: {train_cnt_pos}.  Negative count: {train_cnt_neg}.')\n",
    "print(f'Valid.  Positive count: {valid_cnt_pos}.  Negative count: {valid_cnt_neg}.')\n",
    "\n",
    "write_ner_json(train_ner_data, pth='train_ner.json')\n",
    "write_ner_json(valid_ner_data, pth='valid_ner.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-hobby",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-d57444ea6d3c2668/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dbc90ddc3d4cce9a2784e06752e864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0290e6acecf4b529a484056afc20a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d57444ea6d3c2668/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b82c722398f405d9dd6b8520ee11d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c63d467c5904a6c8224634b1ce69a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba26527cda0b4b94b523ca8f959f2dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3681bee114b4913b4fc891d15991bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d876e2bb1144c8c8e0b5cde38aa91f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d0deb77d8b4e178f6863fc619f4595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf5efc377d847d99f0fb60ab01b9f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [00:37<00:00,  2.66it/s] \n",
      "Training data size: 171 positives + 910 negatives: 100%|██████████| 100/100 [00:36<00:00,  2.74it/s] \n",
      "Training data size: 8 positives + 24 negatives: 100%|██████████| 2/2 [00:21<00:00, 10.65s/it] \n",
      "Training data size: 2 positives + 23 negatives: 100%|██████████| 2/2 [00:21<00:00, 10.64s/it] \n"
     ]
    }
   ],
   "source": [
    "datasets = load_ner_datasets(data_files={'train':'train_ner.json', 'valid':'valid_ner.json'})\n",
    "\n",
    "model_checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = create_tokenizer(model_checkpoint)\n",
    "tokenized_datasets = datasets.map(\n",
    "    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=classlabel.num_classes)\n",
    "\n",
    "metric = load_metric('seqeval')\n",
    "\n",
    "args = TrainingArguments(output_dir='test_ner', num_train_epochs=2, \n",
    "                         learning_rate=2e-5, weight_decay=0.01,\n",
    "                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n",
    "                         save_strategy='epoch', save_total_limit=6)\n",
    "\n",
    "trainer = Trainer(model=model, args=args, \n",
    "                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n",
    "                  data_collator=data_collator, tokenizer=tokenizer, \n",
    "                  compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-stopping",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 02:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.989687</td>\n",
       "      <td>19.763200</td>\n",
       "      <td>1.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.763700</td>\n",
       "      <td>0.528109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>19.617000</td>\n",
       "      <td>1.274000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=0.7636871337890625, metrics={'train_runtime': 221.5724, 'train_samples_per_second': 0.018, 'total_flos': 12817508401152.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 3299872768, 'train_mem_cpu_peaked_delta': 6719229952})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-spider",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "drwxr-xr-x 2 root root 4096 May 29 09:55 checkpoint-2\r\n",
      "drwxr-xr-x 2 root root 4096 May 29 09:57 checkpoint-4\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lrt test_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-dollar",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 02:56, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.763700</td>\n",
       "      <td>0.464731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996959</td>\n",
       "      <td>19.545600</td>\n",
       "      <td>1.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.403027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997223</td>\n",
       "      <td>19.790900</td>\n",
       "      <td>1.263000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8, training_loss=0.22624197602272034, metrics={'train_runtime': 224.2963, 'train_samples_per_second': 0.036, 'total_flos': 25635016802304.0, 'epoch': 4.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 3775193088, 'train_mem_cpu_peaked_delta': 4076724224})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(output_dir='test_ner', num_train_epochs=4, \n",
    "                         learning_rate=2e-5, weight_decay=0.01,\n",
    "                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n",
    "                         save_strategy='epoch', save_total_limit=6)\n",
    "\n",
    "trainer = Trainer(model=model, args=args, \n",
    "                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n",
    "                  data_collator=data_collator, tokenizer=tokenizer, \n",
    "                  compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))\n",
    "trainer.train(resume_from_checkpoint='/kaggle/working/test_ner/checkpoint-4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-purse",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.40302741527557373,\n",
       " 'eval_precision': 0.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_accuracy': 0.9972233240777469,\n",
       " 'eval_runtime': 19.9006,\n",
       " 'eval_samples_per_second': 1.256,\n",
       " 'epoch': 4.0,\n",
       " 'eval_mem_cpu_alloc_delta': 69632,\n",
       " 'eval_mem_cpu_peaked_delta': 0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-bible",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 4}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.9972233240777469}\n"
     ]
    }
   ],
   "source": [
    "predictions, label_ids, _ = trainer.predict(tokenized_datasets['valid'])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_predictions = [[classlabel.names[p] for p, i in zip(prediction, label_id) if i != -100]\n",
    "                    for prediction, label_id in zip(predictions, label_ids)]\n",
    "\n",
    "true_labels = [[classlabel.names[i] for i in label_id if i != -100] for label_id in label_ids]\n",
    "\n",
    "print(true_predictions[0])\n",
    "print(true_labels[0])\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-switch",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NER inference\n",
    "\n",
    "**Turn off the Internet here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-ranch",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_inference_data(papers, sample_submission, classlabel=None, \n",
    "                           sentence_definition='sentence', max_length=64, overlap=20):\n",
    "    '''\n",
    "    Args:\n",
    "        papers (dict): Each list in this dictionary consists of the section of a paper.\n",
    "        sample_submission (pd.DataFrame): Competition 'sample_submission.csv'.\n",
    "    Returns:\n",
    "        test_rows (list): Each list in this list is of the form: \n",
    "             [('goat', 0), ('win', 0), ...] and represents a sentence.  \n",
    "        paper_length (list): Number of sentences in each paper.\n",
    "    '''\n",
    "    test_rows = [] \n",
    "    paper_length = [] \n",
    "\n",
    "    for paper_id in sample_submission['Id']:\n",
    "        paper = papers[paper_id]\n",
    "\n",
    "        sentences = extract_sentences(paper, sentence_definition=sentence_definition)\n",
    "        sentences = shorten_sentences(sentences, max_length=max_length, overlap=overlap)\n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] \n",
    "        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split()\n",
    "            dummy_tags = [classlabel.str2int('O')]*len(sentence_words)\n",
    "            test_rows.append(list(zip(sentence_words, dummy_tags)))\n",
    "\n",
    "        paper_length.append(len(sentences))\n",
    "\n",
    "    print(f'total number of sentences: {len(test_rows)}')\n",
    "    return test_rows, paper_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-genealogy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sentences: 105\n",
      "[('Methods', 0), ('and', 0), ('Materials', 0), ('COGENT', 0), ('is', 0), ('an', 0), ('international', 0), ('GWAS', 0), ('collaboration', 0), ('formed', 0), ('to', 0), ('conduct', 0), ('genetic', 0), ('analyses', 0), ('of', 0), ('g', 0), ('and', 0), ('related', 0), ('neurocognitive', 0), ('processes', 0), ('in', 0), ('healthy', 0), ('individuals', 0), ('Donohoe', 0), ('et', 0), ('al', 0), ('2012', 0), ('Though', 0), ('common', 0), ('GWAS', 0), ('markers', 0), ('have', 0), ('been', 0), ('proposed', 0), ('to', 0), ('account', 0), ('for', 0), ('30', 0), ('or', 0), ('more', 0), ('of', 0), ('the', 0), ('variance', 0), ('in', 0), ('general', 0), ('intelligence', 0), ('in', 0), ('adults', 0), ('individual', 0), ('SNPs', 0), ('only', 0), ('contribute', 0), ('a', 0), ('small', 0), ('fraction', 0), ('of', 0), ('the', 0), ('variance', 0), ('to', 0), ('the', 0), ('heritability', 0), ('of', 0), ('g', 0), ('due', 0), ('to', 0), ('extreme', 0), ('polygenicity', 0), ('Marioni', 0), ('et', 0), ('al', 0), ('2014', 0), ('Detecting', 0), ('SNP', 0), ('associations', 0), ('of', 0), ('such', 0), ('small', 0), ('magnitudes', 0), ('via', 0), ('GWAS', 0), ('requires', 0), ('large', 0), ('samples', 0), ('many', 0), ('times', 0), ('the', 0), ('size', 0), ('an', 0), ('individual', 0), ('lab', 0), ('can', 0), ('ascertain', 0), ('leading', 0), ('to', 0), ('consortia', 0), ('such', 0), ('as', 0), ('COGENT', 0), ('The', 0), ('decision', 0), ('to', 0), ('study', 0), ('g', 0), ('in', 0), ('COGENT', 0), ('stemmed', 0), ('from', 0), ('longstanding', 0), ('evidence', 0), ('that', 0), ('a', 0), ('g', 0), ('factor', 0), ('can', 0), ('be', 0), ('derived', 0), ('consistently', 0), ('captures', 0), ('almost', 0), ('half', 0), ('the', 0), ('variance', 0), ('in', 0), ('overall', 0), ('test', 0), ('performance', 0), ('and', 0), ('is', 0), ('relatively', 0), ('invariant', 0), ('to', 0), ('the', 0), ('neurocognitive', 0), ('test', 0), ('battery', 0), ('used', 0), ('and', 0), ('specific', 0), ('abilities', 0), ('assessed', 0), ('Panizzon', 0), ('et', 0), ('al', 0), ('2014', 0), ('The', 0), ('first', 0), ('phase', 0), ('of', 0), ('COGENT', 0), ('COGENT1', 0), ('resulted', 0), ('in', 0), ('a', 0), ('GWAS', 0), ('of', 0), ('general', 0), ('cognitive', 0), ('ability', 0), ('in', 0), ('5', 0), ('000', 0), ('individuals', 0), ('from', 0), ('the', 0), ('general', 0), ('population', 0), ('Lencz', 0), ('et', 0), ('al', 0), ('2013', 0), ('The', 0), ('next', 0), ('and', 0), ('ongoing', 0), ('wave', 0), ('of', 0), ('data', 0), ('collection', 0), ('in', 0), ('COGENT', 0), ('COGENT2', 0), ('has', 0), ('resulted', 0), ('in', 0), ('the', 0), ('acquisition', 0), ('of', 0), ('15', 0), ('000', 0), ('independent', 0), ('subjects', 0), ('with', 0), ('neurocognitive', 0), ('and', 0), ('GWAS', 0), ('data', 0), ('for', 0), ('analysis', 0), ('see', 0), ('Table', 0), ('1', 0), ('for', 0), ('cohort', 0), ('details', 0), ('To', 0), ('be', 0), ('included', 0), ('as', 0), ('a', 0), ('participant', 0), ('in', 0), ('COGENT', 0), ('data', 0), ('from', 0), ('at', 0), ('least', 0), ('one', 0), ('neuropsychological', 0), ('measure', 0), ('across', 0), ('at', 0), ('least', 0), ('three', 0), ('domains', 0), ('of', 0), ('cognitive', 0), ('performance', 0), ('e', 0), ('g', 0), ('digit', 0), ('span', 0), ('for', 0), ('working', 0), ('memory', 0), ('logical', 0), ('memory', 0), ('for', 0), ('verbal', 0), ('declarative', 0), ('memory', 0), ('and', 0), ('digit', 0), ('symbol', 0), ('coding', 0), ('for', 0), ('processing', 0), ('speed', 0), ('or', 0), ('the', 0), ('use', 0), ('of', 0), ('a', 0), ('validated', 0), ('g', 0), ('sensitive', 0), ('measure', 0), ('was', 0), ('required', 0), ('Tests', 0), ('missing', 0), ('for', 0), ('more', 0), ('than', 0), ('5', 0), ('of', 0), ('the', 0), ('sample', 0), ('in', 0), ('an', 0), ('individual', 0), ('study', 0), ('were', 0), ('excluded', 0), ('Each', 0), ('COGENT', 0), ('study', 0), ('administered', 0), ('an', 0), ('average', 0), ('of', 0), ('10', 0), ('3', 0), ('neurocognitive', 0), ('tests', 0), ('and', 0), ('the', 0), ('internal', 0), ('consistency', 0), ('of', 0), ('performance', 0), ('within', 0), ('each', 0), ('study', 0), ('was', 0), ('strong', 0), ('mean', 0), ('Cronbach', 0), ('s', 0), ('alpha', 0), ('75', 0), ('supplementary', 0), ('table', 0), ('S1', 0), ('The', 0), ('first', 0), ('unrotated', 0), ('principal', 0), ('component', 0), ('accounted', 0), ('for', 0), ('just', 0), ('under', 0), ('half', 0), ('of', 0), ('the', 0), ('variance', 0), ('across', 0), ('the', 0), ('21', 0), ('studies', 0), ('on', 0), ('average', 0), ('as', 0), ('expected', 0), ('based', 0), ('on', 0), ('an', 0), ('extensive', 0), ('prior', 0), ('literature', 0), ('Carroll', 0), ('1993', 0), ('As', 0), ('Figure', 0), ('1', 0), ('shows', 0), ('g', 0), ('had', 0), ('normal', 0), ('distributional', 0), ('properties', 0), ('within', 0), ('all', 0), ('21', 0), ('studies', 0), ('a', 0), ('feature', 0), ('critical', 0), ('to', 0), ('enhancing', 0), ('statistical', 0), ('power', 0), ('in', 0), ('quantitative', 0), ('trait', 0), ('analysis', 0), ('All', 0), ('COGENT', 0), ('samples', 0), ('were', 0), ('genotyped', 0), ('on', 0), ('commercial', 0), ('Illumina', 0), ('or', 0), ('Affymetrix', 0), ('genome', 0), ('wide', 0), ('SNP', 0), ('microarrays', 0), ('supplementary', 0), ('table', 0), ('S1', 0), ('Standard', 0), ('GWAS', 0), ('quality', 0), ('control', 0), ('QC', 0), ('methods', 0), ('were', 0), ('applied', 0), ('to', 0), ('the', 0), ('genetic', 0), ('data', 0), ('described', 0), ('in', 0), ('detail', 0), ('in', 0), ('the', 0), ('supplementary', 0), ('information', 0), ('Subjects', 0), ('in', 0), ('the', 0), ('study', 0), ('were', 0), ('Caucasian', 0), ('of', 0), ('European', 0), ('ancestry', 0), ('which', 0), ('we', 0), ('confirmed', 0), ('by', 0), ('analysis', 0), ('of', 0), ('genotype', 0), ('data', 0), ('using', 0), ('multidimensional', 0), ('scaling', 0), ('MDS', 0), ('Genetic', 0), ('outliers', 0), ('were', 0), ('removed', 0), ('in', 0), ('each', 0), ('study', 0), ('based', 0), ('on', 0), ('MDS', 0), ('axis', 0), ('plotting', 0), ('versus', 0), ('HapMap3', 0), ('ethnic', 0), ('subgroups', 0), ('Note', 0), ('that', 0), ('none', 0), ('of', 0), ('the', 0), ('three', 0), ('SNPs', 0), ('previously', 0), ('associated', 0), ('with', 0), ('education', 0), ('were', 0), ('variants', 0), ('included', 0), ('on', 0), ('commercially', 0), ('available', 0), ('microarrays', 0), ('and', 0), ('thus', 0), ('were', 0), ('imputed', 0), ('into', 0), ('their', 0), ('datasets', 0), ('Rietveld', 0), ('et', 0), ('al', 0), ('2013', 0), ('In', 0), ('the', 0), ('COGENT1', 0), ('studies', 0), ('SNPs', 0), ('were', 0), ('imputed', 0), ('using', 0), ('HapMap3', 0), ('reference', 0), ('panels', 0), ('as', 0), ('previously', 0), ('described', 0), ('Lencz', 0), ('et', 0), ('al', 0), ('2013', 0), ('COGENT2', 0), ('samples', 0), ('that', 0), ('did', 0), ('not', 0), ('have', 0), ('genotypes', 0), ('for', 0), ('the', 0), ('SNPs', 0), ('of', 0), ('interest', 0), ('were', 0), ('imputed', 0), ('using', 0), ('IMPUTE2', 0), ('Howie', 0), ('et', 0), ('al', 0), ('2009', 0), ('and', 0), ('1000', 0), ('Genomes', 0), ('Project', 0), ('reference', 0), ('panels', 0), ('downloaded', 0), ('June', 0), ('2014', 0)]\n",
      "[10, 50, 24, 21]\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test', sample_submission.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "test_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel, \n",
    "                                                 sentence_definition='section', max_length=512, overlap=20)\n",
    "print(test_rows[1])\n",
    "print(paper_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-median",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def ner_predict(test_rows, tokenizer=None, model=None, metric=None):\n",
    "    classlabel = get_ner_classlabel()\n",
    "    \n",
    "    write_ner_json(test_rows, pth='test_ner.json')\n",
    "    datasets = load_ner_datasets(data_files={'test':'test_ner.json'})\n",
    "    print('Tokenizing testset...')\n",
    "    tokenized_datasets = datasets.map(\n",
    "        partial(tokenize_and_align_labels,tokenizer=tokenizer, label_all_tokens=True), \n",
    "        batched=True) \n",
    "\n",
    "    print('Creating data collator...')\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    print('Creating (dummy) training arguments...')\n",
    "    args = TrainingArguments(output_dir='test_ner', num_train_epochs=3, \n",
    "                             learning_rate=2e-5, weight_decay=0.01,\n",
    "                             per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                             evaluation_strategy='epoch', logging_steps=4, report_to='none', \n",
    "                             save_strategy='epoch', save_total_limit=6)\n",
    "\n",
    "    print('Creating trainer...')\n",
    "    trainer = Trainer(model=model, args=args, \n",
    "                      train_dataset=tokenized_datasets['test'], eval_dataset=tokenized_datasets['test'], \n",
    "                      data_collator=data_collator, tokenizer=tokenizer, \n",
    "                      compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))\n",
    "\n",
    "    print('Predicting on test samples...')\n",
    "    predictions, label_ids, _ = trainer.predict(tokenized_datasets['test'])\n",
    "    predictions = predictions.argmax(axis=2)\n",
    "    true_predictions = [\n",
    "        [p for p, i in zip(prediction, label_id) if i != -100]\n",
    "        for prediction, label_id in zip(predictions, label_ids)]\n",
    "\n",
    "    return true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-portuguese",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'test_ner/checkpoint-8/'\n",
    "\n",
    "tokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "metric = load_metric('seqeval')\n",
    "# metric = load_metric('/root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/seqeval.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-flight",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exporting the cached metric \n",
    "\n",
    "# %cd /root/.cache\n",
    "# ! zip -r huggingface_cache.zip huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/\n",
    "# %cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-portland",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-459a66eaa080ee59/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b15e2a00db94a5895d3323cb90cf059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-459a66eaa080ee59/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "Tokenizing testset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c0ee4dbf2e4281bdf36e2a90c7b4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating data collator...\n",
      "Creating (dummy) training arguments...\n",
      "Creating trainer...\n",
      "Predicting on test samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "bert_outputs = ner_predict(test_rows, tokenizer=tokenizer, model=model, metric=metric)\n",
    "print(bert_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-function",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_batch = 100\n",
    "\n",
    "# bert_outputs = []\n",
    "# for i in range(0, len(test_rows), predict_batch):\n",
    "#     print(f'\\rPredicting on samples {i} to {i + predict_batch}...', flush=True)\n",
    "#     b = ner_predict(test_rows[i:i + predict_batch], model=model, tokenizer=tokenizer, metric=metric)\n",
    "#     bert_outputs.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-generation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=None):\n",
    "    '''\n",
    "    Returns:\n",
    "        bert_dataset_labels (list): Each element is a set consisting of labels predicted\n",
    "            by the model.\n",
    "    '''\n",
    "    test_sentences = [list(zip(*row))[0] for row in test_rows]\n",
    "#     test_sentences = [row['tokens'] for row in test_rows]\n",
    "    \n",
    "    bert_dataset_labels = [] # store all dataset labels for each publication\n",
    "\n",
    "    for length in paper_length:\n",
    "        labels = set()\n",
    "        for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n",
    "            curr_phrase = ''\n",
    "            for word, tag in zip(sentence, pred):\n",
    "                if tag == classlabel.str2int('B'): # start a new phrase\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "                    curr_phrase = word\n",
    "                elif tag == classlabel.str2int('I') and curr_phrase: # continue the phrase\n",
    "                    curr_phrase += ' ' + word\n",
    "                else: # end last phrase (if any)\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "            # check if the label is the suffix of the sentence\n",
    "            if curr_phrase:\n",
    "                labels.add(curr_phrase)\n",
    "                curr_phrase = ''\n",
    "\n",
    "        # record dataset labels for this publication\n",
    "        bert_dataset_labels.append(labels)\n",
    "\n",
    "        del test_sentences[:length], bert_outputs[:length]\n",
    "        \n",
    "    return bert_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-consortium",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Tigers EcoNAX dataset', 'present all the'}, {'WGS Equality Definitiveness Dataset'}]\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "sentences = ['They do not present all the features', \n",
    "             'Despite the pretraining on the Tigers EcoNAX dataset',\n",
    "             'Weirdly there has been lots of studies based on WGS Equality Definitiveness Dataset']\n",
    "paper_length = [2, 1]\n",
    "test_rows = [[(word, 0) for word in sentence.split()] for sentence in sentences]\n",
    "bert_outputs = [[0, 0, 0, 2, 1, 1, 0],\n",
    "                [0, 0, 0, 0, 0, 2, 1, 1],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1]]\n",
    "for i, row in enumerate(test_rows):\n",
    "    assert len(row) == len(bert_outputs[i])\n",
    "\n",
    "bert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=classlabel)\n",
    "print(bert_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-prototype",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_bert_labels(bert_dataset_labels):\n",
    "    '''\n",
    "    When several labels for a paper are too similar, keep just one of them.\n",
    "    '''\n",
    "    filtered_bert_labels = []\n",
    "\n",
    "    for labels in bert_dataset_labels:\n",
    "        filtered = []\n",
    "\n",
    "        for label in sorted(labels, key=len):\n",
    "            label = clean_training_text(label, lower=True)\n",
    "            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n",
    "                filtered.append(label)\n",
    "\n",
    "        filtered_bert_labels.append('|'.join(filtered))\n",
    "    return filtered_bert_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-photographer",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moma artists|housing market|moma artists catalogue',\n",
       " 'deep sea rock salts|rhs fertiliser index']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset_labels = [{'moma artists catalogue', 'moma artists', 'housing market'},\n",
    "                       {'rhs flowers fertiliser index', 'deep sea rock salts', 'rhs fertiliser index'}]\n",
    "\n",
    "filter_bert_labels(bert_dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-tours",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Literal matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-scale",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_knowledge_bank(pth):\n",
    "    '''\n",
    "    Args:\n",
    "        pth (str): Path to meta data like 'train.csv', which\n",
    "        needs to have columns: 'dataset_title', 'dataset_label', and 'cleaned_label'.\n",
    "        \n",
    "    Returns:\n",
    "        all_labels (set): All possible strings associated with a dataset from the meta data.\n",
    "    '''\n",
    "    df = load_train_meta(pth, group_id=False)\n",
    "    all_labels = set()\n",
    "    for label_1, label_2, label_3 in df[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n",
    "        all_labels.add(str(label_1).lower())\n",
    "        all_labels.add(str(label_2).lower())\n",
    "        all_labels.add(str(label_3).lower())\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-wheel",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "['2019 ncov complete genome sequences', '2019 ncov genome sequence', '2019 ncov genome sequences', '2019-ncov complete genome sequences', '2019-ncov genome sequence', '2019-ncov genome sequences', 'adni', 'advanced national seismic system (anss) comprehensive catalog (comcat)', 'advanced national seismic system anss comprehensive catalog comcat ', 'advanced national seismic system comprehensive catalog']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "print(len(all_labels))\n",
    "print(sorted(all_labels)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-welsh",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def literal_match(paper, all_labels):\n",
    "    '''\n",
    "    Args:\n",
    "        paper ()\n",
    "    '''\n",
    "    text_1 = '. '.join(section['text'] for section in paper).lower()\n",
    "    text_2 = clean_training_text(text_1, lower=True, total_clean=True)\n",
    "    \n",
    "    labels = set()\n",
    "    for label in all_labels:\n",
    "        if label in text_1 or label in text_2:\n",
    "            labels.add(clean_training_text(label, lower=True, total_clean=True))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-redhead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alzheimer s disease neuroimaging initiative adni|adni',\n",
       " 'common core of data|trends in international mathematics and science study|nces common core of data',\n",
       " 'sea lake and overland surges from hurricanes|noaa storm surge inundation|slosh model',\n",
       " 'rural urban continuum codes']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n",
    "\n",
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "\n",
    "literal_preds = []\n",
    "for paper_id in sample_submission.Id:\n",
    "    paper = papers[paper_id]\n",
    "    literal_preds.append('|'.join(literal_match(paper, all_labels)))\n",
    "    \n",
    "literal_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-consultancy",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overall prediction for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-honor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def combine_matching_and_bert(literal_preds, filtererd_bert_labels):\n",
    "    '''\n",
    "    For a given sentence, if there's a literal match, use that as the final\n",
    "    prediction for the sentence.  If there isn't a literal match,\n",
    "    use what the model predicts.\n",
    "    '''\n",
    "    final_predictions = []\n",
    "    for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n",
    "        if literal_match:\n",
    "            final_predictions.append(literal_match)\n",
    "        else:\n",
    "            final_predictions.append(bert_pred)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-winner",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mongolian racing cars|reallife headphones',\n",
       " 'hifi dataset|headphones collection data']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_preds = ['mongolian racing cars|reallife headphones', '']\n",
    "filtered_bert_labels = ['data|dataset', 'hifi dataset|headphones collection data']\n",
    "combine_matching_and_bert(literal_preds, filtered_bert_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-memorial",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing NER inference data...\n",
      "total number of sentences: 105\n",
      "Loading model, tokenizer, and metric...\n",
      "Predicting on each sentence...\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-f2fa7b31b822d806/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a512e1c8084a7a925724e0d5edbbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f2fa7b31b822d806/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "Tokenizing testset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddef74eb7bb48169ff3e965b8e053e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating data collator...\n",
      "Creating (dummy) training arguments...\n",
      "Creating trainer...\n",
      "Predicting on test samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predicted labels for each article...\n",
      "Keeping just one of labels that are too similar to each other...\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "\n",
    "print('Preparing NER inference data...')\n",
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n",
    "test_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel,\n",
    "                                                 sentence_definition='section', max_length=512, overlap=20)\n",
    "\n",
    "print('Loading model, tokenizer, and metric...')\n",
    "model_checkpoint = 'test_ner/checkpoint-8/'\n",
    "tokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "metric = load_metric('seqeval')\n",
    "\n",
    "print('Predicting on each sentence...')\n",
    "bert_outputs = ner_predict(test_rows, tokenizer=tokenizer, model=model, metric=metric)\n",
    "\n",
    "print('Getting predicted labels for each article...')\n",
    "bert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=classlabel)\n",
    "\n",
    "print('Keeping just one of labels that are too similar to each other...')\n",
    "filtered_bert_labels = filter_bert_labels(bert_dataset_labels)\n",
    "\n",
    "sample_submission['PredictionString'] = filtered_bert_labels\n",
    "\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-haven",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id PredictionString\n",
       "0  2100032a-7c33-4bff-97ef-690822c43466                 \n",
       "1  2f392438-e215-4169-bebf-21ac4ff253e1                 \n",
       "2  3f316b38-1a24-45a9-8d8c-4e05a42257c6                 \n",
       "3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60                 "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-operator",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reference\n",
    "- https://www.kaggle.com/tungmphung/pytorch-bert-for-named-entity-recognition/notebook\n",
    "- https://www.kaggle.com/tungmphung/coleridge-matching-bert-ner/notebook\n",
    "- https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "- https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin\n",
    "- https://huggingface.co/docs/datasets/loading_metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-hacker",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
