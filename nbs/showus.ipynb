{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "challenging-scott",
   "metadata": {
    "tags": []
   },
   "source": [
    "# showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-poland",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#default_exp showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-execution",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\r\n",
      "Installing collected packages: fsspec\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 0.8.7\r\n",
      "    Uninstalling fsspec-0.8.7:\r\n",
      "      Successfully uninstalled fsspec-0.8.7\r\n",
      "Successfully installed fsspec-2021.4.0\r\n",
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.4.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.59.0\r\n",
      "    Uninstalling tqdm-4.59.0:\r\n",
      "      Successfully uninstalled tqdm-4.59.0\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Installing collected packages: tokenizers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.10.2\r\n",
      "    Uninstalling tokenizers-0.10.2:\r\n",
      "      Successfully uninstalled tokenizers-0.10.2\r\n",
      "Successfully installed tokenizers-0.10.1\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.45)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2021.3.17)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (4.0.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.5.1\r\n",
      "    Uninstalling transformers-4.5.1:\r\n",
      "      Successfully uninstalled transformers-4.5.1\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\n",
    "! pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n",
    "! pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n",
    "! pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n",
    "! pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-coaching",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from functools import partial\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers, seqeval\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset, ClassLabel, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-wound",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-admission",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "Path.ls = lambda pth: list(pth.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-genealogy",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-liechtenstein",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_train_meta(pth, group_id=True):\n",
    "    df = pd.read_csv(pth)\n",
    "    if group_id:\n",
    "        df = df.groupby('Id').agg({'pub_title': 'first', 'dataset_title': '|'.join, \n",
    "                                   'dataset_label': '|'.join, 'cleaned_label': '|'.join}).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-colleague",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14316 19661\n",
      "['Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " 'Beginning Postsecondary Students Longitudinal Study|Education Longitudinal Study|Beginning Postsecondary Students'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Beginning Postsecondary Student|Beginning Postsecondary Students']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "df = load_train_meta(pth, group_id=True)\n",
    "df_nogroup = load_train_meta(pth, group_id=False)\n",
    "print(len(df), len(df_nogroup))\n",
    "dup_ids = df_nogroup[df_nogroup.Id.duplicated()].Id.unique()\n",
    "print(df[df.Id.isin(dup_ids)].dataset_label.values[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-israel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_papers(dir_json, paper_ids):\n",
    "    '''\n",
    "    Load papers into a dictionary.\n",
    "    \n",
    "    `papers`: \n",
    "        {''}\n",
    "    '''\n",
    "    \n",
    "    papers = {}\n",
    "    for paper_id in paper_ids:\n",
    "        with open(f'{dir_json}/{paper_id}.json', 'r') as f:\n",
    "            paper = json.load(f)\n",
    "            papers[paper_id] = paper\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-capture",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'section_title': '', 'text': 'predicts that tuition and fees at four-year public institutions will double in 17 years, while tuition and fees at two-year publics and four-year privates will double in 23 and 27 years, respectively. To put this in perspective, the cost of tuition and fees at a public four-year university in 17 years will be akin to pulling out about $18,000 today from one\\'s pocket. And that\\'s just for tuition and fees. Room and board will likely cost another $18,000 to $20,000 by that point. All told, the annual, in-state cost of attendance at a public four-year institution will run about $38,000 in the early 2030s in today\\'s dollars. Over a four-year period, this will total about $150,000. Note. Future trend used annual multiplier of 2.6% for private 4-year, 4.3% for public 4-year, and 3.1% for public 2-year colleges to arrive at increases above inflation, based on 25-year historical data.\\nJust imagine walking into a college financial aid office tomorrow morning and coming up with a $150,000 plan to pay for your son\\'s and daughter\\'s college experience at an in-state, public four-year institution. We are not even talking about the four-year privates, which will run about $70,000 a year in today\\'s dollars, on average, and $120,000 for the highest-cost colleges (in today\\'s dollars per year). Those figures total to about $300,000 for a four-year degree at the average private institutions and about half a million dollars for the elite, very selective institutions. Only a fraction of students attends these high-priced institutions, but the message taken from their pricing drives the above-the-fold news around the country (if not the world). It is no wonder that students self-select themselves out of college long before they are even eligible for admissions: they don\\'t think they have a chance.\\nMost students do not understand how much college costs or how they can finance it. The high cost of college is pushing us to the tipping point (or have we surpassed it?), where the return on investment on higher education has diminished to the point that it may not be a prudent avenue for some prospective students. A 1999 study by the National Center for Education Statistics (NCES) found that only 24% of grades 6-12 parents could estimate the tuition and fee costs of college, compared to just 15% of their children (Horn, Xianglei, & Chapman, 2003) . And while 44% of middle school parents had obtained information on college or could estimate the costs, only 16% of middle school students did the same.\\nTo be fair, understanding the labyrinth of higher education is a challenge for all, regardless of income or background. The equivocal and contradictory reports in the news do not make this information much clearer. Just recently The New York Times published a report about college still being worth it (Leonhardt, 2014) , while The Chronicle of Higher Education showcased the lack of gainful employment of recent graduates (Supiano, 2014) . The challenge is that many students self-select themselves out of the college pipeline because they either do not believe they can afford it, do not feel they are prepared for it, or worse, simply feel they do not belong there (Hoxby & Turner, 2014; Swail, Cabrera, & Lee, 2004) . As Laura Rendon once wrote, college dropout begins to happen in grade school (Rendon, 1997) .\\nRegardless of these heavy financial burdens on college students, I believe the true primary reason that students do not prepare, apply, admit, and succeed in higher education is academic wherewithal. Students who take rigorous coursework are much more likely to finish a bachelor\\'s degrees than others, regardless of ancillary issues such as financial need (Adelman, 1997) . The rigorous course work, in effect, serves as a proxy for other important criterion, such as study skills, time management, and organizational skills (Burrus, Jackson, Holtzman, Roberts, & Mandigo, 2013; Kerka, 2007) . Students need to know \"how\" to learn and how to manage their time. They must develop higher order thinking skills and be able to work in isolation and in groups. While many students drop out because of finances, much more dropout because they just don\\'t have the academic-related skill-sets to succeed.\\nThe challenges facing higher education institutions, especially open admissions institutions, is daunting. The deficiencies in student preparation and learning can be broad and vast. The idea that institutions can essentially fix 13 years of schooling through one bridge program, one semester of Freshman 101, or one supplemental course is mindblowing.\\nRemediation is an example of a concern for students and retention specialists. Data from the NCES illustrate that 21% of first-year undergraduates attending public four-year institutions and 24% at two-year public institutions (2007-08) enrolled in at least one remedial course (Sparks & Malkus, 2013) , and the percentages increase for non-White students (Ross et al., (2012) . The variation in remedial enrollment between open admissions and very selective admissions institutions is also large: 13% vs. 26%. Remediation does impact graduation rates. NCES data suggest an 8% graduation gap between remedial vs. non-remedial students at four-year public institutions (53% vs. 61%).\\n1 1 This analysis is by the author using the NCES QuickStats data tool. Analysis considered first time Beginning Postsecondary Students (BPS) in 2003-04 at a four-year public institution who either did not enroll in a remedial course their freshman year, by the six-year graduation rate in 2009. Interesting note: other non-profit organizations have suggested a much larger gap in performance between remedial students and non-remedial students (e..g, 58% vs. 17%, as reported by the Complete College America). However, the author was unable to substantiate these claims by any viable method and their source work was incomplete.\\nThe articles presented in this special issue of Higher Learning Research Communications discuss important issues such as prediction of dropout, frameworks for retaining students, and various strategies for addressing attrition, such as counseling, service learning, and academic support services. Given my thesis above, it is not to suggest that remediation, study skill training and time management, and many of the other strategies implored in this volume are not of utility. They clearly are. However, the philosophical question remains about how much we do for whom?\\nMy prior research illustrated, with use of Beginning Postsecondary Student (BPS) data from the NCES, that students who have these attributes are much less likely to graduate from college:\\n\\uf09f attend part-time, \\uf09f have a low GPA, \\uf09f are of non-traditional age (e.g., older), \\uf09f are non-White (with the exception of Asian), \\uf09f are first generation, \\uf09f are low income and/or independent, \\uf09f have a variety of risk factors (including having children, being single), \\uf09f delay entry to college, \\uf09f attend an HBCU or HSI, \\uf09f have lower levels of high school mathematics, \\uf09f attend more than one institution (although this can depend), and \\uf09f work more than 20 hours a week.\\nThere are more. But these examples get at the crux of the issues that help determine whether a student will sink or swim in higher education. Some students have many of these attributes. Some only one or a few. The chart below illustrates that 66% of first time college students end up attaining a degree within six years, compared to 44% of students with at least one risk factor.\\n2 Those with multiple risk factors have a much lower change of postsecondary success. Only 34% of those with two or three risk factors, and 30% of those with four or more, graduated with a degree within 6 years.\\n2 Risk factors in this analysis of BPS data include: part-time enrollment, delaying entry into postsecondary education after high school, not having a regular high school diploma; having children, being a single parent, being financially independent of parents, working full time while enrolled. W. S. Swail In the end, none of these should be used to deny access to college for an individual. That stated, somewhere along the continuum there is a line where students will not succeed. A line beyond where no matter what we do in higher education, we cannot fix the source challenges that face a particular student. The question is whether we want to know where that line is. With regard to public policy and public perception, it seems clear that no one currently wants to find that line. We seem completely content with the status quo and not dealing with the hard and difficult issue of drawing a line.\\nThe reality is that our open system of education is costly. With regard to remediation alone, estimates suggest that the cost of current remediation practices is in the area of $3.6 billion (Alliance for Excellent Education, 2011). I\\'ve alluded to the growing costs of higher education, especially with regard to the prices paid by students and parents. Are we at the tipping point of college opportunity?\\nIn the end, what does this all mean? Given this somewhat dreadful information I have showcased, and beyond the policy implications that are handed down from states and the federal government upon which an institution has little or no control, what does an institution do about student access and success?\\nFor starters, I believe the first rule of conduct for an institution is to define for themselves what success looks like and how that success is manifested in a student or cohort at their institution. It is critically important for institutions to understand the nature of success, not just from an academic perspective, but from a social, non-cognitive perspective. In the end, what is it that makes students success at College XYZ? Every institution can run internal multiple regression analyses to provide details on the attributes of graduates compared to dropouts and transfers. Academic data on progress can be merged with social data from institutional surveys to provide such information as study habits, use of time, work, comfortability, and yes, even happiness, to see how those link with academic outcomes. This information can be used in several ways. First, it can be used to help instructional faculty better understand their students. Second, instructors should have some diagnostic tool to gauge when a student is having academic or other challenges. Third, academic advisors and counselors should absolutely use these data to help describe to freshman students what success looks like. In the workshops that I conduct, I suggest that advisors have prepared a one-page sheet to give to their students that lists that student\\'s various academic and non-academic scores/preferences/habits (from institutional surveys) in one column compared with data of successful students from their college in a second column. Success should be visual and put in perspective. I think it is of great merit to illustrate to a student that, if they study X hours per week, use Y institutional support services, and work with their friends and peers, that they, too, can succeed. Students need roadmaps. Give them one.\\nThe second rule of conduct is to be real about admissions. When an institution accepts the registration of a student, they are, in effect, entering a moral, ethical, and legal contract with the student to do whatever they can to help that student succeed. They need to ensure that the student gets the support he or she needs, which means that the institution must have insight into those needs early and often. Conversely, if the institution is not able, willing, or interested in doing so, then they should do the right thing and not admit that student. This sounds harsh, but I see this play out countless times at institutions. After the student is admitted, many aspects of the institution, besides basic instruction, are provided in a distant, noninvasive manner when they should be both intrusive and invasive. In this data age, it is relatively easy for the institution to diagnose the positives and negatives of a student at Day 1. It is up to the institution to identify these issues and provide assistance and not simply expect that students will find assistance on their own. At-risk students, who often are first generation and low-income, are typically the last people to ask for help. Institutions can\\'t just \"phone in\" this type of support. They have to meet students where they are.\\nIf you travel the similar circles as me, it is rare when I meet someone working at an institution who does not want their students to succeed. But sometimes we do some things that hinder rather than help students. Part of student success may be steering students away from your institution or department. If he or she does not belong there, do not enroll them, do not take their money, and do not burden them with loans that can never pay off. Or, just as bad or worse, the growing burden being placed on parents who are taking out PLUS loans or remortgaging their house to pay for college.\\nPerhaps one of the most immoral things we can do is to admit a student who seriously does not have the skills to stay in the game. Back to my original thesis, I do not agree with those that say that all students can learn and succeed in college and we should admit anyone who wants that opportunity. We should not. In consideration of the both financial and opportunity costs, we should be more mindful of the moral authority of accepting students who are not likely to succeed. Statistically, most students who apply to college can succeed. But there is also a large group of students that are ill-prepared to succeed at this level. Some students are not effectively motivated to do the work they need to do. Some just do not have the requisite skills to take on the \"higher\" learning. In many cases, these issues weren\\'t their fault. They are simply outcomes of a system that poorly prepared them for postsecondary education, and in many cases, the world of work. But, nonetheless, here they are at our doorstep. Society has told them that success is difficult without a college degree. This, too, is an injustice, but that is the message we send daily to students. Just because they show up does not mean we can encourage and support them to finish. For some level of student success, we do better by steering them to achieve their goals somewhere else. Somewhere cheaper; somewhere more appropriate to their interests. But not here.\\nWe are fortunate to be served by a fairly well-articulated system of higher education. However, we have not been similarly well served by the infighting of our tiers. To be fair, we created this problem by forcing the community colleges and universities to vie competitively for public funding separately, so it should not be that shocking that this evolved into an us vs. them dialogue. Instead of playing a game of one-upmanship, we are better served by harnessing our system and using it in series, much the way the original California Master Plan envisioned the articulation between their community colleges and two university levels. Yes, there are clearly equity issues involved in this, but we can\\'t talk about all issues in this one, very small, very trivial article.\\nIn fact, we may be able to serve our at-risk youth better in community colleges that teach better, in many ways, than in universities. In 2001, the Tennessee Higher Education Commission moved its remedial coursework to the community college in order to save money but also to keep higher education for \"higher education,\" and by 2012, 22 states had either eliminated funding or made other moves to relegate remedial class work to the community college system (Huse, Wright, Clark, & Hacker, 2005; Pant, 2012) . Some saw this as a knock against college access. In reality, it makes much more sense to let students who are on the academic cusp experiment at a much lower cost afforded by community colleges; institutions that are often in their own communities, that also reduce the extra costs associated with room and board. Virginia, for example, guarantees transfer to all state institutions, including the flagship University of Virginia, Virginia Tech, and the College of William & Mary, to students who complete an associate\\'s degree at a specified academic level. This gives more breathing room to students and also significantly reduces their costs during the first two years of their articulated four-year program. In the end, these students graduate with the parchment of the university.\\nUltimately, we need to decide, on a policy basis, who we want to go to college, who we want to succeed, and who will pay for it. Without delving more into the much-discussed issue of college costs, the amount carried by students and parents will continue to grow, as will the costs borne by states. The costs associated with sending more students to college is not trivial for our society. Another paper can discuss the philosophy of education and these other important questions. But for the institution, it is critical to understand student success and what to do about it. It isn\\'t about saving all students, but it is about changing the culture of an institution to do what can be done to all students that are admitted. If they come, they should be served with the highest regard for the highest reward.'}\n"
     ]
    }
   ],
   "source": [
    "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[-10:]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "print(type(papers))\n",
    "print(\n",
    "    papers[ np.random.choice(df.Id.values) ][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-attempt",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.dataset_label.iloc[3]\n",
    "\n",
    "# paper = papers[df.Id.iloc[4]]\n",
    "# ['\\n'.join([section['section_title'], section['text']]).split() for section in paper if section['text']][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-carnival",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_sample_text(jpth):\n",
    "    sections = json.loads(jpth.read_text())\n",
    "    text = '\\n'.join(section['text'] for section in sections)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-calibration",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s. ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data. Creating a U.S. crosswalk to this system has been a goal of the National Center for Education Statistics and the Office of Research since the late 197,,s, when the National Institute of Education (the predecessor agency to the Office of Educational Research and Improvement) began exploring the idea. The design and implementation of a workable crosswalk, however, awaited the advent of changes to the Classification of Instructional Programs (CIP) system. The 1990 revision of the CIP system laid the foundation for a workable international crosswalk. Adoption of the National Education Goals set global consciousness and international educational comparisons firml\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "print(load_sample_text(jpths_trn[0])[:1_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-tenant",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-tsunami",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_training_text(txt, lower=False, total_clean=False):\n",
    "    \"\"\"\n",
    "    similar to the default clean_text function but without lowercasing.\n",
    "    \"\"\"\n",
    "    txt = str(txt).lower() if lower else str(txt)\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt).strip()\n",
    "    if total_clean:\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-cannon",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle This competition awards 90 000\n",
      "hopkld 7 11 002\n"
     ]
    }
   ],
   "source": [
    "print(clean_training_text('@kaggle This competition awards $90,000!!!!.'))\n",
    "print(clean_training_text('HoPKLd + 7 ! 11,002', total_clean=True, lower=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-error",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def shorten_sentences(sentences, max_length=64, overlap=20):\n",
    "    '''\n",
    "    Args:\n",
    "        sentences (list): List of sentences.\n",
    "        max_length (int): Maximum number of words allowed for each sentence.\n",
    "        overlap (int): If a sentence exceeds `max_length`, we split it to multiple sentences with \n",
    "            this amount of overlapping.\n",
    "    '''\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > max_length:\n",
    "            for p in range(0, len(words), max_length - overlap):\n",
    "                short_sentences.append(' '.join(words[p:p+max_length]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-transport",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s', ' ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data']\n",
      "\n",
      "After: ['The International Standard Classification of Education, known by its acronym', 'its acronym ISCED, was developed by the United Nations Educational,', 'Nations Educational, Scientific, and Cultural Organization during the late 1960s', 'late 1960s and 1970s', 'ISCED was implemented in 1976 and is the recognized international', 'recognized international standard for reporting and interpreting education program data', 'program data']\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "sentences = load_sample_text(jpths_trn[0]).split('.')[:2]\n",
    "short_sentences = shorten_sentences(sentences, max_length=10, overlap=2)\n",
    "print('Before:', sentences)\n",
    "print()\n",
    "print('After:', short_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-break",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def find_sublist(big_list, small_list):\n",
    "    all_positions = []\n",
    "    for i in range(len(big_list) - len(small_list) + 1):\n",
    "        if small_list == big_list[i:i+len(small_list)]:\n",
    "            all_positions.append(i)\n",
    "    \n",
    "    return all_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-falls",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list = ['If', 'the', 'thing', 'above', 'is', 'below', 'that', 'thing', 'which', 'is',\n",
    "            'not', 'as', 'high', 'up', 'on', 'the', 'thing', 'above', 'when', 'it', 'is', \n",
    "            'underneath', 'them.']\n",
    "small_list = ['the', 'thing', 'above']\n",
    "\n",
    "find_sublist(big_list, small_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-directory",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-buffer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_classlabel():\n",
    "    '''\n",
    "    Labels for named entity recognition.\n",
    "        'O': Token not part of a phrase that mentions a dataset.\n",
    "        'I': Intermediate token of a phrase mentioning a dataset.\n",
    "        'B': First token of a phrase mentioning a dataset.\n",
    "    '''\n",
    "    return ClassLabel(names=['O', 'I', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-arbitration",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None)\n",
      "[1, 0, 2] 1\n",
      "B ['B', 'I', 'O']\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "print(classlabel)\n",
    "print(classlabel.str2int(['I', 'O', 'B']), classlabel.str2int('I'))\n",
    "print(classlabel.int2str(2), classlabel.int2str([2, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-supervision",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tag_sentence(sentence, labels, classlabel=None): \n",
    "    '''\n",
    "    requirement: both sentence and labels are already cleaned\n",
    "    '''\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n",
    "                                  for label in labels): # positive sample\n",
    "        nes = [classlabel.str2int('O')] * len(sentence_words)\n",
    "        for label in labels:\n",
    "            label_words = label.split()\n",
    "\n",
    "            all_pos = find_sublist(sentence_words, label_words)\n",
    "            for pos in all_pos:\n",
    "                nes[pos] = classlabel.str2int('B')\n",
    "                for i in range(pos+1, pos+len(label_words)):\n",
    "                    nes[i] = classlabel.str2int('I')\n",
    "\n",
    "        return True, list(zip(sentence_words, nes))\n",
    "        \n",
    "    else: # negative sample\n",
    "        nes = [classlabel.str2int('O')] * len(sentence_words)\n",
    "        return False, list(zip(sentence_words, nes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-mouse",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A label is found in the sentence: True\n",
      "(token, tag) pairs:\n",
      "[('The', 2), ('International', 1), ('Standard', 0), ('Classification', 0), ('of', 0), ('Education', 0), ('known', 0), ('by', 0), ('its', 0), ('acronym', 0), ('ISCED', 0), ('was', 0), ('developed', 0), ('by', 0), ('the', 0), ('United', 2), ('Nations', 1), ('Educational', 1), ('Scientific', 0), ('and', 0), ('Cultural', 2), ('Organization', 1), ('during', 0), ('the', 0), ('late', 0), ('1960s', 0), ('and', 0), ('1970s', 0)]\n"
     ]
    }
   ],
   "source": [
    "sentence = (\"The International Standard Classification of Education, known by its acronym ISCED, \"\n",
    "            \"was developed by the United Nations Educational, \"\n",
    "            \"Scientific, and Cultural Organization during the late 1960s and 1970s\")\n",
    "labels = ['The International', 'Cultural Organization', 'United Nations Educational']\n",
    "\n",
    "sentence = clean_training_text(sentence)\n",
    "labels = [clean_training_text(label) for label in labels]\n",
    "classlabel = get_ner_classlabel()\n",
    "found_any, token_tags = tag_sentence(sentence, labels, classlabel=classlabel)\n",
    "\n",
    "print('A label is found in the sentence:', found_any)\n",
    "print('(token, tag) pairs:')\n",
    "print(token_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-asbestos",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_sentences(paper, sentence_definition='sentence'):\n",
    "    if sentence_definition == 'sentence':\n",
    "        sentences = set(clean_training_text(sentence) \n",
    "                        for sec in paper for sentence in sec['text'].split('.') if sec['text'])\n",
    "    elif sentence_definition == 'section':\n",
    "        sentences = set(clean_training_text(sec['section_title'] + '\\n' + sec['text']) \n",
    "                        for sec in paper if sec['text'])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-address",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence definition = normal sentence\n",
      "302 ['', 'We therefore estimate the following test score production function 6 Our use of distance as the relevant cost component follows originally from Hotelling 1949 who proposed that travel cost which is a function of distance is an important determinant of the demand for recreational goods', 'In the education literature distance has been extensively used as a determinant for school attendance', 'To account for the panel nature of the data and to test for robustness we estimate panel models using both the Arellano Bond 1991 procedure for dynamic panel models and a semi parametric maximum likelihood estimator', 'That is the individual school s likelihood is where the second term is the initial condition', 'Although we lack comprehensive individual student tracking data necessary to follow all students longitudinally we have once lagged test scores for all students contributing current scores', 'This provides evidence that above average students are more likely to exit traditional schools for charters than below average students', 'Borland and Howsen 1992 find weak evidence that lower school district concentration leads to higher achievement', 'Hoxby 2002 examines the effects of vouchers in Milwaukee and charter schools in Michigan and Arizona and finds productivity and achievement gains from increased choice', 'According to the National Center for Education Statistics approximately 2 000 charter schools operated in 2001 Hoffman 2002']\n",
      "\n",
      "Sentence definition = paper section\n",
      "12 ['Abstract Federal No Child Left Behind legislation which enables students of low performing schools to exercise public school choice exemplifies a widespread belief that competing for students will spur public schools to higher achievement We investigate how the introduction of school choice in North Carolina via a dramatic increase in the number of charter schools across the state affects the performance of traditional public schools on statewide tests We find test score gains from competition that are robust to a variety of specifications The introduction of charter school competition causes an approximate one percent increase in the score which constitutes about one quarter of the average yearly growth', 'Is Selection Driving These Results One possible alternative explanation for improved traditional school achievement when a charter school opens nearby is migration from the traditional school to the charter by lower performing students Three pieces of evidence however suggest that if anything the opposite phenomenon occurs in our sample students switching from traditional to charter schools are above average performers We first look at within school variation in achievement over time Enrollment increases in traditional schools within ten kilometers of a charter school were almost two percent less than those in other traditional schools suggesting that some migration from the traditional school to the charter school occurs Repeating this comparison for the percentages of students eligible for free lunch and who are African American two populations with lower than average test scores we find that relative sizes of these populations increase in traditional schools within ten kilometers of a new charter To the extent that these populations proxy for below average scoring students our estimated gains to competition are in spite of rather than a consequence of a change in student population Next we explore this issue using student level data Although we lack comprehensive individual student tracking data necessary to follow all students longitudinally we have once lagged test scores for all students contributing current scores To examine the effect of attrition we calculate the average once lagged score for each school grade and test subject math and reading a measure of previous year ability For example we obtain the average 1998 99 math score for all students in school s grade 4 in 1999 2000 If there is no systematic exiting from traditional schools then the change in the once lagged score should be independent of whether a competing charter school opens That is if low performing students leave a traditional school for a new charter school in 1999 2000 then the change in average score from 1997 98 to 1998 99 for the students attending the traditional school in 1998 99 and 1999 2000 should be greater than the corresponding change for an otherwise identical school not facing new charter school competition This idea can be tested by estimating equation 9 where subscripts denote school s cohort c and time t As described above LAGSCORE represents the once lagged performance composite and COMPETE is a dummy for whether a charter school is located within 10 km Unobserved effects include time effects t school cohort effects s c and idiosyncratic schooltime cohort effects c s t Parameter thus measures whether the change in the lagged score differs by whether the school faced charter school competition If lower scoring students switch from traditional schools to competing charter schools and the result is an increase in the observed achievement then will be positive 9 for all six cohorts that we can follow longitudinally Of the twelve cohort subject pairs beginning with grade 3 in 1997 1998 and ending with grade 8 in 1998 1999 for both subjects reading and math only three experience a significant change in lagged test score all of which are lower after the introduction of a charter school This provides evidence that above average students are more likely to exit traditional schools for charters than below average students Finally for those students that we are able to identify we examine the performance of students who switched from a traditional school to a charter 16 We find that approximately 75 percent of those who switched had a higher score than the average score in the traditional school the year before they left This is direct evidence that charter induced growth in traditional school performance is not a manifestation of an exodus of low scoring students In sum traditional schools experience net gains in performance despite a decrease in average student quality in some cohorts suggesting that our estimated effects of charter school competition provide a lower bound for the true effect']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[100:110]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', df.Id)\n",
    "paper = papers[df.Id.iloc[3]]\n",
    "print('Sentence definition = normal sentence')\n",
    "sentences = extract_sentences(paper, sentence_definition='sentence')\n",
    "print(len(sentences), list(sentences)[:10], end='\\n\\n')\n",
    "print('Sentence definition = paper section')\n",
    "sentences = extract_sentences(paper, sentence_definition='section')\n",
    "print(len(sentences), list(sentences)[:2], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-crack",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_paper_ner_data(paper, labels, classlabel=None,\n",
    "                       sentence_definition='sentence', max_length=64, overlap=20):\n",
    "    '''\n",
    "    Get NER data for a paper.\n",
    "    '''\n",
    "    labels = [clean_training_text(label) for label in labels]\n",
    "    sentences = extract_sentences(paper, sentence_definition=sentence_definition)\n",
    "    sentences = shorten_sentences(sentences, max_length=max_length, overlap=overlap) \n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "\n",
    "    cnt_pos, cnt_neg, ner_data = 0, 0, []\n",
    "    for sentence in sentences:\n",
    "        is_positive, tags = tag_sentence(sentence, labels, classlabel=classlabel)\n",
    "        if is_positive:\n",
    "            cnt_pos += 1\n",
    "            ner_data.append(tags)\n",
    "        elif any(word in sentence.lower() for word in ['data', 'study']): \n",
    "            ner_data.append(tags)\n",
    "            cnt_neg += 1    \n",
    "    return cnt_pos, cnt_neg, ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-graham",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1\n",
      "[512, 512, 512, 512, 259, 101, 303, 512, 512, 49]\n",
      "[('that', 0), ('ADNI', 2), ('researchers', 0), ('are', 0), ('ready', 0), ('with', 0), ('guidance', 0), ('and', 0), ('ongoing', 0), ('research', 0), ('to', 0), ('return', 0), ('amyloid', 0), ('imaging', 0), ('research', 0), ('results', 0), ('to', 0), ('ADNI', 2), ('participants', 0), ('and', 0), ('more', 0), ('generally', 0), ('other', 0), ('biomarker', 0), ('results', 0), ('as', 0), ('well', 0), ('These', 0), ('findings', 0), ('also', 0), ('suggest', 0), ('that', 0), ('biomarker', 0), ('results', 0), ('can', 0), ('be', 0), ('disclosed', 0), ('as', 0), ('part', 0), ('of', 0), ('clinical', 0), ('trials', 0), ('designed', 0), ('to', 0), ('prevent', 0), ('cognitive', 0), ('decline', 0), ('10', 0), ('11', 0)]\n"
     ]
    }
   ],
   "source": [
    "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[230:240]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "\n",
    "idx = 8\n",
    "paper = papers[df.Id.iloc[idx]]\n",
    "labels = df.dataset_label.iloc[idx].split('|')\n",
    "cnt_pos, cnt_neg, ner_data = get_paper_ner_data(paper, labels, classlabel=classlabel,\n",
    "                                                sentence_definition='section', max_length=512, overlap=20)\n",
    "print(cnt_pos, cnt_neg)\n",
    "print([len(sec) for sec in ner_data])\n",
    "print(ner_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-leave",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_data(papers, df=None, classlabel=None, shuffle=True, \n",
    "                 sentence_definition='sentence', max_length=64, overlap=20):\n",
    "    '''\n",
    "    Args:\n",
    "        papers (dict): Like that returned by `load_papers`.\n",
    "        df (pd.DataFrame): Competition's train.csv or a subset of it.\n",
    "    '''\n",
    "    cnt_pos, cnt_neg = 0, 0 \n",
    "    ner_data = []\n",
    "\n",
    "    tqdm._instances.clear()\n",
    "    pbar = tqdm(total=len(df))\n",
    "    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():\n",
    "        paper = papers[id]\n",
    "        labels = dataset_label.split('|')\n",
    "                \n",
    "        cnt_pos_, cnt_neg_, ner_data_ = get_paper_ner_data(\n",
    "            paper, labels, classlabel=classlabel, \n",
    "            sentence_definition=sentence_definition, max_length=max_length, overlap=overlap)\n",
    "        cnt_pos += cnt_pos_\n",
    "        cnt_neg += cnt_neg_\n",
    "        ner_data.extend(ner_data_)\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(ner_data)\n",
    "    return cnt_pos, cnt_neg, ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-wesley",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [00:00<00:00, 107.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive count: 168.   Negative count: 2574\n",
      "[('Likewise', 0), ('there', 0), ('is', 0), ('a', 0), ('familiarity', 0), ('with', 0), ('a', 0), ('variety', 0), ('of', 0), ('studies', 0), ('that', 0), ('provide', 0), ('specific', 0), ('data', 0), ('on', 0), ('the', 0), ('academic', 0), ('achievement', 0), ('of', 0), ('Catholic', 0), ('school', 0), ('students', 0)]\n",
      "CPU times: user 1.03 s, sys: 53.6 ms, total: 1.09 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:100]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "cnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False,\n",
    "                                          sentence_definition='sentence', max_length=64, overlap=20)\n",
    "print(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\n",
    "print(ner_data[250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-priority",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 171 positives + 910 negatives: 100%|██████████| 100/100 [00:00<00:00, 124.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive count: 171.   Negative count: 910\n",
      "[('Reason', 0), ('for', 0), ('review', 0), ('This', 0), ('study', 0), ('was', 0), ('identified', 0), ('for', 0), ('review', 0), ('by', 0), ('receiving', 0), ('media', 0), ('attention', 0)]\n",
      "CPU times: user 836 ms, sys: 95 ms, total: 931 ms\n",
      "Wall time: 940 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:100]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "cnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False, \n",
    "                                          sentence_definition='section', max_length=512, overlap=20)\n",
    "print(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\n",
    "print(ner_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-singer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def write_ner_json(ner_data, pth=Path('train_ner.json')):\n",
    "    with open(pth, 'w') as f:\n",
    "        for row in ner_data:\n",
    "            words, nes = list(zip(*row))\n",
    "            row_json = {'tokens' : words, 'ner_tags' : nes}\n",
    "            json.dump(row_json, f)\n",
    "            f.write('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-revision",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tokens\": [\"There\", \"is\", \"no\", \"dataset\", \"here\"], \"ner_tags\": [0, 0, 0, 0, 0]}\r\n",
      "{\"tokens\": [\"Load\", \"the\", \"UN\", \"Trade\", \"Development\", \"into\", \"view\"], \"ner_tags\": [0, 0, 2, 1, 1, 0, 0]}\r\n"
     ]
    }
   ],
   "source": [
    "ner_data = [\n",
    "    [('There', 0), ('is', 0), ('no', 0), ('dataset', 0), ('here', 0)], \n",
    "    [('Load', 0), ('the', 0), ('UN', 2), ('Trade', 1), ('Development', 1), ('into', 0), ('view', 0)]\n",
    "]\n",
    "write_ner_json(ner_data, pth=Path('/kaggle/tmp_ner.json'))\n",
    "! cat /kaggle/tmp_ner.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-south",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_ner_datasets(data_files=None):\n",
    "    datasets = load_dataset('json', data_files=data_files)\n",
    "    classlabel = get_ner_classlabel()\n",
    "    for split, dataset in datasets.items():\n",
    "        dataset.features['ner_tags'].feature = classlabel\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-works",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-a8d404973fcc5aa0/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a119b98d4ecf414c99e0fd58f89d6171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30bc1ff704d4057b67c75860874dd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a8d404973fcc5aa0/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None)}\n",
      "\n",
      "{'tokens': ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view'], 'ner_tags': [0, 0, 2, 1, 1, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\n",
    "print(datasets['valid'].features)\n",
    "print()\n",
    "print(datasets['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-benjamin",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_tokenizer(model_checkpoint='bert-base-cased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-bennett",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfea212483642a7ab224800b8982eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d493312b91441aca6979205090a3035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fae04d4eef44a2c81e3a832368918e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f7b383b5e645cbb3fba7866dec0c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'input_ids': [101, 138, 188, 21943, 9930, 1104, 1234, 9026, 1121, 1103, 2286, 6194, 3499, 1113, 6356, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "{'input_ids': [101, 144, 6512, 9436, 24372, 1317, 185, 12937, 2042, 15520, 1114, 8626, 2330, 1447, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\n",
    "print(\n",
    "    tokenizer(\"A smattering of people descended from the midday boat on Monday.\"))\n",
    "print()\n",
    "print(\n",
    "    tokenizer(\"Giglio boasts several pristine bays with crystal clear water\".split(), is_split_into_words=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-height",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_and_align_labels(examples, tokenizer=None, label_all_tokens=True):\n",
    "    '''\n",
    "    Adds a new field called 'labels' that are the NER tags to the tokenized input.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer.\n",
    "        examples (datasets.arrow_dataset.Dataset): Dataset.\n",
    "        label_all_tokens (bool): If True, all sub-tokens are given the same tag as the \n",
    "            first sub-token, otherwise all but the first sub-token are given the tag\n",
    "            -100.\n",
    "    '''\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-interaction",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]]}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808da4ac846744d0a8a713764350c5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba67cdc7765c40cb9cf750dc9db5e905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'ner_tags': [[0, 0, 0, 0, 0], [0, 0, 2, 1, 1, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tokens': [['There', 'is', 'no', 'dataset', 'here'], ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view']]}\n",
      "\n",
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'ner_tags': [[0, 0, 0, 0, 0], [0, 0, 2, 1, 1, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tokens': [['There', 'is', 'no', 'dataset', 'here'], ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view']]}\n",
      "{'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\n",
    "tokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\n",
    "\n",
    "print(tokenize_and_align_labels(datasets['train'][:], tokenizer, label_all_tokens=True), end='\\n\\n')\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\n",
    "\n",
    "print(tokenized_datasets['train'][:], end='\\n\\n')\n",
    "print(tokenized_datasets['valid'][:])\n",
    "print(tokenized_datasets['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-thickness",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4da00cca9744415ae641e2fd072fce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n",
       " 'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.3333333333333333}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric('seqeval')\n",
    "\n",
    "predictions = [['O', 'O', 'B', 'I', 'I', 'O']]\n",
    "references = [['B', 'I', 'B', 'O', 'O', 'O']]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-tomato",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def jaccard_similarity(s1, s2):\n",
    "    l1 = set(s1.split(\" \"))\n",
    "    l2 = set(s2.split(\" \"))\n",
    "    intersection = len(list(l1.intersection(l2)))\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-louisville",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity('USGS Frog Counts Data', 'USGA Croc Counts Data') == 1 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-anthropology",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_metrics(p, metric=None, label_list=None):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-sleeping",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.19658119658119658,\n",
       " 'recall': 0.20909090909090908,\n",
       " 'f1': 0.2026431718061674,\n",
       " 'accuracy': 0.35}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "\n",
    "batch_size = 8\n",
    "max_sentence_length = 30\n",
    "\n",
    "predictions = np.random.randn(batch_size, max_sentence_length, classlabel.num_classes)\n",
    "label_ids = np.random.randint(low=0, high=classlabel.num_classes, \n",
    "                              size=(batch_size, max_sentence_length), dtype=np.int16)\n",
    "\n",
    "p = (predictions, label_ids)\n",
    "metric = load_metric('seqeval')\n",
    "compute_metrics(p, metric=metric, label_list=classlabel.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-lover",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NER training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-swing",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 12 positives + 100 negatives: 100%|██████████| 2/2 [00:00<00:00, 47.74it/s]\n",
      "Training data size: 2 positives + 51 negatives: 100%|██████████| 2/2 [00:00<00:00, 96.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train.  Positive count: 12.  Negative count: 100.\n",
      "Valid.  Positive count: 2.  Negative count: 51.\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "train_meta = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:4]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', train_meta.Id)\n",
    "\n",
    "valid_cutoff = int(.50 * len(train_meta))\n",
    "valid_meta = train_meta.iloc[:valid_cutoff].reset_index(drop=True)\n",
    "train_meta = train_meta.iloc[valid_cutoff:].reset_index(drop=True)\n",
    "\n",
    "train_cnt_pos, train_cnt_neg, train_ner_data = get_ner_data(papers, df=train_meta, classlabel=classlabel)\n",
    "valid_cnt_pos, valid_cnt_neg, valid_ner_data = get_ner_data(papers, df=valid_meta, classlabel=classlabel)\n",
    "print(f'Train.  Positive count: {train_cnt_pos}.  Negative count: {train_cnt_neg}.')\n",
    "print(f'Valid.  Positive count: {valid_cnt_pos}.  Negative count: {valid_cnt_neg}.')\n",
    "\n",
    "write_ner_json(train_ner_data, pth='train_ner.json')\n",
    "write_ner_json(valid_ner_data, pth='valid_ner.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-terrorism",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-d39e4bf564f26bd9/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478e0fd4ab5c4b12bc59ebf4e11fdc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672e1ae964dd4b7b9e4c3ddab4af0097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-d39e4bf564f26bd9/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9cecf6aefa47adb7142830744df9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ccdd66302248a48a82f8099189d4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc24206c99e84eebbb36e0507af7158c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976da26c944b499a86857bfef1f24bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcb24a2d6aa43fd8fd2485617e35f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c4234e13204fa3bc97f690f38548f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2361e5f46941639063bd06379ca7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [00:38<00:00,  2.62it/s] \n",
      "Training data size: 171 positives + 910 negatives: 100%|██████████| 100/100 [00:36<00:00,  2.71it/s] \n",
      "Training data size: 2 positives + 51 negatives: 100%|██████████| 2/2 [00:21<00:00, 10.63s/it]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_ner_datasets(data_files={'train':'train_ner.json', 'valid':'valid_ner.json'})\n",
    "\n",
    "model_checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = create_tokenizer(model_checkpoint)\n",
    "tokenized_datasets = datasets.map(\n",
    "    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=classlabel.num_classes)\n",
    "\n",
    "metric = load_metric('seqeval')\n",
    "\n",
    "args = TrainingArguments(output_dir='test_ner', num_train_epochs=2, \n",
    "                         learning_rate=2e-5, weight_decay=0.01,\n",
    "                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n",
    "                         save_strategy='epoch', save_total_limit=6)\n",
    "\n",
    "trainer = Trainer(model=model, args=args, \n",
    "                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n",
    "                  data_collator=data_collator, tokenizer=tokenizer, \n",
    "                  compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-tennis",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 01:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.914700</td>\n",
       "      <td>0.435770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>4.317800</td>\n",
       "      <td>12.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.311900</td>\n",
       "      <td>0.270009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.988406</td>\n",
       "      <td>4.348800</td>\n",
       "      <td>12.187000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14, training_loss=0.5302720453057971, metrics={'train_runtime': 76.5521, 'train_samples_per_second': 0.183, 'total_flos': 5282215376256.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1423454208, 'train_mem_cpu_peaked_delta': 289419264})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-theater",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "drwxr-xr-x 2 root root 4096 May 29 00:56 checkpoint-7\r\n",
      "drwxr-xr-x 2 root root 4096 May 29 00:57 checkpoint-14\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lrt test_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-warning",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 01:11, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>0.134594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.988406</td>\n",
       "      <td>4.299300</td>\n",
       "      <td>12.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.100761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.988406</td>\n",
       "      <td>4.284400</td>\n",
       "      <td>12.371000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28, training_loss=0.06898573573146548, metrics={'train_runtime': 76.1015, 'train_samples_per_second': 0.368, 'total_flos': 10627016242752.0, 'epoch': 4.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 850100224, 'train_mem_cpu_peaked_delta': 454672384})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(output_dir='test_ner', num_train_epochs=4, \n",
    "                         learning_rate=2e-5, weight_decay=0.01,\n",
    "                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n",
    "                         save_strategy='epoch', save_total_limit=6)\n",
    "\n",
    "trainer = Trainer(model=model, args=args, \n",
    "                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n",
    "                  data_collator=data_collator, tokenizer=tokenizer, \n",
    "                  compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))\n",
    "trainer.train(resume_from_checkpoint='/kaggle/working/test_ner/checkpoint-14/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-preliminary",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='8' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.10076110064983368,\n",
       " 'eval_precision': 0.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_accuracy': 0.9884057971014493,\n",
       " 'eval_runtime': 4.4655,\n",
       " 'eval_samples_per_second': 11.869,\n",
       " 'epoch': 4.0,\n",
       " 'eval_mem_cpu_alloc_delta': 139264,\n",
       " 'eval_mem_cpu_peaked_delta': 0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-interface",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 4}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.9884057971014493}\n"
     ]
    }
   ],
   "source": [
    "predictions, label_ids, _ = trainer.predict(tokenized_datasets['valid'])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_predictions = [[classlabel.names[p] for p, i in zip(prediction, label_id) if i != -100]\n",
    "                    for prediction, label_id in zip(predictions, label_ids)]\n",
    "\n",
    "true_labels = [[classlabel.names[i] for i in label_id if i != -100] for label_id in label_ids]\n",
    "\n",
    "print(true_predictions[0])\n",
    "print(true_labels[0])\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-capacity",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NER inference\n",
    "\n",
    "**Turn off the Internet here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-triple",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_inference_data(papers, sample_submission, classlabel=None):\n",
    "    '''\n",
    "    Args:\n",
    "        papers (dict): Each list in this dictionary consists of the section of a paper.\n",
    "        sample_submission (pd.DataFrame): Competition 'sample_submission.csv'.\n",
    "    Returns:\n",
    "        test_rows (list): Each list in this list is of the form: \n",
    "             [('goat', 0), ('win', 0), ...] and represents a sentence.  \n",
    "        paper_length (list): Number of sentences in each paper.\n",
    "    '''\n",
    "    test_rows = [] # test data in NER format\n",
    "    paper_length = [] # store the number of sentences each paper has\n",
    "\n",
    "    for paper_id in sample_submission['Id']:\n",
    "        # load paper\n",
    "        paper = papers[paper_id]\n",
    "\n",
    "        # extract sentences\n",
    "        sentences = [clean_training_text(sentence) for section in paper \n",
    "                     for sentence in section['text'].split('.')\n",
    "                    ]\n",
    "        sentences = shorten_sentences(sentences) # make sentences short\n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "\n",
    "        # collect all sentences in json\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split()\n",
    "            dummy_tags = [classlabel.str2int('O')]*len(sentence_words)\n",
    "            test_rows.append(list(zip(sentence_words, dummy_tags)))\n",
    "\n",
    "        # track which sentence belongs to which data point\n",
    "        paper_length.append(len(sentences))\n",
    "\n",
    "    print(f'total number of sentences: {len(test_rows)}')\n",
    "    return test_rows, paper_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-sapphire",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sentences: 367\n",
      "[[('A', 0), ('recent', 0), ('large', 0), ('genomewide', 0), ('association', 0), ('study', 0), ('GWAS', 0), ('reported', 0), ('a', 0), ('genome', 0), ('wide', 0), ('significant', 0), ('locus', 0), ('for', 0), ('years', 0), ('of', 0), ('education', 0), ('which', 0), ('subsequently', 0), ('demonstrated', 0), ('association', 0), ('to', 0), ('general', 0), ('cognitive', 0), ('ability', 0), ('g', 0), ('in', 0), ('overlapping', 0), ('cohorts', 0)], [('The', 0), ('current', 0), ('study', 0), ('was', 0), ('designed', 0), ('to', 0), ('test', 0), ('whether', 0), ('GWAS', 0), ('hits', 0), ('for', 0), ('educational', 0), ('attainment', 0), ('are', 0), ('involved', 0), ('in', 0), ('general', 0), ('cognitive', 0), ('ability', 0), ('in', 0), ('an', 0), ('independent', 0), ('large', 0), ('scale', 0), ('collection', 0), ('of', 0), ('cohorts', 0)], [('We', 0), ('next', 0), ('conducted', 0), ('meta', 0), ('analyses', 0), ('with', 0), ('24', 0), ('189', 0), ('individuals', 0), ('with', 0), ('neurocognitive', 0), ('data', 0), ('from', 0), ('the', 0), ('educational', 0), ('attainment', 0), ('studies', 0), ('and', 0), ('then', 0), ('with', 0), ('53', 0), ('188', 0), ('largely', 0), ('independent', 0), ('individuals', 0), ('from', 0), ('a', 0), ('recent', 0), ('GWAS', 0), ('of', 0), ('cognition', 0)]]\n",
      "[34, 151, 98, 84]\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test', sample_submission.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "test_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel)\n",
    "print(test_rows[:3])\n",
    "print(paper_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-equity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def ner_predict(test_rows, tokenizer=None, model=None, metric=None):\n",
    "    classlabel = get_ner_classlabel()\n",
    "    \n",
    "    write_ner_json(test_rows, pth='test_ner.json')\n",
    "    datasets = load_ner_datasets(data_files={'test':'test_ner.json'})\n",
    "    print('Tokenizing testset...')\n",
    "    tokenized_datasets = datasets.map(\n",
    "        partial(tokenize_and_align_labels,tokenizer=tokenizer, label_all_tokens=True), \n",
    "        batched=True) \n",
    "\n",
    "    print('Creating data collator...')\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    print('Creating (dummy) training arguments...')\n",
    "    args = TrainingArguments(output_dir='test_ner', num_train_epochs=3, \n",
    "                             learning_rate=2e-5, weight_decay=0.01,\n",
    "                             per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                             evaluation_strategy='epoch', logging_steps=4, report_to='none', \n",
    "                             save_strategy='epoch', save_total_limit=6)\n",
    "\n",
    "    print('Creating trainer...')\n",
    "    trainer = Trainer(model=model, args=args, \n",
    "                      train_dataset=tokenized_datasets['test'], eval_dataset=tokenized_datasets['test'], \n",
    "                      data_collator=data_collator, tokenizer=tokenizer, \n",
    "                      compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))\n",
    "\n",
    "    print('Predicting on test samples...')\n",
    "    predictions, label_ids, _ = trainer.predict(tokenized_datasets['test'])\n",
    "    predictions = predictions.argmax(axis=2)\n",
    "    true_predictions = [\n",
    "        [p for p, i in zip(prediction, label_id) if i != -100]\n",
    "        for prediction, label_id in zip(predictions, label_ids)]\n",
    "\n",
    "    return true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-ceremony",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'test_ner/checkpoint-28/'\n",
    "\n",
    "tokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "metric = load_metric('seqeval')\n",
    "# matric = load_metric('seqeval', cache_dir='/root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-scheduling",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exporting the cached metric \n",
    "\n",
    "# %cd /root/.cache\n",
    "# ! zip -r huggingface_cache.zip huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/\n",
    "# %cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-benchmark",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-407d57de7fdee7ca/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f85ee5ce3c745a7808171d18113219c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-407d57de7fdee7ca/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "Tokenizing testset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75653553ddd643c596ebd2bb2f3e37e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating data collator...\n",
      "Creating (dummy) training arguments...\n",
      "Creating trainer...\n",
      "Predicting on test samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "bert_outputs = ner_predict(test_rows, tokenizer=tokenizer, model=model, metric=metric)\n",
    "print(bert_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-project",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_batch = 100\n",
    "\n",
    "# bert_outputs = []\n",
    "# for i in range(0, len(test_rows), predict_batch):\n",
    "#     print(f'\\rPredicting on samples {i} to {i + predict_batch}...', flush=True)\n",
    "#     b = ner_predict(test_rows[i:i + predict_batch], model=model, tokenizer=tokenizer, metric=metric)\n",
    "#     bert_outputs.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-offering",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=None):\n",
    "    '''\n",
    "    Returns:\n",
    "        bert_dataset_labels (list): Each element is a set consisting of labels predicted\n",
    "            by the model.\n",
    "    '''\n",
    "    test_sentences = [list(zip(*row))[0] for row in test_rows]\n",
    "#     test_sentences = [row['tokens'] for row in test_rows]\n",
    "    \n",
    "    bert_dataset_labels = [] # store all dataset labels for each publication\n",
    "\n",
    "    for length in paper_length:\n",
    "        labels = set()\n",
    "        for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n",
    "            curr_phrase = ''\n",
    "            for word, tag in zip(sentence, pred):\n",
    "                if tag == classlabel.str2int('B'): # start a new phrase\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "                    curr_phrase = word\n",
    "                elif tag == classlabel.str2int('I') and curr_phrase: # continue the phrase\n",
    "                    curr_phrase += ' ' + word\n",
    "                else: # end last phrase (if any)\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "            # check if the label is the suffix of the sentence\n",
    "            if curr_phrase:\n",
    "                labels.add(curr_phrase)\n",
    "                curr_phrase = ''\n",
    "\n",
    "        # record dataset labels for this publication\n",
    "        bert_dataset_labels.append(labels)\n",
    "\n",
    "        del test_sentences[:length], bert_outputs[:length]\n",
    "        \n",
    "    return bert_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-somalia",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Tigers EcoNAX dataset', 'present all the'}, {'WGS Equality Definitiveness Dataset'}]\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "sentences = ['They do not present all the features', \n",
    "             'Despite the pretraining on the Tigers EcoNAX dataset',\n",
    "             'Weirdly there has been lots of studies based on WGS Equality Definitiveness Dataset']\n",
    "paper_length = [2, 1]\n",
    "test_rows = [[(word, 0) for word in sentence.split()] for sentence in sentences]\n",
    "bert_outputs = [[0, 0, 0, 2, 1, 1, 0],\n",
    "                [0, 0, 0, 0, 0, 2, 1, 1],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1]]\n",
    "for i, row in enumerate(test_rows):\n",
    "    assert len(row) == len(bert_outputs[i])\n",
    "\n",
    "bert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=classlabel)\n",
    "print(bert_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-ladder",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_bert_labels(bert_dataset_labels):\n",
    "    '''\n",
    "    When several labels for a paper are too similar, keep just one of them.\n",
    "    '''\n",
    "    filtered_bert_labels = []\n",
    "\n",
    "    for labels in bert_dataset_labels:\n",
    "        filtered = []\n",
    "\n",
    "        for label in sorted(labels, key=len):\n",
    "            label = clean_training_text(label, lower=True)\n",
    "            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n",
    "                filtered.append(label)\n",
    "\n",
    "        filtered_bert_labels.append('|'.join(filtered))\n",
    "    return filtered_bert_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-richards",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moma artists|housing market|moma artists catalogue',\n",
       " 'deep sea rock salts|rhs fertiliser index']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset_labels = [{'moma artists catalogue', 'moma artists', 'housing market'},\n",
    "                       {'rhs flowers fertiliser index', 'deep sea rock salts', 'rhs fertiliser index'}]\n",
    "\n",
    "filter_bert_labels(bert_dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-madison",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Literal matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-breakfast",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_knowledge_bank(pth):\n",
    "    '''\n",
    "    Args:\n",
    "        pth (str): Path to meta data like 'train.csv', which\n",
    "        needs to have columns: 'dataset_title', 'dataset_label', and 'cleaned_label'.\n",
    "        \n",
    "    Returns:\n",
    "        all_labels (set): All possible strings associated with a dataset from the meta data.\n",
    "    '''\n",
    "    df = load_train_meta(pth, group_id=False)\n",
    "    all_labels = set()\n",
    "    for label_1, label_2, label_3 in df[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n",
    "        all_labels.add(str(label_1).lower())\n",
    "        all_labels.add(str(label_2).lower())\n",
    "        all_labels.add(str(label_3).lower())\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-pillow",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "['2019 ncov complete genome sequences', '2019 ncov genome sequence', '2019 ncov genome sequences', '2019-ncov complete genome sequences', '2019-ncov genome sequence', '2019-ncov genome sequences', 'adni', 'advanced national seismic system (anss) comprehensive catalog (comcat)', 'advanced national seismic system anss comprehensive catalog comcat ', 'advanced national seismic system comprehensive catalog']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "print(len(all_labels))\n",
    "print(sorted(all_labels)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-prince",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def literal_match(paper, all_labels):\n",
    "    '''\n",
    "    Args:\n",
    "        paper ()\n",
    "    '''\n",
    "    text_1 = '. '.join(section['text'] for section in paper).lower()\n",
    "    text_2 = clean_training_text(text_1, lower=True, total_clean=True)\n",
    "    \n",
    "    labels = set()\n",
    "    for label in all_labels:\n",
    "        if label in text_1 or label in text_2:\n",
    "            labels.add(clean_training_text(label, lower=True, total_clean=True))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-minneapolis",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'trends in international mathematics and science study|nces common core of data|common core of data',\n",
       " 'sea lake and overland surges from hurricanes|slosh model|noaa storm surge inundation',\n",
       " 'rural urban continuum codes']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n",
    "\n",
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "\n",
    "literal_preds = []\n",
    "for paper_id in sample_submission.Id:\n",
    "    paper = papers[paper_id]\n",
    "    literal_preds.append('|'.join(literal_match(paper, all_labels)))\n",
    "    \n",
    "literal_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-thread",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overall prediction for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-silver",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def combine_matching_and_bert(literal_preds, filtererd_bert_labels):\n",
    "    '''\n",
    "    For a given sentence, if there's a literal match, use that as the final\n",
    "    prediction for the sentence.  If there isn't a literal match,\n",
    "    use what the model predicts.\n",
    "    '''\n",
    "    final_predictions = []\n",
    "    for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n",
    "        if literal_match:\n",
    "            final_predictions.append(literal_match)\n",
    "        else:\n",
    "            final_predictions.append(bert_pred)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-potato",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mongolian racing cars|reallife headphones',\n",
       " 'hifi dataset|headphones collection data']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_preds = ['mongolian racing cars|reallife headphones', '']\n",
    "filtered_bert_labels = ['data|dataset', 'hifi dataset|headphones collection data']\n",
    "combine_matching_and_bert(literal_preds, filtered_bert_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-sleeping",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing NER inference data...\n",
      "total number of sentences: 367\n",
      "Loading model, tokenizer, and metric...\n",
      "Predicting on each sentence...\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-24beded7b19a6d1a/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1353751147954c34966e1d0382e942e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-24beded7b19a6d1a/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "Tokenizing testset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591d37bd237d43fbb4853feec282190f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating data collator...\n",
      "Creating (dummy) training arguments...\n",
      "Creating trainer...\n",
      "Predicting on test samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predicted labels for each article...\n",
      "Keeping just one of labels that are too similar to each other...\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "\n",
    "print('Preparing NER inference data...')\n",
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n",
    "test_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel)\n",
    "\n",
    "print('Loading model, tokenizer, and metric...')\n",
    "model_checkpoint = 'test_ner/checkpoint-28/'\n",
    "tokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "metric = load_metric('seqeval')\n",
    "\n",
    "print('Predicting on each sentence...')\n",
    "bert_outputs = ner_predict(test_rows, tokenizer=tokenizer, model=model, metric=metric)\n",
    "\n",
    "print('Getting predicted labels for each article...')\n",
    "bert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=classlabel)\n",
    "\n",
    "print('Keeping just one of labels that are too similar to each other...')\n",
    "filtered_bert_labels = filter_bert_labels(bert_dataset_labels)\n",
    "\n",
    "sample_submission['PredictionString'] = filtered_bert_labels\n",
    "\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-thompson",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id PredictionString\n",
       "0  2100032a-7c33-4bff-97ef-690822c43466                 \n",
       "1  2f392438-e215-4169-bebf-21ac4ff253e1                 \n",
       "2  3f316b38-1a24-45a9-8d8c-4e05a42257c6                 \n",
       "3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60                 "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-lending",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reference\n",
    "- https://www.kaggle.com/tungmphung/pytorch-bert-for-named-entity-recognition/notebook\n",
    "- https://www.kaggle.com/tungmphung/coleridge-matching-bert-ner/notebook\n",
    "- https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "- https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin\n",
    "- https://huggingface.co/docs/datasets/loading_metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-opera",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
