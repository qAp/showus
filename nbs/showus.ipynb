{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "discrete-investor",
   "metadata": {
    "papermill": {
     "duration": 0.035618,
     "end_time": "2021-05-17T08:27:48.214886",
     "exception": false,
     "start_time": "2021-05-17T08:27:48.179268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interior-garlic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:27:48.296558Z",
     "iopub.status.busy": "2021-05-17T08:27:48.295656Z",
     "iopub.status.idle": "2021-05-17T08:27:48.297378Z",
     "shell.execute_reply": "2021-05-17T08:27:48.297845Z"
    },
    "papermill": {
     "duration": 0.046417,
     "end_time": "2021-05-17T08:27:48.298128",
     "exception": false,
     "start_time": "2021-05-17T08:27:48.251711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#default_exp showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "damaged-revision",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:27:48.390923Z",
     "iopub.status.busy": "2021-05-17T08:27:48.380746Z",
     "iopub.status.idle": "2021-05-17T08:28:24.451939Z",
     "shell.execute_reply": "2021-05-17T08:28:24.452418Z"
    },
    "papermill": {
     "duration": 36.121522,
     "end_time": "2021-05-17T08:28:24.452574",
     "exception": false,
     "start_time": "2021-05-17T08:27:48.331052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\r\n",
      "Installing collected packages: fsspec\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 0.8.7\r\n",
      "    Uninstalling fsspec-0.8.7:\r\n",
      "      Successfully uninstalled fsspec-0.8.7\r\n",
      "Successfully installed fsspec-2021.4.0\r\n",
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.4.0)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.59.0\r\n",
      "    Uninstalling tqdm-4.59.0:\r\n",
      "      Successfully uninstalled tqdm-4.59.0\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Installing collected packages: tokenizers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.10.2\r\n",
      "    Uninstalling tokenizers-0.10.2:\r\n",
      "      Successfully uninstalled tokenizers-0.10.2\r\n",
      "Successfully installed tokenizers-0.10.1\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2021.3.17)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.45)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (4.0.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.5.1\r\n",
      "    Uninstalling transformers-4.5.1:\r\n",
      "      Successfully uninstalled transformers-4.5.1\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\n",
    "! pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n",
    "! pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n",
    "! pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n",
    "! pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "about-directory",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:24.539523Z",
     "iopub.status.busy": "2021-05-17T08:28:24.538884Z",
     "iopub.status.idle": "2021-05-17T08:28:25.516138Z",
     "shell.execute_reply": "2021-05-17T08:28:25.515290Z"
    },
    "papermill": {
     "duration": 1.024217,
     "end_time": "2021-05-17T08:28:25.516274",
     "exception": false,
     "start_time": "2021-05-17T08:28:24.492057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers, seqeval\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opponent-election",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:25.605535Z",
     "iopub.status.busy": "2021-05-17T08:28:25.601504Z",
     "iopub.status.idle": "2021-05-17T08:28:26.293327Z",
     "shell.execute_reply": "2021-05-17T08:28:26.292844Z"
    },
    "papermill": {
     "duration": 0.736584,
     "end_time": "2021-05-17T08:28:26.293458",
     "exception": false,
     "start_time": "2021-05-17T08:28:25.556874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /kaggle/input/coleridge-packages/my_seqeval.py ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-luther",
   "metadata": {
    "papermill": {
     "duration": 0.040362,
     "end_time": "2021-05-17T08:28:26.375777",
     "exception": false,
     "start_time": "2021-05-17T08:28:26.335415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hollywood-disorder",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:26.464345Z",
     "iopub.status.busy": "2021-05-17T08:28:26.463706Z",
     "iopub.status.idle": "2021-05-17T08:28:26.466229Z",
     "shell.execute_reply": "2021-05-17T08:28:26.465744Z"
    },
    "papermill": {
     "duration": 0.048748,
     "end_time": "2021-05-17T08:28:26.466353",
     "exception": false,
     "start_time": "2021-05-17T08:28:26.417605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "Path.ls = lambda pth: list(pth.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-messenger",
   "metadata": {
    "papermill": {
     "duration": 0.042281,
     "end_time": "2021-05-17T08:28:26.550222",
     "exception": false,
     "start_time": "2021-05-17T08:28:26.507941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "large-stanford",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:26.638681Z",
     "iopub.status.busy": "2021-05-17T08:28:26.638044Z",
     "iopub.status.idle": "2021-05-17T08:28:26.640948Z",
     "shell.execute_reply": "2021-05-17T08:28:26.640355Z"
    },
    "papermill": {
     "duration": 0.049829,
     "end_time": "2021-05-17T08:28:26.641091",
     "exception": false,
     "start_time": "2021-05-17T08:28:26.591262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_train_meta(pth, group_id=True):\n",
    "    df = pd.read_csv(pth)\n",
    "    if group_id:\n",
    "        df = df.groupby('Id').agg({'pub_title': 'first', 'dataset_title': '|'.join, \n",
    "                                   'dataset_label': '|'.join, 'cleaned_label': '|'.join}).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "equal-macintosh",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:26.730245Z",
     "iopub.status.busy": "2021-05-17T08:28:26.729626Z",
     "iopub.status.idle": "2021-05-17T08:28:27.607656Z",
     "shell.execute_reply": "2021-05-17T08:28:27.608120Z"
    },
    "papermill": {
     "duration": 0.923665,
     "end_time": "2021-05-17T08:28:27.608293",
     "exception": false,
     "start_time": "2021-05-17T08:28:26.684628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14316 19661\n",
      "['Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " 'Beginning Postsecondary Students Longitudinal Study|Education Longitudinal Study|Beginning Postsecondary Students'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Beginning Postsecondary Student|Beginning Postsecondary Students']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "df = load_train_meta(pth, group_id=True)\n",
    "df_nogroup = load_train_meta(pth, group_id=False)\n",
    "print(len(df), len(df_nogroup))\n",
    "dup_ids = df_nogroup[df_nogroup.Id.duplicated()].Id.unique()\n",
    "print(df[df.Id.isin(dup_ids)].dataset_label.values[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "institutional-billy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:27.725341Z",
     "iopub.status.busy": "2021-05-17T08:28:27.724652Z",
     "iopub.status.idle": "2021-05-17T08:28:27.727287Z",
     "shell.execute_reply": "2021-05-17T08:28:27.726802Z"
    },
    "papermill": {
     "duration": 0.06717,
     "end_time": "2021-05-17T08:28:27.727419",
     "exception": false,
     "start_time": "2021-05-17T08:28:27.660249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_papers(dir_json, paper_ids):\n",
    "    '''\n",
    "    Load papers into a dictionary.\n",
    "    \n",
    "    `papers`: \n",
    "        {''}\n",
    "    '''\n",
    "    \n",
    "    papers = {}\n",
    "    for paper_id in paper_ids:\n",
    "        with open(f'{dir_json}/{paper_id}.json', 'r') as f:\n",
    "            paper = json.load(f)\n",
    "            papers[paper_id] = paper\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "soviet-annual",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:27.814890Z",
     "iopub.status.busy": "2021-05-17T08:28:27.814280Z",
     "iopub.status.idle": "2021-05-17T08:28:27.950377Z",
     "shell.execute_reply": "2021-05-17T08:28:27.950994Z"
    },
    "papermill": {
     "duration": 0.182661,
     "end_time": "2021-05-17T08:28:27.951231",
     "exception": false,
     "start_time": "2021-05-17T08:28:27.768570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'section_title': 'Abstract', 'text': \"This study examines the wage gender gap of young adults in the 1970s, 1980s, and 2000 in the US. Using quantile regression we estimate the gender gap across the entire wage distribution. We also study the importance of high school characteristics in predicting future labor market performance. We conduct analyses for three major racial/ethnic groups in the US: Whites, Blacks, and Hispanics, employing data from two rich longitudinal studies: NLS and NELS. Our results indicate that while some school characteristics are positive and significant predictors of future wages for Whites, they are less so for the two minority groups. We find significant wage gender disparities favoring men across all three surveys in the 1970s, 1980s, and 2000. The wage gender gap is more pronounced in higher paid jobs (90th quantile) for all groups, indicating the presence of a persistent and alarming ''glass ceiling.'' Ó 2007 Elsevier Inc. All rights reserved. JEL Codes: J16; J24; J31\"}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id.unique()[:10])\n",
    "print(type(papers))\n",
    "print(\n",
    "    papers[ random.choice(list(papers.keys())) ][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "verbal-panic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:28.039105Z",
     "iopub.status.busy": "2021-05-17T08:28:28.038459Z",
     "iopub.status.idle": "2021-05-17T08:28:28.042586Z",
     "shell.execute_reply": "2021-05-17T08:28:28.042105Z"
    },
    "papermill": {
     "duration": 0.049529,
     "end_time": "2021-05-17T08:28:28.042712",
     "exception": false,
     "start_time": "2021-05-17T08:28:27.993183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_sample_text(jpth):\n",
    "    sections = json.loads(jpth.read_text())\n",
    "    text = '\\n'.join(section['text'] for section in sections)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "facial-exhibit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:28.130270Z",
     "iopub.status.busy": "2021-05-17T08:28:28.129695Z",
     "iopub.status.idle": "2021-05-17T08:28:28.422410Z",
     "shell.execute_reply": "2021-05-17T08:28:28.421864Z"
    },
    "papermill": {
     "duration": 0.33802,
     "end_time": "2021-05-17T08:28:28.422553",
     "exception": false,
     "start_time": "2021-05-17T08:28:28.084533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s. ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data. Creating a U.S. crosswalk to this system has been a goal of the National Center for Education Statistics and the Office of Research since the late 197,,s, when the National Institute of Education (the predecessor agency to the Office of Educational Research and Improvement) began exploring the idea. The design and implementation of a workable crosswalk, however, awaited the advent of changes to the Classification of Instructional Programs (CIP) system. The 1990 revision of the CIP system laid the foundation for a workable international crosswalk. Adoption of the National Education Goals set global consciousness and international educational comparisons firml\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "print(load_sample_text(jpths_trn[0])[:1_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-merit",
   "metadata": {
    "papermill": {
     "duration": 0.041266,
     "end_time": "2021-05-17T08:28:28.506171",
     "exception": false,
     "start_time": "2021-05-17T08:28:28.464905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "resistant-packaging",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:28.593836Z",
     "iopub.status.busy": "2021-05-17T08:28:28.593249Z",
     "iopub.status.idle": "2021-05-17T08:28:28.598185Z",
     "shell.execute_reply": "2021-05-17T08:28:28.598621Z"
    },
    "papermill": {
     "duration": 0.050745,
     "end_time": "2021-05-17T08:28:28.598776",
     "exception": false,
     "start_time": "2021-05-17T08:28:28.548031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_training_text(txt, lower=False, total_clean=False):\n",
    "    \"\"\"\n",
    "    similar to the default clean_text function but without lowercasing.\n",
    "    \"\"\"\n",
    "    txt = str(txt).lower() if lower else str(txt)\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt).strip()\n",
    "    if total_clean:\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "upset-heavy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:28.689037Z",
     "iopub.status.busy": "2021-05-17T08:28:28.688472Z",
     "iopub.status.idle": "2021-05-17T08:28:28.693597Z",
     "shell.execute_reply": "2021-05-17T08:28:28.694041Z"
    },
    "papermill": {
     "duration": 0.051729,
     "end_time": "2021-05-17T08:28:28.694216",
     "exception": false,
     "start_time": "2021-05-17T08:28:28.642487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle This competition awards 90 000\n",
      "hopkld 7 11 002\n"
     ]
    }
   ],
   "source": [
    "print(clean_training_text('@kaggle This competition awards $90,000!!!!.'))\n",
    "print(clean_training_text('HoPKLd + 7 ! 11,002', total_clean=True, lower=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "prescribed-mandate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:28.783403Z",
     "iopub.status.busy": "2021-05-17T08:28:28.782820Z",
     "iopub.status.idle": "2021-05-17T08:28:28.789154Z",
     "shell.execute_reply": "2021-05-17T08:28:28.788671Z"
    },
    "papermill": {
     "duration": 0.05178,
     "end_time": "2021-05-17T08:28:28.789298",
     "exception": false,
     "start_time": "2021-05-17T08:28:28.737518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def shorten_sentences(sentences, max_length=64, overlap=20):\n",
    "    '''\n",
    "    Args:\n",
    "        sentences (list): List of sentences.\n",
    "        max_length (int): Maximum number of words allowed for each sentence.\n",
    "        overlap (int): If a sentence exceeds `max_length`, we split it to multiple sentences with \n",
    "            this amount of overlapping.\n",
    "    '''\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > max_length:\n",
    "            for p in range(0, len(words), max_length - overlap):\n",
    "                short_sentences.append(' '.join(words[p:p+max_length]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "recognized-attitude",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:28.884799Z",
     "iopub.status.busy": "2021-05-17T08:28:28.884157Z",
     "iopub.status.idle": "2021-05-17T08:28:29.042296Z",
     "shell.execute_reply": "2021-05-17T08:28:29.041682Z"
    },
    "papermill": {
     "duration": 0.209724,
     "end_time": "2021-05-17T08:28:29.042428",
     "exception": false,
     "start_time": "2021-05-17T08:28:28.832704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s', ' ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data']\n",
      "\n",
      "After: ['The International Standard Classification of Education, known by its acronym', 'its acronym ISCED, was developed by the United Nations Educational,', 'Nations Educational, Scientific, and Cultural Organization during the late 1960s', 'late 1960s and 1970s', 'ISCED was implemented in 1976 and is the recognized international', 'recognized international standard for reporting and interpreting education program data', 'program data']\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "sentences = load_sample_text(jpths_trn[0]).split('.')[:2]\n",
    "short_sentences = shorten_sentences(sentences, max_length=10, overlap=2)\n",
    "print('Before:', sentences)\n",
    "print()\n",
    "print('After:', short_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "private-breed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:29.134880Z",
     "iopub.status.busy": "2021-05-17T08:28:29.134221Z",
     "iopub.status.idle": "2021-05-17T08:28:29.136575Z",
     "shell.execute_reply": "2021-05-17T08:28:29.137023Z"
    },
    "papermill": {
     "duration": 0.050889,
     "end_time": "2021-05-17T08:28:29.137197",
     "exception": false,
     "start_time": "2021-05-17T08:28:29.086308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def find_sublist(big_list, small_list):\n",
    "    all_positions = []\n",
    "    for i in range(len(big_list) - len(small_list) + 1):\n",
    "        if small_list == big_list[i:i+len(small_list)]:\n",
    "            all_positions.append(i)\n",
    "    \n",
    "    return all_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "choice-medicare",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:29.226660Z",
     "iopub.status.busy": "2021-05-17T08:28:29.225732Z",
     "iopub.status.idle": "2021-05-17T08:28:29.235265Z",
     "shell.execute_reply": "2021-05-17T08:28:29.234815Z"
    },
    "papermill": {
     "duration": 0.054372,
     "end_time": "2021-05-17T08:28:29.235388",
     "exception": false,
     "start_time": "2021-05-17T08:28:29.181016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list = ['If', 'the', 'thing', 'above', 'is', 'below', 'that', 'thing', 'which', 'is',\n",
    "            'not', 'as', 'high', 'up', 'on', 'the', 'thing', 'above', 'when', 'it', 'is', \n",
    "            'underneath', 'them.']\n",
    "small_list = ['the', 'thing', 'above']\n",
    "\n",
    "find_sublist(big_list, small_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-portfolio",
   "metadata": {
    "papermill": {
     "duration": 0.042927,
     "end_time": "2021-05-17T08:28:29.321907",
     "exception": false,
     "start_time": "2021-05-17T08:28:29.278980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sealed-radius",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:29.417373Z",
     "iopub.status.busy": "2021-05-17T08:28:29.416802Z",
     "iopub.status.idle": "2021-05-17T08:28:29.419337Z",
     "shell.execute_reply": "2021-05-17T08:28:29.419748Z"
    },
    "papermill": {
     "duration": 0.054027,
     "end_time": "2021-05-17T08:28:29.419893",
     "exception": false,
     "start_time": "2021-05-17T08:28:29.365866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tag_sentence(sentence, labels): \n",
    "    '''\n",
    "    requirement: both sentence and labels are already cleaned\n",
    "    '''\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n",
    "                                  for label in labels): # positive sample\n",
    "        nes = ['O'] * len(sentence_words)\n",
    "        for label in labels:\n",
    "            label_words = label.split()\n",
    "\n",
    "            all_pos = find_sublist(sentence_words, label_words)\n",
    "            for pos in all_pos:\n",
    "                nes[pos] = 'B'\n",
    "                for i in range(pos+1, pos+len(label_words)):\n",
    "                    nes[i] = 'I'\n",
    "\n",
    "        return True, list(zip(sentence_words, nes))\n",
    "        \n",
    "    else: # negative sample\n",
    "        nes = ['O'] * len(sentence_words)\n",
    "        return False, list(zip(sentence_words, nes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "medium-lawyer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:29.511872Z",
     "iopub.status.busy": "2021-05-17T08:28:29.510995Z",
     "iopub.status.idle": "2021-05-17T08:28:29.519566Z",
     "shell.execute_reply": "2021-05-17T08:28:29.518763Z"
    },
    "papermill": {
     "duration": 0.055368,
     "end_time": "2021-05-17T08:28:29.519737",
     "exception": false,
     "start_time": "2021-05-17T08:28:29.464369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A label is found in the sentence: True\n",
      "(token, tag) pairs:\n",
      "[('The', 'B'), ('International', 'I'), ('Standard', 'O'), ('Classification', 'O'), ('of', 'O'), ('Education', 'O'), ('known', 'O'), ('by', 'O'), ('its', 'O'), ('acronym', 'O'), ('ISCED', 'O'), ('was', 'O'), ('developed', 'O'), ('by', 'O'), ('the', 'O'), ('United', 'B'), ('Nations', 'I'), ('Educational', 'I'), ('Scientific', 'O'), ('and', 'O'), ('Cultural', 'B'), ('Organization', 'I'), ('during', 'O'), ('the', 'O'), ('late', 'O'), ('1960s', 'O'), ('and', 'O'), ('1970s', 'O')]\n"
     ]
    }
   ],
   "source": [
    "sentence = (\"The International Standard Classification of Education, known by its acronym ISCED, \"\n",
    "            \"was developed by the United Nations Educational, \"\n",
    "            \"Scientific, and Cultural Organization during the late 1960s and 1970s\")\n",
    "labels = ['The International', 'Cultural Organization', 'United Nations Educational']\n",
    "\n",
    "sentence = clean_training_text(sentence)\n",
    "labels = [clean_training_text(label) for label in labels]\n",
    "found_any, token_tags = tag_sentence(sentence, labels)\n",
    "\n",
    "print('A label is found in the sentence:', found_any)\n",
    "print('(token, tag) pairs:')\n",
    "print(token_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "weekly-termination",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:29.621060Z",
     "iopub.status.busy": "2021-05-17T08:28:29.620521Z",
     "iopub.status.idle": "2021-05-17T08:28:29.623115Z",
     "shell.execute_reply": "2021-05-17T08:28:29.622659Z"
    },
    "papermill": {
     "duration": 0.056643,
     "end_time": "2021-05-17T08:28:29.623238",
     "exception": false,
     "start_time": "2021-05-17T08:28:29.566595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_data(papers, df=None, shuffle=True):\n",
    "    '''\n",
    "    Args:\n",
    "        papers (dict): Like that returned by `load_papers`.\n",
    "        df (pd.DataFrame): Competition's train.csv or a subset of it.\n",
    "    '''\n",
    "    cnt_pos, cnt_neg = 0, 0 \n",
    "    ner_data = []\n",
    "\n",
    "    tqdm._instances.clear()\n",
    "    pbar = tqdm(total=len(df))\n",
    "    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():\n",
    "        paper = papers[id]\n",
    "\n",
    "        labels = dataset_label.split('|')\n",
    "        labels = [clean_training_text(label) for label in labels]\n",
    "\n",
    "        sentences = set([clean_training_text(sentence) for section in paper \n",
    "                     for sentence in section['text'].split('.')])\n",
    "        sentences = shorten_sentences(sentences) \n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "\n",
    "        # positive sample\n",
    "        for sentence in sentences:\n",
    "            is_positive, tags = tag_sentence(sentence, labels)\n",
    "            if is_positive:\n",
    "                cnt_pos += 1\n",
    "                ner_data.append(tags)\n",
    "            elif any(word in sentence.lower() for word in ['data', 'study']): \n",
    "                ner_data.append(tags)\n",
    "                cnt_neg += 1\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n",
    "#         print(f\"\\rProcessing paper {i:05d} / {len(df)}. Training data size: {cnt_pos} positives + {cnt_neg} negatives\", \n",
    "#               flush=True, end='')\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(ner_data)\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "changing-operator",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:29.719367Z",
     "iopub.status.busy": "2021-05-17T08:28:29.718259Z",
     "iopub.status.idle": "2021-05-17T08:28:29.957344Z",
     "shell.execute_reply": "2021-05-17T08:28:29.956574Z"
    },
    "papermill": {
     "duration": 0.289614,
     "end_time": "2021-05-17T08:28:29.957535",
     "exception": false,
     "start_time": "2021-05-17T08:28:29.667921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 54 positives + 406 negatives: 100%|██████████| 20/20 [00:00<00:00, 152.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'O'), ('statistical', 'O'), ('significance', 'O'), ('of', 'O'), ('the', 'O'), ('study', 'O'), ('s', 'O'), ('domain', 'O'), ('average', 'O'), ('was', 'O'), ('determined', 'O'), ('by', 'O'), ('the', 'O'), ('WWC', 'O'), ('the', 'O'), ('study', 'O'), ('is', 'O'), ('characterized', 'O'), ('as', 'O'), ('having', 'O'), ('a', 'O'), ('statistically', 'O'), ('significant', 'O'), ('positive', 'O'), ('effect', 'O'), ('because', 'O'), ('univariate', 'O'), ('statistical', 'O'), ('tests', 'O'), ('are', 'O'), ('reported', 'O'), ('for', 'O'), ('each', 'O'), ('outcome', 'O'), ('measure', 'O'), ('and', 'O'), ('both', 'O'), ('effects', 'O'), ('are', 'O'), ('positive', 'O'), ('and', 'O'), ('statistically', 'O'), ('significant', 'O'), ('accounting', 'O'), ('for', 'O'), ('multiple', 'O'), ('comparisons', 'O')]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:20]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "ner_data = get_ner_data(papers, df, shuffle=False)\n",
    "print(ner_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "awful-chicago",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:30.069044Z",
     "iopub.status.busy": "2021-05-17T08:28:30.068122Z",
     "iopub.status.idle": "2021-05-17T08:28:30.071250Z",
     "shell.execute_reply": "2021-05-17T08:28:30.070749Z"
    },
    "papermill": {
     "duration": 0.061484,
     "end_time": "2021-05-17T08:28:30.071375",
     "exception": false,
     "start_time": "2021-05-17T08:28:30.009891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def write_ner_json(ner_data, pth=Path('train_ner.json')):\n",
    "    with open(pth, 'w') as f:\n",
    "        for row in ner_data:\n",
    "            words, nes = list(zip(*row))\n",
    "            row_json = {'tokens' : words, 'tags' : nes}\n",
    "            json.dump(row_json, f)\n",
    "            f.write('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "tropical-special",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:30.182574Z",
     "iopub.status.busy": "2021-05-17T08:28:30.181891Z",
     "iopub.status.idle": "2021-05-17T08:28:30.869233Z",
     "shell.execute_reply": "2021-05-17T08:28:30.868443Z"
    },
    "papermill": {
     "duration": 0.746291,
     "end_time": "2021-05-17T08:28:30.869410",
     "exception": false,
     "start_time": "2021-05-17T08:28:30.123119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tokens\": [\"There\", \"is\", \"no\", \"dataset\", \"here\"], \"tags\": [\"O\", \"O\", \"O\", \"O\", \"O\"]}\r\n",
      "{\"tokens\": [\"Load\", \"the\", \"UN\", \"Trade\", \"Development\", \"into\", \"view\"], \"tags\": [\"O\", \"O\", \"B\", \"I\", \"I\", \"O\", \"O\"]}\r\n"
     ]
    }
   ],
   "source": [
    "ner_data = [\n",
    "    [('There', 'O'), ('is', 'O'), ('no', 'O'), ('dataset', 'O'), ('here', 'O')], \n",
    "    [('Load', 'O'), ('the', 'O'), ('UN', 'B'), ('Trade', 'I'), ('Development', 'I'), ('into', 'O'), ('view', 'O')]\n",
    "]\n",
    "write_ner_json(ner_data, pth=Path('/kaggle/tmp_ner.json'))\n",
    "! cat /kaggle/tmp_ner.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "closing-reduction",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:30.977936Z",
     "iopub.status.busy": "2021-05-17T08:28:30.977363Z",
     "iopub.status.idle": "2021-05-17T08:28:30.980425Z",
     "shell.execute_reply": "2021-05-17T08:28:30.979934Z"
    },
    "papermill": {
     "duration": 0.059049,
     "end_time": "2021-05-17T08:28:30.980555",
     "exception": false,
     "start_time": "2021-05-17T08:28:30.921506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# datasets = load_dataset('json', data_files={'train': '/kaggle/tmp_ner.json', \n",
    "#                                             'valid': '/kaggle/tmp_ner.json'})\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "# assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "\n",
    "# example = datasets['train'][0]\n",
    "\n",
    "# tokenized_input = tokenizer(example['tokens'], is_split_into_words=True)\n",
    "\n",
    "# tokenized_input\n",
    "\n",
    "# print(example['tokens'])\n",
    "# print(tokenizer.convert_ids_to_tokens(tokenized_input['input_ids']))\n",
    "\n",
    "# type(tokenizer)\n",
    "\n",
    "# def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "#     '''\n",
    "#     Adds a new field called 'labels' that are the NER tags to tokenized input.\n",
    "    \n",
    "#     Args:\n",
    "#         tokenizer (transformers.AutoTokenizer): Tokenizer.\n",
    "#         examples (datasets.arrow_dataset.Dataset): Dataset.\n",
    "#         label_all_tokens (bool): If True, all sub-tokens are given the same tag as the \n",
    "#             first sub-token, otherwise all but the first sub-token are given the tag\n",
    "#             -100.\n",
    "#     '''\n",
    "#     tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "#     labels = []\n",
    "#     for i, label in enumerate(examples[\"tags\"]):\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "#         previous_word_idx = None\n",
    "#         label_ids = []\n",
    "#         for word_idx in word_ids:\n",
    "#             # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "#             # ignored in the loss function.\n",
    "#             if word_idx is None:\n",
    "#                 label_ids.append(-100)\n",
    "#             # We set the label for the first token of each word.\n",
    "#             elif word_idx != previous_word_idx:\n",
    "#                 label_ids.append(label[word_idx])\n",
    "#             # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "#             # the label_all_tokens flag.\n",
    "#             else:\n",
    "#                 label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "#             previous_word_idx = word_idx\n",
    "\n",
    "#         labels.append(label_ids)\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs\n",
    "\n",
    "# examples = datasets['train']\n",
    "# tokenize_and_align_labels(examples, label_all_tokens=True)\n",
    "\n",
    "# datasets.Features\n",
    "\n",
    "# datasets['train'].features['tags']\n",
    "\n",
    "# from functools import partial\n",
    "\n",
    "# tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bottom-comment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:31.093953Z",
     "iopub.status.busy": "2021-05-17T08:28:31.092917Z",
     "iopub.status.idle": "2021-05-17T08:28:31.095285Z",
     "shell.execute_reply": "2021-05-17T08:28:31.095693Z"
    },
    "papermill": {
     "duration": 0.064056,
     "end_time": "2021-05-17T08:28:31.095851",
     "exception": false,
     "start_time": "2021-05-17T08:28:31.031795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kaggle_run_ner(model_name_or_path='bert-base-cased', \n",
    "                   train_file='./train_ner.json', validation_file='./train_ner.json',\n",
    "                   num_train_epochs=1, per_device_train_batch_size=8, per_device_eval_batch_size=8,\n",
    "                   save_steps=15000, output_dir='./output', report_to='none', seed=123):\n",
    "    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n",
    "    --model_name_or_path {model_name_or_path} \\\n",
    "    --train_file {train_file} \\\n",
    "    --validation_file {validation_file} \\\n",
    "    --num_train_epochs {num_train_epochs} \\\n",
    "    --per_device_train_batch_size {per_device_train_batch_size} \\\n",
    "    --per_device_eval_batch_size {per_device_eval_batch_size} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --report_to {report_to} \\\n",
    "    --seed {seed} \\\n",
    "    --do_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "golden-galaxy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:28:31.203501Z",
     "iopub.status.busy": "2021-05-17T08:28:31.202509Z",
     "iopub.status.idle": "2021-05-17T08:29:17.596727Z",
     "shell.execute_reply": "2021-05-17T08:29:17.596221Z"
    },
    "papermill": {
     "duration": 46.450101,
     "end_time": "2021-05-17T08:29:17.596874",
     "exception": false,
     "start_time": "2021-05-17T08:28:31.146773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 5 positives + 26 negatives: 100%|██████████| 2/2 [00:00<00:00, 202.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-a8513ebe8fafebcc/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a8513ebe8fafebcc/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 08:28:40,417 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1yu7n1ty\r\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 484kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 08:28:40,689 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 08:28:40,689 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\r\n",
      "[INFO|configuration_utils.py:472] 2021-05-17 08:28:40,690 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\r\n",
      "[INFO|configuration_utils.py:508] 2021-05-17 08:28:40,691 >> Model config BertConfig {\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:472] 2021-05-17 08:28:40,961 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\r\n",
      "[INFO|configuration_utils.py:508] 2021-05-17 08:28:40,962 >> Model config BertConfig {\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 08:28:41,234 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_orfnwon\r\n",
      "Downloading: 100%|████████████████████████████| 213k/213k [00:00<00:00, 846kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 08:28:41,759 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 08:28:41,759 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 08:28:42,030 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprfzkq3r_\r\n",
      "Downloading: 100%|███████████████████████████| 436k/436k [00:00<00:00, 1.39MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 08:28:42,618 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 08:28:42,618 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 08:28:43,431 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpycw9uq29\r\n",
      "Downloading: 100%|███████████████████████████| 29.0/29.0 [00:00<00:00, 21.8kB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 08:28:43,703 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 08:28:43,703 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 08:28:43,703 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 08:28:43,704 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 08:28:43,704 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 08:28:43,704 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-05-17 08:28:43,704 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\r\n",
      "[INFO|file_utils.py:1402] 2021-05-17 08:28:44,004 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0smk7tfj\r\n",
      "Downloading: 100%|███████████████████████████| 436M/436M [00:10<00:00, 42.6MB/s]\r\n",
      "[INFO|file_utils.py:1406] 2021-05-17 08:28:54,502 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\r\n",
      "[INFO|file_utils.py:1409] 2021-05-17 08:28:54,502 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\r\n",
      "[INFO|modeling_utils.py:1051] 2021-05-17 08:28:54,503 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\r\n",
      "[WARNING|modeling_utils.py:1159] 2021-05-17 08:28:58,373 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\r\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "[WARNING|modeling_utils.py:1170] 2021-05-17 08:28:58,374 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 57.90ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-05-17 08:28:58,596 >> The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tags, tokens.\r\n",
      "[INFO|trainer.py:988] 2021-05-17 08:28:58,857 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:989] 2021-05-17 08:28:58,857 >>   Num examples = 31\r\n",
      "[INFO|trainer.py:990] 2021-05-17 08:28:58,857 >>   Num Epochs = 1\r\n",
      "[INFO|trainer.py:991] 2021-05-17 08:28:58,857 >>   Instantaneous batch size per device = 8\r\n",
      "[INFO|trainer.py:992] 2021-05-17 08:28:58,857 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\r\n",
      "[INFO|trainer.py:993] 2021-05-17 08:28:58,857 >>   Gradient Accumulation steps = 1\r\n",
      "[INFO|trainer.py:994] 2021-05-17 08:28:58,857 >>   Total optimization steps = 4\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:17<00:00,  4.34s/it][INFO|trainer.py:1171] 2021-05-17 08:29:16,012 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 17.155, 'train_samples_per_second': 0.233, 'epoch': 1.0}\r\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:17<00:00,  4.29s/it]\r\n",
      "[INFO|trainer.py:1600] 2021-05-17 08:29:16,143 >> Saving model checkpoint to ./output\r\n",
      "[INFO|configuration_utils.py:318] 2021-05-17 08:29:16,144 >> Configuration saved in ./output/config.json\r\n",
      "[INFO|modeling_utils.py:837] 2021-05-17 08:29:16,709 >> Model weights saved in ./output/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:1896] 2021-05-17 08:29:16,710 >> tokenizer config file saved in ./output/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:1902] 2021-05-17 08:29:16,710 >> Special tokens file saved in ./output/special_tokens_map.json\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-05-17 08:29:16,744 >> ***** train metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:29:16,744 >>   epoch                      =        1.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:29:16,744 >>   init_mem_cpu_alloc_delta   =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:29:16,744 >>   init_mem_cpu_peaked_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:29:16,744 >>   train_mem_cpu_alloc_delta  =     1501MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:29:16,744 >>   train_mem_cpu_peaked_delta =      334MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:29:16,744 >>   train_runtime              = 0:00:17.15\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:29:16,744 >>   train_samples              =         31\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:29:16,744 >>   train_samples_per_second   =      0.233\r\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:2]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', df.Id)\n",
    "ner_data = get_ner_data(papers, df)\n",
    "write_ner_json(ner_data, pth=Path('./train_ner.json'))\n",
    "kaggle_run_ner(save_steps=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-scheme",
   "metadata": {
    "papermill": {
     "duration": 0.090492,
     "end_time": "2021-05-17T08:29:17.778919",
     "exception": false,
     "start_time": "2021-05-17T08:29:17.688427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Literal matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "unexpected-boards",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:29:17.967542Z",
     "iopub.status.busy": "2021-05-17T08:29:17.966768Z",
     "iopub.status.idle": "2021-05-17T08:29:17.969740Z",
     "shell.execute_reply": "2021-05-17T08:29:17.969137Z"
    },
    "papermill": {
     "duration": 0.099342,
     "end_time": "2021-05-17T08:29:17.969867",
     "exception": false,
     "start_time": "2021-05-17T08:29:17.870525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_knowledge_bank(pth):\n",
    "    '''\n",
    "    Args:\n",
    "        pth (str): Path to meta data like 'train.csv', which\n",
    "        needs to have columns: 'dataset_title', 'dataset_label', and 'cleaned_label'.\n",
    "        \n",
    "    Returns:\n",
    "        all_labels (set): All possible strings associated with a dataset from the meta data.\n",
    "    '''\n",
    "    df = load_train_meta(pth, group_id=False)\n",
    "    all_labels = set()\n",
    "    for label_1, label_2, label_3 in df[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n",
    "        all_labels.add(str(label_1).lower())\n",
    "        all_labels.add(str(label_2).lower())\n",
    "        all_labels.add(str(label_3).lower())\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecological-creek",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:29:18.156682Z",
     "iopub.status.busy": "2021-05-17T08:29:18.156033Z",
     "iopub.status.idle": "2021-05-17T08:29:18.283474Z",
     "shell.execute_reply": "2021-05-17T08:29:18.282328Z"
    },
    "papermill": {
     "duration": 0.222353,
     "end_time": "2021-05-17T08:29:18.283647",
     "exception": false,
     "start_time": "2021-05-17T08:29:18.061294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "['2019 ncov complete genome sequences', '2019 ncov genome sequence', '2019 ncov genome sequences', '2019-ncov complete genome sequences', '2019-ncov genome sequence', '2019-ncov genome sequences', 'adni', 'advanced national seismic system (anss) comprehensive catalog (comcat)', 'advanced national seismic system anss comprehensive catalog comcat ', 'advanced national seismic system comprehensive catalog']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "print(len(all_labels))\n",
    "print(sorted(all_labels)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "specialized-small",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:29:18.483698Z",
     "iopub.status.busy": "2021-05-17T08:29:18.482838Z",
     "iopub.status.idle": "2021-05-17T08:29:18.485224Z",
     "shell.execute_reply": "2021-05-17T08:29:18.485639Z"
    },
    "papermill": {
     "duration": 0.096283,
     "end_time": "2021-05-17T08:29:18.485792",
     "exception": false,
     "start_time": "2021-05-17T08:29:18.389509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def literal_match(paper, all_labels):\n",
    "    '''\n",
    "    Args:\n",
    "        paper ()\n",
    "    '''\n",
    "    text_1 = '. '.join(section['text'] for section in paper).lower()\n",
    "    text_2 = clean_training_text(text_1, lower=True, total_clean=True)\n",
    "    \n",
    "    labels = set()\n",
    "    for label in all_labels:\n",
    "        if label in text_1 or label in text_2:\n",
    "            labels.add(clean_training_text(label, lower=True, total_clean=True))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "beneficial-foster",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:29:18.673653Z",
     "iopub.status.busy": "2021-05-17T08:29:18.672990Z",
     "iopub.status.idle": "2021-05-17T08:29:18.911104Z",
     "shell.execute_reply": "2021-05-17T08:29:18.910442Z"
    },
    "papermill": {
     "duration": 0.336368,
     "end_time": "2021-05-17T08:29:18.911244",
     "exception": false,
     "start_time": "2021-05-17T08:29:18.574876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adni|alzheimer s disease neuroimaging initiative adni',\n",
       " 'nces common core of data|common core of data|trends in international mathematics and science study',\n",
       " 'noaa storm surge inundation|slosh model|sea lake and overland surges from hurricanes',\n",
       " 'rural urban continuum codes']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n",
    "\n",
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "\n",
    "literal_preds = []\n",
    "for paper_id in sample_submission.Id:\n",
    "    paper = papers[paper_id]\n",
    "    literal_preds.append('|'.join(literal_match(paper, all_labels)))\n",
    "    \n",
    "literal_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-brazil",
   "metadata": {
    "papermill": {
     "duration": 0.094393,
     "end_time": "2021-05-17T08:29:19.098349",
     "exception": false,
     "start_time": "2021-05-17T08:29:19.003956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bert model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "narrative-passport",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:29:19.292834Z",
     "iopub.status.busy": "2021-05-17T08:29:19.292185Z",
     "iopub.status.idle": "2021-05-17T08:29:19.294840Z",
     "shell.execute_reply": "2021-05-17T08:29:19.294246Z"
    },
    "papermill": {
     "duration": 0.103149,
     "end_time": "2021-05-17T08:29:19.294972",
     "exception": false,
     "start_time": "2021-05-17T08:29:19.191823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_inference_data(papers, sample_submission):\n",
    "    '''\n",
    "    Args:\n",
    "        papers (dict): Each list in this dictionary consists of the section of a paper.\n",
    "        sample_submission (pd.DataFrame): Competition 'sample_submission.csv'.\n",
    "    Returns:\n",
    "        test_rows (list): Each dict in this list is of the form: \n",
    "            {'tokens': ['goat', 'win', ...], 'tags': ['O', 'O', ...]}\n",
    "            and represents a sentence.  \n",
    "        paper_length (list): Number of sentences in each paper.\n",
    "    '''\n",
    "    test_rows = [] # test data in NER format\n",
    "    paper_length = [] # store the number of sentences each paper has\n",
    "\n",
    "    for paper_id in sample_submission['Id']:\n",
    "        # load paper\n",
    "        paper = papers[paper_id]\n",
    "\n",
    "        # extract sentences\n",
    "        sentences = [clean_training_text(sentence) for section in paper \n",
    "                     for sentence in section['text'].split('.')\n",
    "                    ]\n",
    "        sentences = shorten_sentences(sentences) # make sentences short\n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "\n",
    "        # collect all sentences in json\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split()\n",
    "            dummy_tags = ['O']*len(sentence_words)\n",
    "            test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n",
    "\n",
    "        # track which sentence belongs to which data point\n",
    "        paper_length.append(len(sentences))\n",
    "\n",
    "    print(f'total number of sentences: {len(test_rows)}')\n",
    "    return test_rows, paper_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "comprehensive-annual",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:29:19.483451Z",
     "iopub.status.busy": "2021-05-17T08:29:19.482844Z",
     "iopub.status.idle": "2021-05-17T08:29:19.532245Z",
     "shell.execute_reply": "2021-05-17T08:29:19.531721Z"
    },
    "papermill": {
     "duration": 0.14588,
     "end_time": "2021-05-17T08:29:19.532379",
     "exception": false,
     "start_time": "2021-05-17T08:29:19.386499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of sentences: 367\n",
      "[{'tokens': ['A', 'recent', 'large', 'genomewide', 'association', 'study', 'GWAS', 'reported', 'a', 'genome', 'wide', 'significant', 'locus', 'for', 'years', 'of', 'education', 'which', 'subsequently', 'demonstrated', 'association', 'to', 'general', 'cognitive', 'ability', 'g', 'in', 'overlapping', 'cohorts'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'tokens': ['The', 'current', 'study', 'was', 'designed', 'to', 'test', 'whether', 'GWAS', 'hits', 'for', 'educational', 'attainment', 'are', 'involved', 'in', 'general', 'cognitive', 'ability', 'in', 'an', 'independent', 'large', 'scale', 'collection', 'of', 'cohorts'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'tokens': ['We', 'next', 'conducted', 'meta', 'analyses', 'with', '24', '189', 'individuals', 'with', 'neurocognitive', 'data', 'from', 'the', 'educational', 'attainment', 'studies', 'and', 'then', 'with', '53', '188', 'largely', 'independent', 'individuals', 'from', 'a', 'recent', 'GWAS', 'of', 'cognition'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
      "[34, 151, 98, 84]\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test', sample_submission.Id)\n",
    "test_rows, paper_length = get_ner_inference_data(papers, sample_submission)\n",
    "print(test_rows[:3])\n",
    "print(paper_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "official-broadway",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:29:19.734100Z",
     "iopub.status.busy": "2021-05-17T08:29:19.733460Z",
     "iopub.status.idle": "2021-05-17T08:29:19.736128Z",
     "shell.execute_reply": "2021-05-17T08:29:19.735640Z"
    },
    "papermill": {
     "duration": 0.111349,
     "end_time": "2021-05-17T08:29:19.736265",
     "exception": false,
     "start_time": "2021-05-17T08:29:19.624916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def kaggle_run_ner_predict(model_name_or_path='/kaggle/input/coleridge-bert-models/output', \n",
    "                           train_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n",
    "                           validation_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n",
    "                           test_file='./input_data/test_ner_input.json', \n",
    "                           output_dir='./pred'):\n",
    "    '''\n",
    "    Args:\n",
    "        test_file (Path, str): Path to json file in which each row represents an input\n",
    "            sample to the model (representing a sentence in this context).  Each row\n",
    "            is a dictionary of the form:\n",
    "            {'tokens': ['hi', 'there', ...], 'tags': ['O', 'O', ...]}\n",
    "        output_dir (Path, str): Path to the directory in which prediction results are saved.\n",
    "    '''\n",
    "    os.environ[\"MODEL_PATH\"] = f\"{model_name_or_path}\"\n",
    "    os.environ[\"TRAIN_FILE\"] = f\"{train_file}\"\n",
    "    os.environ[\"VALIDATION_FILE\"] = f\"{validation_file}\"\n",
    "    os.environ[\"TEST_FILE\"] = f\"{test_file}\"\n",
    "    os.environ[\"OUTPUT_DIR\"] = f\"{output_dir}\"\n",
    "    \n",
    "    ! python /kaggle/input/kaggle-ner-utils/kaggle_run_ner.py \\\n",
    "    --model_name_or_path \"$MODEL_PATH\" \\\n",
    "    --validation_file \"$VALIDATION_FILE\" \\\n",
    "    --train_file \"$TRAIN_FILE\" \\\n",
    "    --test_file \"$TEST_FILE\" \\\n",
    "    --output_dir \"$OUTPUT_DIR\" \\\n",
    "    --report_to 'none' \\\n",
    "    --seed 123 \\\n",
    "    --do_predict\n",
    "\n",
    "def run_inference(test_rows, predict_batch=64_000, \n",
    "                  model_name_or_path='/kaggle/input/coleridge-bert-models/output', \n",
    "                  train_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n",
    "                  validation_file='/kaggle/input/coleridge-bert-models/train_ner.json', \n",
    "                  test_file='./input_data/test_ner_input.json', \n",
    "                  output_dir='./pred'):\n",
    "    '''\n",
    "    '''\n",
    "    test_file = Path(test_file)\n",
    "    test_file.parent.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    bert_outputs = []\n",
    "    for batch_begin in range(0, len(test_rows), predict_batch):\n",
    "        # write data rows to input file\n",
    "        with open(test_file, 'w') as f:\n",
    "            for row in test_rows[batch_begin:batch_begin + predict_batch]:\n",
    "                json.dump(row, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        # remove output dir\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)\n",
    "\n",
    "        # do predict\n",
    "        kaggle_run_ner_predict(\n",
    "            model_name_or_path=model_name_or_path, \n",
    "            train_file=train_file, validation_file=validation_file, test_file=test_file, \n",
    "            output_dir=output_dir)\n",
    "\n",
    "        # read predictions\n",
    "        with open(f'{output_dir}/test_predictions.txt') as f:\n",
    "            this_preds = f.read().split('\\n')[:-1]\n",
    "            bert_outputs += [pred.split() for pred in this_preds]\n",
    "    return bert_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "royal-cambodia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:29:19.925404Z",
     "iopub.status.busy": "2021-05-17T08:29:19.923740Z",
     "iopub.status.idle": "2021-05-17T08:30:21.742736Z",
     "shell.execute_reply": "2021-05-17T08:30:21.742211Z"
    },
    "papermill": {
     "duration": 61.916068,
     "end_time": "2021-05-17T08:30:21.742891",
     "exception": false,
     "start_time": "2021-05-17T08:29:19.826823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-6a3f3cd2a23edb39/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\r\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-6a3f3cd2a23edb39/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\r\n",
      "[INFO|configuration_utils.py:470] 2021-05-17 08:29:24,935 >> loading configuration file /kaggle/working/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-05-17 08:29:24,935 >> Model config BertConfig {\r\n",
      "  \"_name_or_path\": \"bert-base-cased\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|configuration_utils.py:470] 2021-05-17 08:29:24,935 >> loading configuration file /kaggle/working/output/config.json\r\n",
      "[INFO|configuration_utils.py:508] 2021-05-17 08:29:24,936 >> Model config BertConfig {\r\n",
      "  \"_name_or_path\": \"bert-base-cased\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"BertForTokenClassification\"\r\n",
      "  ],\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"finetuning_task\": \"ner\",\r\n",
      "  \"gradient_checkpointing\": false,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2\r\n",
      "  },\r\n",
      "  \"layer_norm_eps\": 1e-12,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"model_type\": \"bert\",\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"position_embedding_type\": \"absolute\",\r\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 28996\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-05-17 08:29:24,936 >> Didn't find file /kaggle/working/output/tokenizer.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1637] 2021-05-17 08:29:24,936 >> Didn't find file /kaggle/working/output/added_tokens.json. We won't load it.\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 08:29:24,936 >> loading file /kaggle/working/output/vocab.txt\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 08:29:24,936 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 08:29:24,936 >> loading file None\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 08:29:24,937 >> loading file /kaggle/working/output/special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:1700] 2021-05-17 08:29:24,937 >> loading file /kaggle/working/output/tokenizer_config.json\r\n",
      "[INFO|modeling_utils.py:1049] 2021-05-17 08:29:24,989 >> loading weights file /kaggle/working/output/pytorch_model.bin\r\n",
      "[INFO|modeling_utils.py:1167] 2021-05-17 08:29:28,542 >> All model checkpoint weights were used when initializing BertForTokenClassification.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:1176] 2021-05-17 08:29:28,542 >> All the weights of BertForTokenClassification were initialized from the model checkpoint at /kaggle/working/output/.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 13.33ba/s]\r\n",
      "[INFO|trainer.py:485] 2021-05-17 08:29:29,048 >> The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, tags.\r\n",
      "[INFO|trainer.py:1817] 2021-05-17 08:29:29,049 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:1818] 2021-05-17 08:29:29,049 >>   Num examples = 367\r\n",
      "[INFO|trainer.py:1819] 2021-05-17 08:29:29,050 >>   Batch size = 8\r\n",
      "100%|███████████████████████████████████████████| 46/46 [00:50<00:00,  1.04s/it]/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\r\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\r\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\r\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\r\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\r\n",
      "  avg = a.mean(axis)\r\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\r\n",
      "  ret = ret.dtype.type(ret / rcount)\r\n",
      "[INFO|trainer_pt_utils.py:735] 2021-05-17 08:30:20,968 >> ***** test metrics *****\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   init_mem_cpu_alloc_delta  =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   init_mem_cpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   test_accuracy             =        1.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   test_f1                   =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   test_loss                 =     0.2179\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   test_mem_cpu_alloc_delta  =        9MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   test_mem_cpu_peaked_delta =        0MB\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   test_precision            =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   test_recall               =        0.0\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,968 >>   test_runtime              = 0:00:51.75\r\n",
      "[INFO|trainer_pt_utils.py:740] 2021-05-17 08:30:20,969 >>   test_samples_per_second   =       7.09\r\n",
      "100%|███████████████████████████████████████████| 46/46 [00:50<00:00,  1.10s/it]\r\n"
     ]
    }
   ],
   "source": [
    "predict_batch = 64_000 \n",
    "\n",
    "model_name_or_path = '/kaggle/working/output/' #'/kaggle/input/coleridge-bert-models/output'\n",
    "test_file = './input_data/test_ner_input.json'\n",
    "train_file = 'train_ner.json' #'/kaggle/input/coleridge-bert-models/train_ner.json'\n",
    "validation_file = 'train_ner.json' #'/kaggle/input/coleridge-bert-models/train_ner.json'\n",
    "output_dir = './pred'\n",
    "\n",
    "bert_outputs = run_inference(test_rows, predict_batch=predict_batch, \n",
    "                             model_name_or_path=model_name_or_path, \n",
    "                             test_file=test_file, train_file=train_file, validation_file=validation_file,\n",
    "                             output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "requested-today",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:21.968877Z",
     "iopub.status.busy": "2021-05-17T08:30:21.964231Z",
     "iopub.status.idle": "2021-05-17T08:30:22.661018Z",
     "shell.execute_reply": "2021-05-17T08:30:22.660507Z"
    },
    "papermill": {
     "duration": 0.809742,
     "end_time": "2021-05-17T08:30:22.661180",
     "exception": false,
     "start_time": "2021-05-17T08:30:21.851438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./pred/test_predictions.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls {output_dir}/test_predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "unusual-astronomy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:22.898373Z",
     "iopub.status.busy": "2021-05-17T08:30:22.897580Z",
     "iopub.status.idle": "2021-05-17T08:30:22.901168Z",
     "shell.execute_reply": "2021-05-17T08:30:22.900670Z"
    },
    "papermill": {
     "duration": 0.125259,
     "end_time": "2021-05-17T08:30:22.901310",
     "exception": false,
     "start_time": "2021-05-17T08:30:22.776051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_bert_dataset_labels(test_rows, paper_length, bert_outputs):\n",
    "    '''\n",
    "    Returns:\n",
    "        bert_dataset_labels (list): Each element is a set consisting of labels predicted\n",
    "            by the model.\n",
    "    '''\n",
    "    test_sentences = [row['tokens'] for row in test_rows]\n",
    "    \n",
    "    bert_dataset_labels = [] # store all dataset labels for each publication\n",
    "\n",
    "    for length in paper_length:\n",
    "        labels = set()\n",
    "        for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n",
    "            curr_phrase = ''\n",
    "            for word, tag in zip(sentence, pred):\n",
    "                if tag == 'B': # start a new phrase\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "                    curr_phrase = word\n",
    "                elif tag == 'I' and curr_phrase: # continue the phrase\n",
    "                    curr_phrase += ' ' + word\n",
    "                else: # end last phrase (if any)\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "            # check if the label is the suffix of the sentence\n",
    "            if curr_phrase:\n",
    "                labels.add(curr_phrase)\n",
    "                curr_phrase = ''\n",
    "\n",
    "        # record dataset labels for this publication\n",
    "        bert_dataset_labels.append(labels)\n",
    "\n",
    "        del test_sentences[:length], bert_outputs[:length]\n",
    "        \n",
    "    return bert_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "egyptian-industry",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:23.135956Z",
     "iopub.status.busy": "2021-05-17T08:30:23.135134Z",
     "iopub.status.idle": "2021-05-17T08:30:23.138408Z",
     "shell.execute_reply": "2021-05-17T08:30:23.137906Z"
    },
    "papermill": {
     "duration": 0.125167,
     "end_time": "2021-05-17T08:30:23.138538",
     "exception": false,
     "start_time": "2021-05-17T08:30:23.013371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = ['They do not present all the features', \n",
    "             'Despite the pretraining on the Tigers EcoNAX dataset',\n",
    "             'Weirdly there has been lots of studies based on WGS Equality Definitiveness Dataset']\n",
    "paper_length = [2, 1]\n",
    "test_rows = [{'tokens': sentence.split(), 'tags': len(sentence.split()) * ['O']} \n",
    "             for sentence in sentences]\n",
    "bert_outputs = [['O', 'O', 'O', 'B', 'I', 'I', 'O'],\n",
    "                ['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I'],\n",
    "                ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I']]\n",
    "\n",
    "for i, row in enumerate(test_rows):\n",
    "    assert len(row['tokens']) == len(row['tags']) == len(bert_outputs[i])\n",
    "\n",
    "bert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "israeli-hobby",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:23.366269Z",
     "iopub.status.busy": "2021-05-17T08:30:23.365618Z",
     "iopub.status.idle": "2021-05-17T08:30:23.369924Z",
     "shell.execute_reply": "2021-05-17T08:30:23.369437Z"
    },
    "papermill": {
     "duration": 0.119776,
     "end_time": "2021-05-17T08:30:23.370059",
     "exception": false,
     "start_time": "2021-05-17T08:30:23.250283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Tigers EcoNAX dataset', 'present all the'},\n",
       " {'WGS Equality Definitiveness Dataset'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "technical-purpose",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:23.597979Z",
     "iopub.status.busy": "2021-05-17T08:30:23.597287Z",
     "iopub.status.idle": "2021-05-17T08:30:23.600573Z",
     "shell.execute_reply": "2021-05-17T08:30:23.599976Z"
    },
    "papermill": {
     "duration": 0.119358,
     "end_time": "2021-05-17T08:30:23.600697",
     "exception": false,
     "start_time": "2021-05-17T08:30:23.481339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def jaccard_similarity(s1, s2):\n",
    "    l1 = set(s1.split(\" \"))\n",
    "    l2 = set(s2.split(\" \"))\n",
    "    intersection = len(list(l1.intersection(l2)))\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "undefined-richmond",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:23.827582Z",
     "iopub.status.busy": "2021-05-17T08:30:23.826678Z",
     "iopub.status.idle": "2021-05-17T08:30:23.830934Z",
     "shell.execute_reply": "2021-05-17T08:30:23.830426Z"
    },
    "papermill": {
     "duration": 0.119904,
     "end_time": "2021-05-17T08:30:23.831056",
     "exception": false,
     "start_time": "2021-05-17T08:30:23.711152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity('USGS Frog Counts Data', 'USGA Croc Counts Data') == 1 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "vocational-cinema",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:24.061856Z",
     "iopub.status.busy": "2021-05-17T08:30:24.061256Z",
     "iopub.status.idle": "2021-05-17T08:30:24.063676Z",
     "shell.execute_reply": "2021-05-17T08:30:24.064219Z"
    },
    "papermill": {
     "duration": 0.121779,
     "end_time": "2021-05-17T08:30:24.064384",
     "exception": false,
     "start_time": "2021-05-17T08:30:23.942605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_bert_labels(bert_dataset_labels):\n",
    "    '''\n",
    "    When several labels for a paper are too similar, keep just one of them.\n",
    "    '''\n",
    "    filtered_bert_labels = []\n",
    "\n",
    "    for labels in bert_dataset_labels:\n",
    "        filtered = []\n",
    "\n",
    "        for label in sorted(labels, key=len):\n",
    "            label = clean_training_text(label, lower=True)\n",
    "            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n",
    "                filtered.append(label)\n",
    "\n",
    "        filtered_bert_labels.append('|'.join(filtered))\n",
    "    return filtered_bert_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cross-trading",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:24.319932Z",
     "iopub.status.busy": "2021-05-17T08:30:24.318779Z",
     "iopub.status.idle": "2021-05-17T08:30:24.321592Z",
     "shell.execute_reply": "2021-05-17T08:30:24.319383Z"
    },
    "papermill": {
     "duration": 0.143802,
     "end_time": "2021-05-17T08:30:24.321766",
     "exception": false,
     "start_time": "2021-05-17T08:30:24.177964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moma artists|housing market|moma artists catalogue',\n",
       " 'deep sea rock salts|rhs fertiliser index']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset_labels = [{'moma artists catalogue', 'moma artists', 'housing market'},\n",
    "                       {'rhs flowers fertiliser index', 'deep sea rock salts', 'rhs fertiliser index'}]\n",
    "\n",
    "filter_bert_labels(bert_dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-internet",
   "metadata": {
    "papermill": {
     "duration": 0.110676,
     "end_time": "2021-05-17T08:30:24.554964",
     "exception": false,
     "start_time": "2021-05-17T08:30:24.444288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overall prediction for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "single-attendance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:24.780099Z",
     "iopub.status.busy": "2021-05-17T08:30:24.779193Z",
     "iopub.status.idle": "2021-05-17T08:30:24.784067Z",
     "shell.execute_reply": "2021-05-17T08:30:24.784515Z"
    },
    "papermill": {
     "duration": 0.117277,
     "end_time": "2021-05-17T08:30:24.784675",
     "exception": false,
     "start_time": "2021-05-17T08:30:24.667398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def combine_matching_and_bert(literal_preds, filtererd_bert_labels):\n",
    "    final_predictions = []\n",
    "    for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n",
    "        if literal_match:\n",
    "            final_predictions.append(literal_match)\n",
    "        else:\n",
    "            final_predictions.append(bert_pred)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "tamil-miracle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T08:30:25.009804Z",
     "iopub.status.busy": "2021-05-17T08:30:25.008873Z",
     "iopub.status.idle": "2021-05-17T08:30:25.015034Z",
     "shell.execute_reply": "2021-05-17T08:30:25.014592Z"
    },
    "papermill": {
     "duration": 0.118763,
     "end_time": "2021-05-17T08:30:25.015203",
     "exception": false,
     "start_time": "2021-05-17T08:30:24.896440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mongolian racing cars|reallife headphones',\n",
       " 'hifi dataset|headphones collection data']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_preds = ['mongolian racing cars|reallife headphones', '']\n",
    "filtered_bert_labels = ['data|dataset', 'hifi dataset|headphones collection data']\n",
    "combine_matching_and_bert(literal_preds, filtered_bert_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-boutique",
   "metadata": {
    "papermill": {
     "duration": 0.111416,
     "end_time": "2021-05-17T08:30:25.237382",
     "exception": false,
     "start_time": "2021-05-17T08:30:25.125966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reference\n",
    "- https://www.kaggle.com/tungmphung/pytorch-bert-for-named-entity-recognition/notebook\n",
    "- https://www.kaggle.com/tungmphung/coleridge-matching-bert-ner/notebook\n",
    "- https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-chambers",
   "metadata": {
    "papermill": {
     "duration": 0.109026,
     "end_time": "2021-05-17T08:30:25.456522",
     "exception": false,
     "start_time": "2021-05-17T08:30:25.347496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 165.378417,
   "end_time": "2021-05-17T08:30:26.275809",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-17T08:27:40.897392",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
