{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# showus"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#default_exp showus"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Processing /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\nInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 0.8.7\n    Uninstalling fsspec-0.8.7:\n      Successfully uninstalled fsspec-0.8.7\nSuccessfully installed fsspec-2021.4.0\nLooking in links: file:///kaggle/input/coleridge-packages/packages/datasets\nProcessing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.4.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\nProcessing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\nRequirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\nProcessing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\nProcessing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\nInstalling collected packages: tqdm, xxhash, huggingface-hub, datasets\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.59.0\n    Uninstalling tqdm-4.59.0:\n      Successfully uninstalled tqdm-4.59.0\nSuccessfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\nProcessing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\nProcessing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\nInstalling collected packages: tokenizers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.10.2\n    Uninstalling tokenizers-0.10.2:\n      Successfully uninstalled tokenizers-0.10.2\nSuccessfully installed tokenizers-0.10.1\nProcessing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.45)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2021.3.17)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (4.0.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.5.1\n    Uninstalling transformers-4.5.1:\n      Successfully uninstalled transformers-4.5.1\nSuccessfully installed transformers-4.5.0.dev0\n"
    }
   ],
   "source": "! pip install /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\n! pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n! pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n! pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n! pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\nimport os, shutil\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport itertools\nfrom functools import partial\nimport re\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport transformers, seqeval\nfrom transformers import AutoTokenizer, DataCollatorForTokenClassification\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_dataset, ClassLabel, load_metric"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Utilities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\nPath.ls = lambda pth: list(pth.iterdir())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Data I/O"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_train_meta(pth, group_id=True):\n    df = pd.read_csv(pth)\n    if group_id:\n        df = df.groupby('Id').agg({'pub_title': 'first', 'dataset_title': '|'.join, \n                                   'dataset_label': '|'.join, 'cleaned_label': '|'.join}).reset_index()\n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "14316 19661\n['Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n 'Beginning Postsecondary Students Longitudinal Study|Education Longitudinal Study|Beginning Postsecondary Students'\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n 'Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n 'Beginning Postsecondary Student|Beginning Postsecondary Students']\n"
    }
   ],
   "source": "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\ndf = load_train_meta(pth, group_id=True)\ndf_nogroup = load_train_meta(pth, group_id=False)\nprint(len(df), len(df_nogroup))\ndup_ids = df_nogroup[df_nogroup.Id.duplicated()].Id.unique()\nprint(df[df.Id.isin(dup_ids)].dataset_label.values[-10:])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_papers(dir_json, paper_ids):\n    '''\n    Load papers into a dictionary.\n    \n    `papers`: \n        {''}\n    '''\n    \n    papers = {}\n    for paper_id in paper_ids:\n        with open(f'{dir_json}/{paper_id}.json', 'r') as f:\n            paper = json.load(f)\n            papers[paper_id] = paper\n    return papers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'dict'>\n{'section_title': 'Abstract', 'text': 'Mathematics, as no other school subject, evokes conflicting emotions and contradictory attitudes -from \"the gate to a career\" and \"the queen of science\" to the widespread acceptance of mathematical ignorance in society. The process of studying mathematics requires systematic work and patience, as mathematical knowledge has a cumulative nature. In the case of mathematics education, some students abandon mathematics at quite early levels of education and begin to consider themselves \"humanists\", which results in serious consequences for future educational and career choices. In this paper, I propose a description of the process of escaping from mathematics in the context of students\\' perceptions of this subject, using the results of two studies -one qualitative and the other quantitative.'}\n"
    }
   ],
   "source": "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[-10:]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\nprint(type(papers))\nprint(\n    papers[ np.random.choice(df.Id.values) ][0]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# df.dataset_label.iloc[3]\n\n# paper = papers[df.Id.iloc[4]]\n# ['\\n'.join([section['section_title'], section['text']]).split() for section in paper if section['text']][2]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_sample_text(jpth):\n    sections = json.loads(jpth.read_text())\n    text = '\\n'.join(section['text'] for section in sections)\n    return text"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s. ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data. Creating a U.S. crosswalk to this system has been a goal of the National Center for Education Statistics and the Office of Research since the late 197,,s, when the National Institute of Education (the predecessor agency to the Office of Educational Research and Improvement) began exploring the idea. The design and implementation of a workable crosswalk, however, awaited the advent of changes to the Classification of Instructional Programs (CIP) system. The 1990 revision of the CIP system laid the foundation for a workable international crosswalk. Adoption of the National Education Goals set global consciousness and international educational comparisons firml\n"
    }
   ],
   "source": "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\nprint(load_sample_text(jpths_trn[0])[:1_000])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Data processing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef clean_training_text(txt, lower=False, total_clean=False):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    txt = str(txt).lower() if lower else str(txt)\n    txt = re.sub('[^A-Za-z0-9]+', ' ', txt).strip()\n    if total_clean:\n        txt = re.sub(' +', ' ', txt)\n    return txt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "kaggle This competition awards 90 000\nhopkld 7 11 002\n"
    }
   ],
   "source": "print(clean_training_text('@kaggle This competition awards $90,000!!!!.'))\nprint(clean_training_text('HoPKLd + 7 ! 11,002', total_clean=True, lower=True))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef shorten_sentences(sentences, max_length=64, overlap=20):\n    '''\n    Args:\n        sentences (list): List of sentences.\n        max_length (int): Maximum number of words allowed for each sentence.\n        overlap (int): If a sentence exceeds `max_length`, we split it to multiple sentences with \n            this amount of overlapping.\n    '''\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > max_length:\n            for p in range(0, len(words), max_length - overlap):\n                short_sentences.append(' '.join(words[p:p+max_length]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Before: ['The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s', ' ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data']\n\nAfter: ['The International Standard Classification of Education, known by its acronym', 'its acronym ISCED, was developed by the United Nations Educational,', 'Nations Educational, Scientific, and Cultural Organization during the late 1960s', 'late 1960s and 1970s', 'ISCED was implemented in 1976 and is the recognized international', 'recognized international standard for reporting and interpreting education program data', 'program data']\n"
    }
   ],
   "source": "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\nsentences = load_sample_text(jpths_trn[0]).split('.')[:2]\nshort_sentences = shorten_sentences(sentences, max_length=10, overlap=2)\nprint('Before:', sentences)\nprint()\nprint('After:', short_sentences)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef find_sublist(big_list, small_list):\n    all_positions = []\n    for i in range(len(big_list) - len(small_list) + 1):\n        if small_list == big_list[i:i+len(small_list)]:\n            all_positions.append(i)\n    \n    return all_positions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 15]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "big_list = ['If', 'the', 'thing', 'above', 'is', 'below', 'that', 'thing', 'which', 'is',\n            'not', 'as', 'high', 'up', 'on', 'the', 'thing', 'above', 'when', 'it', 'is', \n            'underneath', 'them.']\nsmall_list = ['the', 'thing', 'above']\n\nfind_sublist(big_list, small_list)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Named Entity Recognition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_ner_classlabel():\n    '''\n    Labels for named entity recognition.\n        'O': Token not part of a phrase that mentions a dataset.\n        'I': Intermediate token of a phrase mentioning a dataset.\n        'B': First token of a phrase mentioning a dataset.\n    '''\n    return ClassLabel(names=['O', 'I', 'B'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None)\n[1, 0, 2] 1\nB ['B', 'I', 'O']\n"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\nprint(classlabel)\nprint(classlabel.str2int(['I', 'O', 'B']), classlabel.str2int('I'))\nprint(classlabel.int2str(2), classlabel.int2str([2, 1, 0]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef tag_sentence(sentence, labels, classlabel=None): \n    '''\n    requirement: both sentence and labels are already cleaned\n    '''\n    sentence_words = sentence.split()\n    \n    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n                                  for label in labels): # positive sample\n        nes = [classlabel.str2int('O')] * len(sentence_words)\n        for label in labels:\n            label_words = label.split()\n\n            all_pos = find_sublist(sentence_words, label_words)\n            for pos in all_pos:\n                nes[pos] = classlabel.str2int('B')\n                for i in range(pos+1, pos+len(label_words)):\n                    nes[i] = classlabel.str2int('I')\n\n        return True, list(zip(sentence_words, nes))\n        \n    else: # negative sample\n        nes = [classlabel.str2int('O')] * len(sentence_words)\n        return False, list(zip(sentence_words, nes))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "A label is found in the sentence: True\n(token, tag) pairs:\n[('The', 2), ('International', 1), ('Standard', 0), ('Classification', 0), ('of', 0), ('Education', 0), ('known', 0), ('by', 0), ('its', 0), ('acronym', 0), ('ISCED', 0), ('was', 0), ('developed', 0), ('by', 0), ('the', 0), ('United', 2), ('Nations', 1), ('Educational', 1), ('Scientific', 0), ('and', 0), ('Cultural', 2), ('Organization', 1), ('during', 0), ('the', 0), ('late', 0), ('1960s', 0), ('and', 0), ('1970s', 0)]\n"
    }
   ],
   "source": "sentence = (\"The International Standard Classification of Education, known by its acronym ISCED, \"\n            \"was developed by the United Nations Educational, \"\n            \"Scientific, and Cultural Organization during the late 1960s and 1970s\")\nlabels = ['The International', 'Cultural Organization', 'United Nations Educational']\n\nsentence = clean_training_text(sentence)\nlabels = [clean_training_text(label) for label in labels]\nclasslabel = get_ner_classlabel()\nfound_any, token_tags = tag_sentence(sentence, labels, classlabel=classlabel)\n\nprint('A label is found in the sentence:', found_any)\nprint('(token, tag) pairs:')\nprint(token_tags)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef extract_sentences(paper, sentence_definition='sentence'):\n    if sentence_definition == 'sentence':\n        sentences = set(clean_training_text(sentence) \n                        for sec in paper for sentence in sec['text'].split('.') if sec['text'])\n    elif sentence_definition == 'section':\n        sentences = set(clean_training_text(sec['section_title'] + '\\n' + sec['text']) \n                        for sec in paper if sec['text'])\n    return sentences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Sentence definition = normal sentence\n302 ['The introduction of charter school competition causes an approximate one percent increase in the score which constitutes about one quarter of the average yearly growth', '', '003 which is statistically insignificant', 'One possible alternative explanation for improved traditional school achievement when a charter school opens nearby is migration from the traditional school to the charter by lower performing students', 'Finally for those students that we are able to identify we examine the performance of students who switched from a traditional school to a charter', 'As an alternative to distance or the log of distance in some regressions we calculate five indicator variables that equal 1 if and only if the traditional school is within a given number of kilometers 5 10 15 20 and 25 kilometers of the nearest charter', 'We therefore estimate the following test score production function 6 Our use of distance as the relevant cost component follows originally from Hotelling 1949 who proposed that travel cost which is a function of distance is an important determinant of the demand for recreational goods', 'Because the lagged SCORE term and the i term a school specific time invariant component are correlated OLS estimation will be inconsistent', 'We also calcaulate an indicator for whether a charter school is operating in the same county as the traditional school the mean of which also increases over time', 'More common are charters that focus on traditional subjects but use an atypical curriculum']\n\nSentence definition = paper section\n12 ['Conclusion Using North Carolina data on charter school location and achievement test results we explore the effect of school choice on school quality We find traditional school achievement gains to charter school competition across a wide set of models Overall the results imply an approximate one percent increase in achievement when a traditional school faces competition from a charter school This increase represents approximately one quarter of the mean standard deviation of observed gains suggesting a considerable return to school choice Our results conflict with Bettinger s 1999 finding that charter school competition has no effect on traditional schools but this difference may be due to different pre charter competitive environments in North Carolina and Michigan North Carolina has 117 independent traditional school districts while Michigan has over 500 North Carolina school districts correspond roughly to counties so residents have less ability to exert Tiebout choice over their school districts Michigan parents in contrast have a much larger number of school districts within a small distance of their residences 17 A caveat is that we make two sets of simplifying assumptions regarding school choice First we ignore all non charter school intra system choices For example almost seventy percent of North Carolina school districts offer some form of school choice such as intra district transfers magnets and year round schools North Carolina Office of Lieutenant Governor 2000 Combined with private and alternative schools and home schooling residents of most North Carolina counties have some form of school choice such as intra district transfers magnet schools and year round schools Ignoring these alternatives leads to an overestimate of the distance to the nearest competitor The direction of any resulting bias is not clear However if one interprets the results as the effect of charter school choice rather than school choice in general then this problem is eliminated Second we make some important assumptions about transferring into a charter school Interdistrict transfers are allowed in the model It is assumed that there are open seats in the charter so that a threat to disenroll is credible We also assume that the size of the charter has no effect on the impact of competition though it is possible that this impact will increase with size of the charter Nevertheless this paper adds to the literature on school competition using a simple model that incorporates cost and quality and heretofore unanalyzed data The results suggest important gains in traditional school achievement due to the introduction and growth of charter school choice Standard errors in parentheses Significant at 5 Significant at 1 Each cell contains the parameter estimate on the distance measure from a regression using data only from the indicated year with the corresponding distance variable as the only included distance measure Thus the table reports an estimate for 3 5 15 regressions Other included regressors include the percent of the students that are African American Hispanic and free lunch eligible the county population density school enrollment and the natural log of the lagged performance score instrumented by the natural log of the twice lagged score The sample size in 1997 98 is less than 1 307 due to missing data for the twice lagged score Each cell represents a separate regression in which the given variable is the only distance indicator included N 1 307 schools in all cases Significant at 1 percent Significant at 5 percent Estimated standard errors are in parentheses Points of support specifies the number of discrete points of heterogeneity specified in the model 1 point assumes that there is no time invariant heterogeneity Each model also includes the logarithm of the lagged performance composite the proportion of the students that are Hispanic African American and free lunch eligible student to faculty ratio enrollment the county population density and year indicators Standard errors are in parentheses Each cell contains the parameter estimate on a charter within 10 kilometers indicator in a fixed effect average score regression that also includes year indicators The samples are cohorts of students over time For example the top panel follows those students in 7th grade in 1997 1998 to 8th grade 1998 1999 the cohort is not observed the following year because students are not observed after 8th grade The second row 6th grade 1997 1998 contains data for the three years 1997 1998 6th grade 1998 1999 7th grade and 1999 2000 8th grade', 'Maximum likelihood models Finally we specify the maximum likelihood model of Equation 8 which accounts for the timeinvariant unobserved characteristics as well as the initial condition The endogeneity is incorporated by points of support one point of support assumes that there is no time invariant heterogeneity As the points of support increase the model estimates more support points of the heterogeneity If heterogeneity exists then the one point of support model will yield inconsistent results Table 7 shows the results which are similar to those obtained in the cross sectional and panel IV regressions 15 Two distinct patterns emerge First the estimated effect of competition falls as distance increases as expected with just one exception the estimated effect of being 15 km is smaller than that of the estimated effect of begin within 20 km of a charter Traditional schools within 5 kilometers of a charter experience a one percent achievement gain traditional schools within 25 kilometers experience only about half of that gain Second unobserved heterogeneity has very little impact on the distance indicator coefficient estimates Although specification tests reject the parsimonious one point model which assumes that there is no unobserved heterogeneity the quantitative effect on the estimates of moving to the four point model is inconsequential']\n\n"
    }
   ],
   "source": "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[100:110]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', df.Id)\npaper = papers[df.Id.iloc[3]]\nprint('Sentence definition = normal sentence')\nsentences = extract_sentences(paper, sentence_definition='sentence')\nprint(len(sentences), list(sentences)[:10], end='\\n\\n')\nprint('Sentence definition = paper section')\nsentences = extract_sentences(paper, sentence_definition='section')\nprint(len(sentences), list(sentences)[:2], end='\\n\\n')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_paper_ner_data(paper, labels, sentence_definition='sentence', \n                       max_length=64, overlap=20):\n    '''\n    Get NER data for a paper.\n    '''\n    labels = [clean_training_text(label) for label in labels]\n    sentences = extract_sentences(paper, sentence_definition=sentence_definition)\n    sentences = shorten_sentences(sentences, max_length=max_length, overlap=overlap) \n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n\n    cnt_pos, cnt_neg, ner_data = 0, 0, []\n    for sentence in sentences:\n        is_positive, tags = tag_sentence(sentence, labels, classlabel=classlabel)\n        if is_positive:\n            cnt_pos += 1\n            ner_data.append(tags)\n        elif any(word in sentence.lower() for word in ['data', 'study']): \n            ner_data.append(tags)\n            cnt_neg += 1    \n    return cnt_pos, cnt_neg, ner_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "9 1\n[512, 512, 512, 512, 259, 303, 101, 512, 512, 49]\n[('that', 0), ('ADNI', 2), ('researchers', 0), ('are', 0), ('ready', 0), ('with', 0), ('guidance', 0), ('and', 0), ('ongoing', 0), ('research', 0), ('to', 0), ('return', 0), ('amyloid', 0), ('imaging', 0), ('research', 0), ('results', 0), ('to', 0), ('ADNI', 2), ('participants', 0), ('and', 0), ('more', 0), ('generally', 0), ('other', 0), ('biomarker', 0), ('results', 0), ('as', 0), ('well', 0), ('These', 0), ('findings', 0), ('also', 0), ('suggest', 0), ('that', 0), ('biomarker', 0), ('results', 0), ('can', 0), ('be', 0), ('disclosed', 0), ('as', 0), ('part', 0), ('of', 0), ('clinical', 0), ('trials', 0), ('designed', 0), ('to', 0), ('prevent', 0), ('cognitive', 0), ('decline', 0), ('10', 0), ('11', 0)]\n"
    }
   ],
   "source": "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[230:240]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n\nidx = 8\npaper = papers[df.Id.iloc[idx]]\nlabels = df.dataset_label.iloc[idx].split('|')\ncnt_pos, cnt_neg, ner_data = get_paper_ner_data(paper, labels, sentence_definition='section', \n                                               max_length=512, overlap=20)\nprint(cnt_pos, cnt_neg)\nprint([len(sec) for sec in ner_data])\nprint(ner_data[-1])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_ner_data(papers, df=None, classlabel=None, shuffle=True, \n                 sentence_definition='sentence', max_length=64, overlap=20):\n    '''\n    Args:\n        papers (dict): Like that returned by `load_papers`.\n        df (pd.DataFrame): Competition's train.csv or a subset of it.\n    '''\n    cnt_pos, cnt_neg = 0, 0 \n    ner_data = []\n\n    tqdm._instances.clear()\n    pbar = tqdm(total=len(df))\n    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():\n        paper = papers[id]\n        labels = dataset_label.split('|')\n                \n        cnt_pos_, cnt_neg_, ner_data_ = get_paper_ner_data(\n            paper, labels, sentence_definition=sentence_definition, max_length=max_length, overlap=overlap)\n        cnt_pos += cnt_pos_\n        cnt_neg += cnt_neg_\n        ner_data.extend(ner_data_)\n\n        pbar.update(1)\n        pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n\n    if shuffle:\n        random.shuffle(ner_data)\n    return cnt_pos, cnt_neg, ner_data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Training data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [00:00<00:00, 107.11it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Postive count: 168.   Negative count: 2574\n[('Most', 0), ('notable', 0), ('are', 0), ('the', 0), ('studies', 0), ('based', 0), ('on', 0), ('three', 0), ('national', 0), ('databases', 0), ('High', 0), ('School', 0), ('and', 0), ('Beyond', 0), ('the', 0), ('National', 0), ('Assessment', 0), ('of', 0), ('Educational', 0), ('Progress', 0), ('and', 0), ('the', 0), ('National', 2), ('Education', 1), ('Longitudinal', 1), ('Study', 1)]\nCPU times: user 1.02 s, sys: 53.3 ms, total: 1.07 s\nWall time: 1.09 s\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Training data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [00:19<00:00, 107.11it/s]"
    }
   ],
   "source": "%%time\ndf = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:100]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\nclasslabel = get_ner_classlabel()\ncnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False,\n                                          sentence_definition='sentence', max_length=64, overlap=20)\nprint(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\nprint(ner_data[250])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Training data size: 171 positives + 910 negatives: 100%|██████████| 100/100 [00:00<00:00, 118.28it/s]"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Postive count: 171.   Negative count: 910\n[('Study', 0), ('Notes', 0), ('A', 0), ('correction', 0), ('for', 0), ('multiple', 0), ('comparisons', 0), ('was', 0), ('needed', 0), ('but', 0), ('did', 0), ('not', 0), ('affect', 0), ('the', 0), ('statistical', 0), ('significance', 0), ('of', 0), ('the', 0), ('outcomes', 0), ('The', 0), ('p', 0), ('values', 0), ('presented', 0), ('here', 0), ('were', 0), ('reported', 0), ('in', 0), ('the', 0), ('original', 0), ('study', 0), ('Effect', 0), ('sizes', 0), ('were', 0), ('computed', 0), ('using', 0), ('the', 0), ('covariate', 0), ('adjusted', 0), ('mean', 0), ('difference', 0), ('and', 0), ('standard', 0), ('errors', 0), ('reported', 0), ('in', 0), ('Table', 0), ('1', 0), ('of', 0), ('the', 0), ('manuscript', 0), ('This', 0), ('quasi', 0), ('experimental', 0), ('study', 0), ('used', 0), ('propensity', 0), ('score', 0), ('matching', 0), ('to', 0), ('create', 0), ('a', 0), ('comparison', 0), ('group', 0), ('therefore', 0), ('the', 0), ('WWC', 0), ('required', 0), ('that', 0), ('baseline', 0), ('equivalence', 0), ('be', 0), ('established', 0), ('to', 0), ('determine', 0), ('whether', 0), ('the', 0), ('study', 0), ('met', 0), ('standards', 0), ('with', 0), ('reservations', 0), ('Because', 0), ('the', 0), ('two', 0), ('outcomes', 0), ('reported', 0), ('above', 0), ('did', 0), ('not', 0), ('have', 0), ('pretests', 0), ('two', 0), ('key', 0), ('covariates', 0), ('were', 0), ('selected', 0), ('by', 0), ('the', 0), ('WWC', 0), ('from', 0), ('among', 0), ('those', 0), ('used', 0), ('by', 0), ('the', 0), ('study', 0), ('author', 0), ('in', 0), ('the', 0), ('propensity', 0), ('score', 0), ('matching', 0), ('family', 0), ('income', 0), ('and', 0), ('prior', 0), ('achievement', 0), ('In', 0), ('response', 0), ('to', 0), ('a', 0), ('request', 0), ('by', 0), ('the', 0), ('WWC', 0), ('the', 0), ('author', 0), ('provided', 0), ('baseline', 0), ('means', 0), ('and', 0), ('standard', 0), ('deviations', 0), ('for', 0), ('the', 0), ('analytic', 0), ('sample', 0), ('on', 0), ('these', 0), ('two', 0), ('covariates', 0), ('The', 0), ('baseline', 0), ('effect', 0), ('sizes', 0), ('for', 0), ('family', 0), ('income', 0), ('and', 0), ('prior', 0), ('achievement', 0), ('were', 0), ('0', 0), ('02', 0), ('and', 0), ('0', 0), ('00', 0), ('respectively', 0), ('The', 0), ('study', 0), ('therefore', 0), ('meets', 0), ('WWC', 0), ('standards', 0), ('with', 0), ('reservations', 0), ('because', 0), ('baseline', 0), ('equivalence', 0), ('was', 0), ('met', 0), ('on', 0), ('these', 0), ('two', 0), ('covariates', 0), ('Table', 0), ('Notes', 0), ('Positive', 0), ('results', 0), ('for', 0), ('mean', 0), ('difference', 0), ('effect', 0), ('size', 0), ('and', 0), ('improvement', 0), ('index', 0), ('favor', 0), ('the', 0), ('intervention', 0), ('group', 0), ('negative', 0), ('results', 0), ('favor', 0), ('the', 0), ('comparison', 0), ('group', 0), ('The', 0), ('effect', 0), ('size', 0), ('is', 0), ('a', 0), ('standardized', 0), ('measure', 0), ('of', 0), ('the', 0), ('effect', 0), ('of', 0), ('an', 0), ('intervention', 0), ('on', 0), ('student', 0), ('outcomes', 0), ('representing', 0), ('the', 0), ('change', 0), ('measured', 0), ('in', 0), ('standard', 0), ('deviations', 0), ('in', 0), ('an', 0), ('average', 0), ('student', 0), ('s', 0), ('outcome', 0), ('that', 0), ('can', 0), ('be', 0), ('expected', 0), ('if', 0), ('the', 0), ('student', 0), ('is', 0), ('given', 0), ('the', 0), ('intervention', 0), ('The', 0), ('improvement', 0), ('index', 0), ('is', 0), ('an', 0), ('alternate', 0), ('presentation', 0), ('of', 0), ('the', 0), ('effect', 0), ('size', 0), ('reflecting', 0), ('the', 0), ('change', 0), ('in', 0), ('an', 0), ('average', 0), ('student', 0), ('s', 0), ('percentile', 0), ('rank', 0), ('that', 0), ('can', 0), ('be', 0), ('expected', 0), ('if', 0), ('the', 0), ('student', 0), ('is', 0), ('given', 0), ('the', 0), ('intervention', 0), ('nr', 0), ('not', 0), ('reported', 0)]\nCPU times: user 881 ms, sys: 92.2 ms, total: 973 ms\nWall time: 993 ms\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Training data size: 171 positives + 910 negatives: 100%|██████████| 100/100 [00:18<00:00, 118.28it/s]"
    }
   ],
   "source": "%%time\ndf = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:100]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\nclasslabel = get_ner_classlabel()\ncnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False, \n                                          sentence_definition='section', max_length=512, overlap=20)\nprint(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\nprint(ner_data[3])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef write_ner_json(ner_data, pth=Path('train_ner.json')):\n    with open(pth, 'w') as f:\n        for row in ner_data:\n            words, nes = list(zip(*row))\n            row_json = {'tokens' : words, 'ner_tags' : nes}\n            json.dump(row_json, f)\n            f.write('\\n')    "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{\"tokens\": [\"There\", \"is\", \"no\", \"dataset\", \"here\"], \"ner_tags\": [0, 0, 0, 0, 0]}\n{\"tokens\": [\"Load\", \"the\", \"UN\", \"Trade\", \"Development\", \"into\", \"view\"], \"ner_tags\": [0, 0, 2, 1, 1, 0, 0]}\n"
    }
   ],
   "source": "ner_data = [\n    [('There', 0), ('is', 0), ('no', 0), ('dataset', 0), ('here', 0)], \n    [('Load', 0), ('the', 0), ('UN', 2), ('Trade', 1), ('Development', 1), ('into', 0), ('view', 0)]\n]\nwrite_ner_json(ner_data, pth=Path('/kaggle/tmp_ner.json'))\n! cat /kaggle/tmp_ner.json"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef load_ner_datasets(data_files=None):\n    datasets = load_dataset('json', data_files=data_files)\n    classlabel = get_ner_classlabel()\n    for split, dataset in datasets.items():\n        dataset.features['ner_tags'].feature = classlabel\n    return datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-b4953f1660584c7e/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b4953f1660584c7e/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None)}\n\n{'tokens': ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view'], 'ner_tags': [0, 0, 2, 1, 1, 0, 0]}\n"
    }
   ],
   "source": "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\nprint(datasets['valid'].features)\nprint()\nprint(datasets['train'][1])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef create_tokenizer(model_checkpoint='bert-base-cased'):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n    return tokenizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01450d15e934226b247aa2e73624f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8044facedd0847fb8f25fa1244499ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07314b9df8044d7a0449068dbf467a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576627f57e6445f89ca9e4630ea14042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n{'input_ids': [101, 138, 188, 21943, 9930, 1104, 1234, 9026, 1121, 1103, 2286, 6194, 3499, 1113, 6356, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n{'input_ids': [101, 144, 6512, 9436, 24372, 1317, 185, 12937, 2042, 15520, 1114, 8626, 2330, 1447, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
    }
   ],
   "source": "tokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\nprint(\n    tokenizer(\"A smattering of people descended from the midday boat on Monday.\"))\nprint()\nprint(\n    tokenizer(\"Giglio boasts several pristine bays with crystal clear water\".split(), is_split_into_words=True)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef tokenize_and_align_labels(examples, tokenizer=None, label_all_tokens=True):\n    '''\n    Adds a new field called 'labels' that are the NER tags to the tokenized input.\n    \n    Args:\n        tokenizer (transformers.AutoTokenizer): Tokenizer.\n        examples (datasets.arrow_dataset.Dataset): Dataset.\n        label_all_tokens (bool): If True, all sub-tokens are given the same tag as the \n            first sub-token, otherwise all but the first sub-token are given the tag\n            -100.\n    '''\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(label[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]]}\n\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f8e18aafdc444f93d8683c2483c3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e51df443024d5e9e1c41e479dbb907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'ner_tags': [[0, 0, 0, 0, 0], [0, 0, 2, 1, 1, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tokens': [['There', 'is', 'no', 'dataset', 'here'], ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view']]}\n\n{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'ner_tags': [[0, 0, 0, 0, 0], [0, 0, 2, 1, 1, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tokens': [['There', 'is', 'no', 'dataset', 'here'], ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view']]}\n{'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
    }
   ],
   "source": "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\ntokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\n\nprint(tokenize_and_align_labels(datasets['train'][:], tokenizer, label_all_tokens=True), end='\\n\\n')\n\ntokenized_datasets = datasets.map(\n    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\n\nprint(tokenized_datasets['train'][:], end='\\n\\n')\nprint(tokenized_datasets['valid'][:])\nprint(tokenized_datasets['train'].features)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927edcecfb5e46e8814c6ec259f4e288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961.0, style=ProgressStyle(description…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "text/plain": "{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 2},\n 'overall_precision': 0.0,\n 'overall_recall': 0.0,\n 'overall_f1': 0.0,\n 'overall_accuracy': 0.3333333333333333}"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "metric = load_metric('seqeval')\n\npredictions = [['O', 'O', 'B', 'I', 'I', 'O']]\nreferences = [['B', 'I', 'B', 'O', 'O', 'O']]\nmetric.compute(predictions=predictions, references=references)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef jaccard_similarity(s1, s2):\n    l1 = set(s1.split(\" \"))\n    l2 = set(s2.split(\" \"))\n    intersection = len(list(l1.intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "jaccard_similarity('USGS Frog Counts Data', 'USGA Croc Counts Data') == 1 / 3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef compute_metrics(p, metric=None, label_list=None):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'precision': 0.24742268041237114,\n 'recall': 0.20512820512820512,\n 'f1': 0.22429906542056074,\n 'accuracy': 0.29583333333333334}"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\n\nbatch_size = 8\nmax_sentence_length = 30\n\npredictions = np.random.randn(batch_size, max_sentence_length, classlabel.num_classes)\nlabel_ids = np.random.randint(low=0, high=classlabel.num_classes, \n                              size=(batch_size, max_sentence_length), dtype=np.int16)\n\np = (predictions, label_ids)\nmetric = load_metric('seqeval')\ncompute_metrics(p, metric=metric, label_list=classlabel.names)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## NER training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Training data size: 12 positives + 100 negatives: 100%|██████████| 2/2 [00:00<00:00, 35.76it/s]\nTraining data size: 2 positives + 51 negatives: 100%|██████████| 2/2 [00:00<00:00, 67.78it/s] "
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train.  Positive count: 12.  Negative count: 100.\nValid.  Positive count: 2.  Negative count: 51.\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "\n"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\ntrain_meta = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:4]\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', train_meta.Id)\n\nvalid_cutoff = int(.50 * len(train_meta))\nvalid_meta = train_meta.iloc[:valid_cutoff].reset_index(drop=True)\ntrain_meta = train_meta.iloc[valid_cutoff:].reset_index(drop=True)\n\ntrain_cnt_pos, train_cnt_neg, train_ner_data = get_ner_data(papers, df=train_meta, classlabel=classlabel)\nvalid_cnt_pos, valid_cnt_neg, valid_ner_data = get_ner_data(papers, df=valid_meta, classlabel=classlabel)\nprint(f'Train.  Positive count: {train_cnt_pos}.  Negative count: {train_cnt_neg}.')\nprint(f'Valid.  Positive count: {valid_cnt_pos}.  Negative count: {valid_cnt_neg}.')\n\nwrite_ner_json(train_ner_data, pth='train_ner.json')\nwrite_ner_json(valid_ner_data, pth='valid_ner.json')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-b26d4902d94204c1/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b26d4902d94204c1/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e364a4b7d741df957be7de03363575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b805cdc77b814417ae1577fce645d1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d531edf17ea41a899d7edb03bd92440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2e3d9e9bec467e8f97fe390f848202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569f8c7cbbda4c089a85bfb3c3abdb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96392771997f4d98b1a8dc51b3742e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629df7cdb961454088f3068ca416bd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descri…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining data size: 168 positives + 2574 negatives: 100%|██████████| 100/100 [00:33<00:00,  3.03it/s] \nTraining data size: 171 positives + 910 negatives: 100%|██████████| 100/100 [00:31<00:00,  3.14it/s] \n"
    }
   ],
   "source": "datasets = load_ner_datasets(data_files={'train':'train_ner.json', 'valid':'valid_ner.json'})\n\nmodel_checkpoint = 'distilbert-base-cased'\ntokenizer = create_tokenizer(model_checkpoint)\ntokenized_datasets = datasets.map(\n    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=classlabel.num_classes)\n\nmetric = load_metric('seqeval')\n\nargs = TrainingArguments(output_dir='test_ner', num_train_epochs=2, \n                         learning_rate=2e-5, weight_decay=0.01,\n                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n                         save_strategy='epoch', save_total_limit=6)\n\ntrainer = Trainer(model=model, args=args, \n                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n                  data_collator=data_collator, tokenizer=tokenizer, \n                  compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14/14 01:07, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.824900</td>\n      <td>0.360206</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.988406</td>\n      <td>4.240800</td>\n      <td>12.498000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.239800</td>\n      <td>0.209530</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.988406</td>\n      <td>4.118300</td>\n      <td>12.869000</td>\n    </tr>\n  </tbody>\n</table><p>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=14, training_loss=0.4512731943811689, metrics={'train_runtime': 72.4713, 'train_samples_per_second': 0.193, 'total_flos': 5069424709440.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1241903104, 'train_mem_cpu_peaked_delta': 431964160})"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "trainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "total 8\ndrwxr-xr-x 2 root root 4096 May 28 11:40 checkpoint-7\ndrwxr-xr-x 2 root root 4096 May 28 11:41 checkpoint-14\n"
    }
   ],
   "source": "! ls -lrt test_ner"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 01:15, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>3</td>\n      <td>0.119000</td>\n      <td>0.108546</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.988406</td>\n      <td>4.406200</td>\n      <td>12.028000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.081600</td>\n      <td>0.089146</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.988406</td>\n      <td>4.370100</td>\n      <td>12.128000</td>\n    </tr>\n  </tbody>\n</table><p>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=28, training_loss=0.052854603954723904, metrics={'train_runtime': 78.352, 'train_samples_per_second': 0.357, 'total_flos': 10232727654240.0, 'epoch': 4.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 898568192, 'train_mem_cpu_peaked_delta': 554917888})"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "args = TrainingArguments(output_dir='test_ner', num_train_epochs=4, \n                         learning_rate=2e-5, weight_decay=0.01,\n                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n                         save_strategy='epoch', save_total_limit=6)\n\ntrainer = Trainer(model=model, args=args, \n                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n                  data_collator=data_collator, tokenizer=tokenizer, \n                  compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))\ntrainer.train(resume_from_checkpoint='/kaggle/working/test_ner/checkpoint-14/')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='8' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:08]\n    </div>\n    ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.08914603292942047,\n 'eval_precision': 0.0,\n 'eval_recall': 0.0,\n 'eval_f1': 0.0,\n 'eval_accuracy': 0.9884057971014493,\n 'eval_runtime': 4.3316,\n 'eval_samples_per_second': 12.236,\n 'epoch': 4.0,\n 'eval_mem_cpu_alloc_delta': 253952,\n 'eval_mem_cpu_peaked_delta': 0}"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "trainer.evaluate()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 4}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.9884057971014493}\n"
    }
   ],
   "source": "predictions, label_ids, _ = trainer.predict(tokenized_datasets['valid'])\npredictions = np.argmax(predictions, axis=2)\n\ntrue_predictions = [[classlabel.names[p] for p, i in zip(prediction, label_id) if i != -100]\n                    for prediction, label_id in zip(predictions, label_ids)]\n\ntrue_labels = [[classlabel.names[i] for i in label_id if i != -100] for label_id in label_ids]\n\nprint(true_predictions[0])\nprint(true_labels[0])\n\nresults = metric.compute(predictions=true_predictions, references=true_labels)\nprint(results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## NER inference\n\n**Turn off the Internet here**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_ner_inference_data(papers, sample_submission, classlabel=None):\n    '''\n    Args:\n        papers (dict): Each list in this dictionary consists of the section of a paper.\n        sample_submission (pd.DataFrame): Competition 'sample_submission.csv'.\n    Returns:\n        test_rows (list): Each list in this list is of the form: \n             [('goat', 0), ('win', 0), ...] and represents a sentence.  \n        paper_length (list): Number of sentences in each paper.\n    '''\n    test_rows = [] # test data in NER format\n    paper_length = [] # store the number of sentences each paper has\n\n    for paper_id in sample_submission['Id']:\n        # load paper\n        paper = papers[paper_id]\n\n        # extract sentences\n        sentences = [clean_training_text(sentence) for section in paper \n                     for sentence in section['text'].split('.')\n                    ]\n        sentences = shorten_sentences(sentences) # make sentences short\n        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n\n        # collect all sentences in json\n        for sentence in sentences:\n            sentence_words = sentence.split()\n            dummy_tags = [classlabel.str2int('O')]*len(sentence_words)\n            test_rows.append(list(zip(sentence_words, dummy_tags)))\n\n        # track which sentence belongs to which data point\n        paper_length.append(len(sentences))\n\n    print(f'total number of sentences: {len(test_rows)}')\n    return test_rows, paper_length"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "total number of sentences: 367\n[[('A', 0), ('recent', 0), ('large', 0), ('genomewide', 0), ('association', 0), ('study', 0), ('GWAS', 0), ('reported', 0), ('a', 0), ('genome', 0), ('wide', 0), ('significant', 0), ('locus', 0), ('for', 0), ('years', 0), ('of', 0), ('education', 0), ('which', 0), ('subsequently', 0), ('demonstrated', 0), ('association', 0), ('to', 0), ('general', 0), ('cognitive', 0), ('ability', 0), ('g', 0), ('in', 0), ('overlapping', 0), ('cohorts', 0)], [('The', 0), ('current', 0), ('study', 0), ('was', 0), ('designed', 0), ('to', 0), ('test', 0), ('whether', 0), ('GWAS', 0), ('hits', 0), ('for', 0), ('educational', 0), ('attainment', 0), ('are', 0), ('involved', 0), ('in', 0), ('general', 0), ('cognitive', 0), ('ability', 0), ('in', 0), ('an', 0), ('independent', 0), ('large', 0), ('scale', 0), ('collection', 0), ('of', 0), ('cohorts', 0)], [('We', 0), ('next', 0), ('conducted', 0), ('meta', 0), ('analyses', 0), ('with', 0), ('24', 0), ('189', 0), ('individuals', 0), ('with', 0), ('neurocognitive', 0), ('data', 0), ('from', 0), ('the', 0), ('educational', 0), ('attainment', 0), ('studies', 0), ('and', 0), ('then', 0), ('with', 0), ('53', 0), ('188', 0), ('largely', 0), ('independent', 0), ('individuals', 0), ('from', 0), ('a', 0), ('recent', 0), ('GWAS', 0), ('of', 0), ('cognition', 0)]]\n[34, 151, 98, 84]\n"
    }
   ],
   "source": "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test', sample_submission.Id)\nclasslabel = get_ner_classlabel()\ntest_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel)\nprint(test_rows[:3])\nprint(paper_length)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef ner_predict(test_rows, tokenizer=None, model=None, metric=None):\n    classlabel = get_ner_classlabel()\n    \n    write_ner_json(test_rows, pth='test_ner.json')\n    datasets = load_ner_datasets(data_files={'test':'test_ner.json'})\n    print('Tokenizing testset...')\n    tokenized_datasets = datasets.map(\n        partial(tokenize_and_align_labels,tokenizer=tokenizer, label_all_tokens=True), \n        batched=True) \n\n    print('Creating data collator...')\n    data_collator = DataCollatorForTokenClassification(tokenizer)\n    print('Creating (dummy) training arguments...')\n    args = TrainingArguments(output_dir='test_ner', num_train_epochs=3, \n                             learning_rate=2e-5, weight_decay=0.01,\n                             per_device_train_batch_size=16, per_device_eval_batch_size=16,\n                             evaluation_strategy='epoch', logging_steps=4, report_to='none', \n                             save_strategy='epoch', save_total_limit=6)\n\n    print('Creating trainer...')\n    trainer = Trainer(model=model, args=args, \n                      train_dataset=tokenized_datasets['test'], eval_dataset=tokenized_datasets['test'], \n                      data_collator=data_collator, tokenizer=tokenizer, \n                      compute_metrics=partial(compute_metrics, metric=metric, label_list=classlabel.names))\n\n    print('Predicting on test samples...')\n    predictions, label_ids, _ = trainer.predict(tokenized_datasets['test'])\n    predictions = predictions.argmax(axis=2)\n    true_predictions = [\n        [p for p, i in zip(prediction, label_id) if i != -100]\n        for prediction, label_id in zip(predictions, label_ids)]\n\n    return true_predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_checkpoint = 'test_ner/checkpoint-28/'\n\ntokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\nmetric = load_metric('seqeval')\n# matric = load_metric('seqeval', cache_dir='/root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exporting the cached metric \n\n# %cd /root/.cache\n# ! zip -r huggingface_cache.zip huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/\n# %cd "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-f2b45dcac33fe5e7/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f2b45dcac33fe5e7/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\nTokenizing testset...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33ea7f2ce1d4f0a9d76580243561366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nCreating data collator...\nCreating (dummy) training arguments...\nCreating trainer...\nPredicting on test samples...\n"
    },
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [23/23 00:30]\n    </div>\n    ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n  avg = a.mean(axis)\n/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n  ret = ret.dtype.type(ret / rcount)\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
    }
   ],
   "source": "bert_outputs = ner_predict(test_rows, tokenizer=tokenizer, model=model, metric=metric)\nprint(bert_outputs[0])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# predict_batch = 100\n\n# bert_outputs = []\n# for i in range(0, len(test_rows), predict_batch):\n#     print(f'\\rPredicting on samples {i} to {i + predict_batch}...', flush=True)\n#     b = ner_predict(test_rows[i:i + predict_batch], model=model, tokenizer=tokenizer, metric=metric)\n#     bert_outputs.append(b)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=None):\n    '''\n    Returns:\n        bert_dataset_labels (list): Each element is a set consisting of labels predicted\n            by the model.\n    '''\n    test_sentences = [list(zip(*row))[0] for row in test_rows]\n#     test_sentences = [row['tokens'] for row in test_rows]\n    \n    bert_dataset_labels = [] # store all dataset labels for each publication\n\n    for length in paper_length:\n        labels = set()\n        for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n            curr_phrase = ''\n            for word, tag in zip(sentence, pred):\n                if tag == classlabel.str2int('B'): # start a new phrase\n                    if curr_phrase:\n                        labels.add(curr_phrase)\n                        curr_phrase = ''\n                    curr_phrase = word\n                elif tag == classlabel.str2int('I') and curr_phrase: # continue the phrase\n                    curr_phrase += ' ' + word\n                else: # end last phrase (if any)\n                    if curr_phrase:\n                        labels.add(curr_phrase)\n                        curr_phrase = ''\n            # check if the label is the suffix of the sentence\n            if curr_phrase:\n                labels.add(curr_phrase)\n                curr_phrase = ''\n\n        # record dataset labels for this publication\n        bert_dataset_labels.append(labels)\n\n        del test_sentences[:length], bert_outputs[:length]\n        \n    return bert_dataset_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[{'Tigers EcoNAX dataset', 'present all the'}, {'WGS Equality Definitiveness Dataset'}]\n"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\nsentences = ['They do not present all the features', \n             'Despite the pretraining on the Tigers EcoNAX dataset',\n             'Weirdly there has been lots of studies based on WGS Equality Definitiveness Dataset']\npaper_length = [2, 1]\ntest_rows = [[(word, 0) for word in sentence.split()] for sentence in sentences]\nbert_outputs = [[0, 0, 0, 2, 1, 1, 0],\n                [0, 0, 0, 0, 0, 2, 1, 1],\n                [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1]]\nfor i, row in enumerate(test_rows):\n    assert len(row) == len(bert_outputs[i])\n\nbert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=classlabel)\nprint(bert_dataset_labels)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef filter_bert_labels(bert_dataset_labels):\n    '''\n    When several labels for a paper are too similar, keep just one of them.\n    '''\n    filtered_bert_labels = []\n\n    for labels in bert_dataset_labels:\n        filtered = []\n\n        for label in sorted(labels, key=len):\n            label = clean_training_text(label, lower=True)\n            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n                filtered.append(label)\n\n        filtered_bert_labels.append('|'.join(filtered))\n    return filtered_bert_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['moma artists|housing market|moma artists catalogue',\n 'deep sea rock salts|rhs fertiliser index']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "bert_dataset_labels = [{'moma artists catalogue', 'moma artists', 'housing market'},\n                       {'rhs flowers fertiliser index', 'deep sea rock salts', 'rhs fertiliser index'}]\n\nfilter_bert_labels(bert_dataset_labels)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Literal matching"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef create_knowledge_bank(pth):\n    '''\n    Args:\n        pth (str): Path to meta data like 'train.csv', which\n        needs to have columns: 'dataset_title', 'dataset_label', and 'cleaned_label'.\n        \n    Returns:\n        all_labels (set): All possible strings associated with a dataset from the meta data.\n    '''\n    df = load_train_meta(pth, group_id=False)\n    all_labels = set()\n    for label_1, label_2, label_3 in df[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n        all_labels.add(str(label_1).lower())\n        all_labels.add(str(label_2).lower())\n        all_labels.add(str(label_3).lower())\n    return all_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "180\n['2019 ncov complete genome sequences', '2019 ncov genome sequence', '2019 ncov genome sequences', '2019-ncov complete genome sequences', '2019-ncov genome sequence', '2019-ncov genome sequences', 'adni', 'advanced national seismic system (anss) comprehensive catalog (comcat)', 'advanced national seismic system anss comprehensive catalog comcat ', 'advanced national seismic system comprehensive catalog']\n"
    }
   ],
   "source": "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\nall_labels = create_knowledge_bank(pth)\nprint(len(all_labels))\nprint(sorted(all_labels)[:10])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef literal_match(paper, all_labels):\n    '''\n    Args:\n        paper ()\n    '''\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = clean_training_text(text_1, lower=True, total_clean=True)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_training_text(label, lower=True, total_clean=True))\n    return labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['adni|alzheimer s disease neuroimaging initiative adni',\n 'nces common core of data|trends in international mathematics and science study|common core of data',\n 'slosh model|sea lake and overland surges from hurricanes|noaa storm surge inundation',\n 'rural urban continuum codes']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n\npth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\nall_labels = create_knowledge_bank(pth)\n\nliteral_preds = []\nfor paper_id in sample_submission.Id:\n    paper = papers[paper_id]\n    literal_preds.append('|'.join(literal_match(paper, all_labels)))\n    \nliteral_preds"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Overall prediction for submission"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#export\ndef combine_matching_and_bert(literal_preds, filtererd_bert_labels):\n    '''\n    For a given sentence, if there's a literal match, use that as the final\n    prediction for the sentence.  If there isn't a literal match,\n    use what the model predicts.\n    '''\n    final_predictions = []\n    for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n        if literal_match:\n            final_predictions.append(literal_match)\n        else:\n            final_predictions.append(bert_pred)\n    return final_predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['mongolian racing cars|reallife headphones',\n 'hifi dataset|headphones collection data']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "literal_preds = ['mongolian racing cars|reallife headphones', '']\nfiltered_bert_labels = ['data|dataset', 'hifi dataset|headphones collection data']\ncombine_matching_and_bert(literal_preds, filtered_bert_labels)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Preparing NER inference data...\ntotal number of sentences: 367\nLoading model, tokenizer, and metric...\nPredicting on each sentence...\nDownloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-bb784e9f227f9de3/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-bb784e9f227f9de3/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\nTokenizing testset...\n"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26f3bb0cd4c427687db3eed3d645ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nCreating data collator...\nCreating (dummy) training arguments...\nCreating trainer...\nPredicting on test samples...\n"
    },
    {
     "data": {
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [23/23 00:29]\n    </div>\n    ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Getting predicted labels for each article...\nKeeping just one of labels that are too similar to each other...\n"
    }
   ],
   "source": "classlabel = get_ner_classlabel()\n\nprint('Preparing NER inference data...')\nsample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\npapers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\ntest_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel)\n\nprint('Loading model, tokenizer, and metric...')\nmodel_checkpoint = 'test_ner/checkpoint-28/'\ntokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\nmetric = load_metric('seqeval')\n\nprint('Predicting on each sentence...')\nbert_outputs = ner_predict(test_rows, tokenizer=tokenizer, model=model, metric=metric)\n\nprint('Getting predicted labels for each article...')\nbert_dataset_labels = get_bert_dataset_labels(test_rows, paper_length, bert_outputs, classlabel=classlabel)\n\nprint('Keeping just one of labels that are too similar to each other...')\nfiltered_bert_labels = filter_bert_labels(bert_dataset_labels)\n\nsample_submission['PredictionString'] = filtered_bert_labels\n\nsample_submission.to_csv('submission.csv', index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                     Id PredictionString\n0  2100032a-7c33-4bff-97ef-690822c43466                 \n1  2f392438-e215-4169-bebf-21ac4ff253e1                 \n2  3f316b38-1a24-45a9-8d8c-4e05a42257c6                 \n3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60                 "
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "sample_submission.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Reference\n- https://www.kaggle.com/tungmphung/pytorch-bert-for-named-entity-recognition/notebook\n- https://www.kaggle.com/tungmphung/coleridge-matching-bert-ner/notebook\n- https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n- https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin\n- https://huggingface.co/docs/datasets/loading_metrics.html"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
