{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "latter-coffee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-allah",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#default_exp showus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-railway",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\r\n",
      "Installing collected packages: fsspec\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 0.8.7\r\n",
      "    Uninstalling fsspec-0.8.7:\r\n",
      "      Successfully uninstalled fsspec-0.8.7\r\n",
      "Successfully installed fsspec-2021.4.0\r\n",
      "Looking in links: file:///kaggle/input/coleridge-packages/packages/datasets\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/datasets-1.5.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/huggingface_hub-0.0.7-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Processing /kaggle/input/coleridge-packages/packages/datasets/tqdm-4.49.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.3)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.11.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\n",
      "Installing collected packages: tqdm, xxhash, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.59.0\r\n",
      "    Uninstalling tqdm-4.59.0:\r\n",
      "      Successfully uninstalled tqdm-4.59.0\r\n",
      "Successfully installed datasets-1.5.0 huggingface-hub-0.0.7 tqdm-4.49.0 xxhash-2.0.0\r\n",
      "Processing /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (1.19.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval==1.2.2) (0.24.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.5.4)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (2.1.0)\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Processing /kaggle/input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Installing collected packages: tokenizers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.10.2\r\n",
      "    Uninstalling tokenizers-0.10.2:\r\n",
      "      Successfully uninstalled tokenizers-0.10.2\r\n",
      "Successfully installed tokenizers-0.10.1\r\n",
      "Processing /kaggle/input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.0.45)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.0.12)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2021.3.17)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (4.49.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (2.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (1.19.5)\r\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (0.10.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.5.0.dev0) (20.9)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.5.0.dev0) (3.7.4.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (4.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.5.1\r\n",
      "    Uninstalling transformers-4.5.1:\r\n",
      "      Successfully uninstalled transformers-4.5.1\r\n",
      "Successfully installed transformers-4.5.0.dev0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install /kaggle/input/nlp-packages/datasets/datasets/fsspec-2021.4.0-py3-none-any.whl\n",
    "! pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n",
    "! pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n",
    "! pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n",
    "! pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-means",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from functools import partial\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers, seqeval\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset, ClassLabel, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-giving",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-furniture",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "Path.ls = lambda pth: list(pth.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-frequency",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-termination",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_train_meta(pth, group_id=True):\n",
    "    df = pd.read_csv(pth)\n",
    "    if group_id:\n",
    "        df = df.groupby('Id').agg({'pub_title': 'first', 'dataset_title': '|'.join, \n",
    "                                   'dataset_label': '|'.join, 'cleaned_label': '|'.join}).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-swedish",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14316 19661\n",
      "['Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " 'Beginning Postsecondary Students Longitudinal Study|Education Longitudinal Study|Beginning Postsecondary Students'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Baltimore Longitudinal Study of Aging (BLSA)|Baltimore Longitudinal Study of Aging'\n",
      " \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n",
      " 'Beginning Postsecondary Student|Beginning Postsecondary Students']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "df = load_train_meta(pth, group_id=True)\n",
    "df_nogroup = load_train_meta(pth, group_id=False)\n",
    "print(len(df), len(df_nogroup))\n",
    "dup_ids = df_nogroup[df_nogroup.Id.duplicated()].Id.unique()\n",
    "print(df[df.Id.isin(dup_ids)].dataset_label.values[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-monitor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_papers(dir_json, paper_ids):\n",
    "    '''\n",
    "    Load papers into a dictionary.\n",
    "    \n",
    "    `papers`: \n",
    "        {''}\n",
    "    '''\n",
    "    \n",
    "    papers = {}\n",
    "    for paper_id in paper_ids:\n",
    "        with open(f'{dir_json}/{paper_id}.json', 'r') as f:\n",
    "            paper = json.load(f)\n",
    "            papers[paper_id] = paper\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-choice",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'section_title': 'Abstract', 'text': \"Water-quality data for nitrate, fecalindicator bacteria, pesticides, and volatile organic compounds collected in parts of Middle Tennessee and northern Alabama indicate that the Mississippian carbonate aquifer in these areas is susceptible to contamination from point and nonpoint sources. Thirty randomly located wells (predominantly domestic), two springs, and two additional publicsupply wells were sampled in the summer of 1999 as part of the U.S. Geological Survey's National Water-Quality Assessment (NAWQA) Program.\"}\n"
     ]
    }
   ],
   "source": [
    "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[-10:]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "print(type(papers))\n",
    "print(\n",
    "    papers[ np.random.choice(df.Id.values) ][0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-copyright",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_sample_text(jpth):\n",
    "    sections = json.loads(jpth.read_text())\n",
    "    text = '\\n'.join(section['text'] for section in sections)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-possible",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s. ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data. Creating a U.S. crosswalk to this system has been a goal of the National Center for Education Statistics and the Office of Research since the late 197,,s, when the National Institute of Education (the predecessor agency to the Office of Educational Research and Improvement) began exploring the idea. The design and implementation of a workable crosswalk, however, awaited the advent of changes to the Classification of Instructional Programs (CIP) system. The 1990 revision of the CIP system laid the foundation for a workable international crosswalk. Adoption of the National Education Goals set global consciousness and international educational comparisons firml\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "print(load_sample_text(jpths_trn[0])[:1_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-sender",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-desire",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_training_text(txt, lower=False, total_clean=False):\n",
    "    \"\"\"\n",
    "    similar to the default clean_text function but without lowercasing.\n",
    "    \"\"\"\n",
    "    txt = str(txt).lower() if lower else str(txt)\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt).strip()\n",
    "    if total_clean:\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-track",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle This competition awards 90 000\n",
      "hopkld 7 11 002\n"
     ]
    }
   ],
   "source": [
    "print(clean_training_text('@kaggle This competition awards $90,000!!!!.'))\n",
    "print(clean_training_text('HoPKLd + 7 ! 11,002', total_clean=True, lower=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-lending",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def shorten_sentences(sentences, max_length=64, overlap=20):\n",
    "    '''\n",
    "    Args:\n",
    "        sentences (list): List of sentences.\n",
    "        max_length (int): Maximum number of words allowed for each sentence.\n",
    "        overlap (int): If a sentence exceeds `max_length`, we split it to multiple sentences with \n",
    "            this amount of overlapping.\n",
    "    '''\n",
    "    short_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        if len(words) > max_length:\n",
    "            for p in range(0, len(words), max_length - overlap):\n",
    "                short_sentences.append(' '.join(words[p:p+max_length]))\n",
    "        else:\n",
    "            short_sentences.append(sentence)\n",
    "    return short_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-swimming",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['The International Standard Classification of Education, known by its acronym ISCED, was developed by the United Nations Educational, Scientific, and Cultural Organization during the late 1960s and 1970s', ' ISCED was implemented in 1976 and is the recognized international standard for reporting and interpreting education program data']\n",
      "\n",
      "After: ['The International Standard Classification of Education, known by its acronym', 'its acronym ISCED, was developed by the United Nations Educational,', 'Nations Educational, Scientific, and Cultural Organization during the late 1960s', 'late 1960s and 1970s', 'ISCED was implemented in 1976 and is the recognized international', 'recognized international standard for reporting and interpreting education program data', 'program data']\n"
     ]
    }
   ],
   "source": [
    "jpths_trn = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train/').ls()\n",
    "sentences = load_sample_text(jpths_trn[0]).split('.')[:2]\n",
    "short_sentences = shorten_sentences(sentences, max_length=10, overlap=2)\n",
    "print('Before:', sentences)\n",
    "print()\n",
    "print('After:', short_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-brain",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def find_sublist(big_list, small_list):\n",
    "    all_positions = []\n",
    "    for i in range(len(big_list) - len(small_list) + 1):\n",
    "        if small_list == big_list[i:i+len(small_list)]:\n",
    "            all_positions.append(i)\n",
    "    \n",
    "    return all_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-adrian",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list = ['If', 'the', 'thing', 'above', 'is', 'below', 'that', 'thing', 'which', 'is',\n",
    "            'not', 'as', 'high', 'up', 'on', 'the', 'thing', 'above', 'when', 'it', 'is', \n",
    "            'underneath', 'them.']\n",
    "small_list = ['the', 'thing', 'above']\n",
    "\n",
    "find_sublist(big_list, small_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-characteristic",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-tuning",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_classlabel():\n",
    "    '''\n",
    "    Labels for named entity recognition.\n",
    "        'O': Token not part of a phrase that mentions a dataset.\n",
    "        'I': Intermediate token of a phrase mentioning a dataset.\n",
    "        'B': First token of a phrase mentioning a dataset.\n",
    "    '''\n",
    "    return ClassLabel(names=['O', 'I', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-warrior",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None)\n",
      "[1, 0, 2] 1\n",
      "B ['B', 'I', 'O']\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "print(classlabel)\n",
    "print(classlabel.str2int(['I', 'O', 'B']), classlabel.str2int('I'))\n",
    "print(classlabel.int2str(2), classlabel.int2str([2, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-adoption",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tag_sentence(sentence, labels, classlabel=None): \n",
    "    '''\n",
    "    requirement: both sentence and labels are already cleaned\n",
    "    '''\n",
    "    sentence_words = sentence.split()\n",
    "    \n",
    "    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n",
    "                                  for label in labels): # positive sample\n",
    "        nes = [classlabel.str2int('O')] * len(sentence_words)\n",
    "        for label in labels:\n",
    "            label_words = label.split()\n",
    "\n",
    "            all_pos = find_sublist(sentence_words, label_words)\n",
    "            for pos in all_pos:\n",
    "                nes[pos] = classlabel.str2int('B')\n",
    "                for i in range(pos+1, pos+len(label_words)):\n",
    "                    nes[i] = classlabel.str2int('I')\n",
    "\n",
    "        return True, list(zip(sentence_words, nes))\n",
    "        \n",
    "    else: # negative sample\n",
    "        nes = [classlabel.str2int('O')] * len(sentence_words)\n",
    "        return False, list(zip(sentence_words, nes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-bradford",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A label is found in the sentence: True\n",
      "(token, tag) pairs:\n",
      "[('The', 2), ('International', 1), ('Standard', 0), ('Classification', 0), ('of', 0), ('Education', 0), ('known', 0), ('by', 0), ('its', 0), ('acronym', 0), ('ISCED', 0), ('was', 0), ('developed', 0), ('by', 0), ('the', 0), ('United', 2), ('Nations', 1), ('Educational', 1), ('Scientific', 0), ('and', 0), ('Cultural', 2), ('Organization', 1), ('during', 0), ('the', 0), ('late', 0), ('1960s', 0), ('and', 0), ('1970s', 0)]\n"
     ]
    }
   ],
   "source": [
    "sentence = (\"The International Standard Classification of Education, known by its acronym ISCED, \"\n",
    "            \"was developed by the United Nations Educational, \"\n",
    "            \"Scientific, and Cultural Organization during the late 1960s and 1970s\")\n",
    "labels = ['The International', 'Cultural Organization', 'United Nations Educational']\n",
    "\n",
    "sentence = clean_training_text(sentence)\n",
    "labels = [clean_training_text(label) for label in labels]\n",
    "classlabel = get_ner_classlabel()\n",
    "found_any, token_tags = tag_sentence(sentence, labels, classlabel=classlabel)\n",
    "\n",
    "print('A label is found in the sentence:', found_any)\n",
    "print('(token, tag) pairs:')\n",
    "print(token_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-pavilion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_sentences(paper, sentence_definition='sentence'):\n",
    "    if sentence_definition == 'sentence':\n",
    "        sentences = set(clean_training_text(sentence) \n",
    "                        for sec in paper for sentence in sec['text'].split('.') if sec['text'])\n",
    "    elif sentence_definition == 'section':\n",
    "        sentences = set(clean_training_text(sec['section_title'] + '\\n' + sec['text']) \n",
    "                        for sec in paper if sec['text'])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-portland",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..... Sentence definition = normal sentence\n",
      "302 ['', '7 percent so ignoring statistical significance halving the distance to the nearest charter would bring about an increase of just less than ten percent of the average achievement gain']\n",
      "\n",
      "..... Sentence definition = paper section\n",
      "12 ['IV panel models This elasticity might seem small at first blush but recall that it is conditional on the previous year performance composite The average performance composite gain in 1999 2000 for example is 1 11 points 1 7 percent so ignoring statistical significance halving the distance to the nearest charter would bring about an increase of just less than ten percent of the average achievement gain Models 2 through 6 control for charter school competition using indicators for whether a charter school was operating within a given distance In all of these models charter school competition raises the performance composite of the traditional school The effect is significant at standard significance levels for all indicators except within five kilometers and is nearly significant for that indicator In all five cases charter school competition increases traditional school performance by about one percent This represents more than one half of the average achievement gain of 1 7 percent in 1999 2000 The magnitudes of these effects are roughly two to five times as large as that of decreasing the student faculty ratio by 1 Introducing school choice seems like a promising alternative to lowering class size which has received substantially more public policy attention and is likely more cost effective in the context of charter schools Since state funding follows the student an increase in the charter school system implies no increase in spending 14 For instance in 2002 the North Carolina Governor s Office proposed a US 26 million increase in the state budget to reduce average class size by roughly 1 8 students Ignoring statistical significance this would increase scores by approximately 36 percent about one third of the increase attributable to the opening of a neighboring charter school Model 7 uses an indicator for whether a charter school is operating in the same county as the traditional school The point estimate on this parameter is again positive and similar in magnitude to the other indicators although it is marginally insignificant', 'Conceptual model Consider an agent e g a school principal who manages the sole traditional public school in the feeder district Parents can choose for their children to attend either the traditional school or a competing charter school that has a price of attendance p 5 Assume that the utility function of the agent has two components effort exerted by the agent e and membership of the school M M is a demand function defined as where q is the quality of the traditional school We assume that U is decreasing in e increasing in M and concave in each Since M is a demand function it is increasing in both the price of a substitute good p and the quality q The agent can increase the quality of the school by exerting more effort perhaps through more staff meetings greater vigilance of instruction quality or implementation of after school programs although the second derivative of q e is negative The agent chooses e to maximize U e M q e p The first order condition is where subscripts denote partial derivatives Equation 4 indicates that at the optimal effort level the marginal disutility from effort equals the marginal utility stemming from the increased membership that results from the positive impact of additional effort on school quality We examine the impact of increased charter school availability on traditional school test scores or in terms of the model the effect of decreases in p on q Since q e 0 the sign of q p is the same as the sign of e p Using the implicit function theorem it can be shown that a sufficient condition for e p 0 is That is a decrease in cost of charter school attendance increases the quality of the competing traditional school if the marginal disutility of effort is increasing in membership or symmetrically the marginal utility of membership is decreasing in effort This condition implies that as enrollment in the traditional school increases the agent has less incentive to exert substantial effort Alternatively at high levels of effort increases in membership provide little benefit We define the traditional school s competitor as the nearest charter school and examine whether the distance to the competitor influences the performance of the traditional school 6 Since charter schools charge no tuition a major component of the price of attending the competitor is travel cost Travel cost in terms of lost wages depreciation of the transport vehicle and lost leisure time increases as distance from the traditional school increases holding other factors constant Thus we use the distance to the nearest charter school as a proxy for travel cost and the price of attendance p to examine whether charter school competition increases traditional school quality Private schools and neighboring traditional school districts also compete with traditional schools We ignore these two additional sources of competition because the cost of switching to either is substantially higher than that of switching to a charter school Unlike traditional and charter schools tuition at private schools is considerable Meanwhile switching between neighboring traditional school districts typically requires the family to move its residence Thus threats to transfer to a nearby charter school are more credible than threats to transfer to a neighboring traditional or private school']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[100:110]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', df.Id)\n",
    "paper = papers[df.Id.iloc[3]]\n",
    "print('..... Sentence definition = normal sentence')\n",
    "sentences = extract_sentences(paper, sentence_definition='sentence')\n",
    "print(len(sentences), list(sentences)[:2], end='\\n\\n')\n",
    "print('..... Sentence definition = paper section')\n",
    "sentences = extract_sentences(paper, sentence_definition='section')\n",
    "print(len(sentences), list(sentences)[:2][:1_000], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-nursery",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_paper_ner_data(paper, labels, classlabel=None,\n",
    "                       sentence_definition='sentence', max_length=64, overlap=20):\n",
    "    '''\n",
    "    Get NER data for a single paper.\n",
    "    '''\n",
    "    labels = [clean_training_text(label) for label in labels]\n",
    "    sentences = extract_sentences(paper, sentence_definition=sentence_definition)\n",
    "    sentences = shorten_sentences(sentences, max_length=max_length, overlap=overlap) \n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n",
    "\n",
    "    cnt_pos, cnt_neg, ner_data = 0, 0, []\n",
    "    for sentence in sentences:\n",
    "        is_positive, tags = tag_sentence(sentence, labels, classlabel=classlabel)\n",
    "        if is_positive:\n",
    "            cnt_pos += 1\n",
    "            ner_data.append(tags)\n",
    "        elif any(word in sentence.lower() for word in ['data', 'study']): \n",
    "            ner_data.append(tags)\n",
    "            cnt_neg += 1    \n",
    "    return cnt_pos, cnt_neg, ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-holly",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1\n",
      "[512, 512, 512, 512, 259, 512, 512, 49, 101, 303]\n",
      "[('AUTHOR', 0), ('CONTRIBUTIONS', 0), ('Dr', 0), ('Shulman', 0), ('contributed', 0), ('to', 0), ('study', 0), ('concept', 0), ('and', 0), ('design', 0), ('acquisition', 0), ('of', 0), ('data', 0), ('analysis', 0), ('and', 0), ('interpretation', 0), ('critical', 0), ('revision', 0), ('of', 0), ('the', 0), ('manuscript', 0), ('for', 0), ('important', 0), ('intellectual', 0), ('content', 0), ('and', 0), ('study', 0), ('supervision', 0), ('Ms', 0), ('Harkins', 0), ('contributed', 0), ('to', 0), ('study', 0), ('concept', 0), ('and', 0), ('design', 0), ('acquisition', 0), ('of', 0), ('data', 0), ('analysis', 0), ('and', 0), ('interpretation', 0), ('and', 0), ('critical', 0), ('revision', 0), ('of', 0), ('the', 0), ('manuscript', 0), ('for', 0), ('important', 0), ('intellectual', 0), ('content', 0), ('Dr', 0), ('Green', 0), ('contributed', 0), ('to', 0), ('study', 0), ('concept', 0), ('and', 0), ('design', 0), ('analysis', 0), ('and', 0), ('interpretation', 0), ('critical', 0), ('revision', 0), ('of', 0), ('the', 0), ('manuscript', 0), ('for', 0), ('important', 0), ('intellectual', 0), ('content', 0), ('and', 0), ('study', 0), ('supervision', 0), ('Dr', 0), ('Karlawish', 0), ('contributed', 0), ('to', 0), ('study', 0), ('concept', 0), ('and', 0), ('design', 0), ('acquisition', 0), ('of', 0), ('data', 0), ('analysis', 0), ('and', 0), ('interpretation', 0), ('critical', 0), ('revision', 0), ('of', 0), ('the', 0), ('manuscript', 0), ('for', 0), ('important', 0), ('intellectual', 0), ('content', 0), ('and', 0), ('study', 0), ('supervision', 0)]\n"
     ]
    }
   ],
   "source": [
    "df = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv', group_id=True).iloc[230:240]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "\n",
    "idx = 8\n",
    "paper = papers[df.Id.iloc[idx]]\n",
    "labels = df.dataset_label.iloc[idx].split('|')\n",
    "cnt_pos, cnt_neg, ner_data = get_paper_ner_data(paper, labels, classlabel=classlabel,\n",
    "                                                sentence_definition='section', max_length=512, overlap=20)\n",
    "print(cnt_pos, cnt_neg)\n",
    "print([len(sec) for sec in ner_data])\n",
    "print(ner_data[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-convention",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_data(papers, df=None, classlabel=None, shuffle=True, \n",
    "                 sentence_definition='sentence', max_length=64, overlap=20):\n",
    "    '''\n",
    "    Get NER data for a list of papers.\n",
    "    \n",
    "    Args:\n",
    "        papers (dict): Like that returned by `load_papers`.\n",
    "        df (pd.DataFrame): Competition's train.csv or a subset of it.\n",
    "    Returns:\n",
    "        cnt_pos (int): Number of samples (or 'sentences') that are tagged or partly\n",
    "            tagged as datasets.\n",
    "        cnt_neg (int): Number of samples (or 'sentences') that are not tagged\n",
    "            or partly tagged as datasets.\n",
    "        ner_data (list): List of samples, or 'sentences'. Each element is of the form:\n",
    "            [('There', 0), ('has', 0), ('been', 0), ...]\n",
    "    '''\n",
    "    cnt_pos, cnt_neg = 0, 0 \n",
    "    ner_data = []\n",
    "\n",
    "    tqdm._instances.clear()\n",
    "    pbar = tqdm(total=len(df))\n",
    "    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():\n",
    "        paper = papers[id]\n",
    "        labels = dataset_label.split('|')\n",
    "                \n",
    "        cnt_pos_, cnt_neg_, ner_data_ = get_paper_ner_data(\n",
    "            paper, labels, classlabel=classlabel, \n",
    "            sentence_definition=sentence_definition, max_length=max_length, overlap=overlap)\n",
    "        cnt_pos += cnt_pos_\n",
    "        cnt_neg += cnt_neg_\n",
    "        ner_data.extend(ner_data_)\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(ner_data)\n",
    "    return cnt_pos, cnt_neg, ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-parameter",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 29 positives + 223 negatives: 100%|██████████| 10/10 [00:00<00:00, 158.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive count: 29.   Negative count: 223\n",
      "[('Likewise', 0), ('there', 0), ('is', 0), ('a', 0), ('familiarity', 0), ('with', 0), ('a', 0), ('variety', 0), ('of', 0), ('studies', 0), ('that', 0), ('provide', 0), ('specific', 0), ('data', 0), ('on', 0), ('the', 0), ('academic', 0), ('achievement', 0), ('of', 0), ('Catholic', 0), ('school', 0), ('students', 0)]\n",
      "CPU times: user 125 ms, sys: 5.07 ms, total: 130 ms\n",
      "Wall time: 196 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:10]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "cnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False,\n",
    "                                          sentence_definition='sentence', max_length=64, overlap=20)\n",
    "print(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\n",
    "print(ner_data[250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-range",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 27 positives + 70 negatives: 100%|██████████| 10/10 [00:00<00:00, 194.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postive count: 27.   Negative count: 70\n",
      "[('WWC', 0), ('Rating', 0), ('The', 0), ('research', 0), ('described', 0), ('in', 0), ('this', 0), ('report', 0), ('meets', 0), ('WWC', 0), ('evidence', 0), ('standards', 0), ('with', 0), ('reservations', 0), ('Cautions', 0), ('Although', 0), ('the', 0), ('study', 0), ('matched', 0), ('students', 0), ('who', 0), ('participated', 0), ('in', 0), ('dual', 0), ('enrollment', 0), ('programs', 0), ('to', 0), ('those', 0), ('who', 0), ('did', 0), ('not', 0), ('students', 0), ('who', 0), ('self', 0), ('selected', 0), ('to', 0), ('participate', 0), ('in', 0), ('dual', 0), ('enrollment', 0), ('programs', 0), ('may', 0), ('have', 0), ('been', 0), ('different', 0), ('from', 0), ('students', 0), ('in', 0), ('general', 0), ('high', 0), ('school', 0), ('programs', 0), ('in', 0), ('ways', 0), ('that', 0), ('were', 0), ('unobserved', 0), ('in', 0), ('the', 0), ('study', 0), ('data', 0), ('Study', 0), ('sample', 0), ('A', 0), ('nationally', 0), ('representative', 0), ('sample', 0), ('of', 0), ('eighth', 0), ('graders', 0), ('was', 0), ('first', 0), ('surveyed', 0), ('in', 0), ('the', 0), ('spring', 0), ('of', 0), ('1988', 0)]\n",
      "CPU times: user 110 ms, sys: 12.2 ms, total: 123 ms\n",
      "Wall time: 124 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:10]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train/', df.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "cnt_pos, cnt_neg, ner_data = get_ner_data(papers, df, classlabel=classlabel, shuffle=False, \n",
    "                                          sentence_definition='section', max_length=512, overlap=20)\n",
    "print(f'Postive count: {cnt_pos}.   Negative count: {cnt_neg}')\n",
    "print(ner_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-storage",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def write_ner_json(ner_data, pth=Path('train_ner.json')):\n",
    "    '''\n",
    "    Save NER data to json file.\n",
    "    '''\n",
    "    with open(pth, 'w') as f:\n",
    "        for row in ner_data:\n",
    "            words, nes = list(zip(*row))\n",
    "            row_json = {'tokens' : words, 'ner_tags' : nes}\n",
    "            json.dump(row_json, f)\n",
    "            f.write('\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-environment",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tokens\": [\"There\", \"is\", \"no\", \"dataset\", \"here\"], \"ner_tags\": [0, 0, 0, 0, 0]}\r\n",
      "{\"tokens\": [\"Load\", \"the\", \"UN\", \"Trade\", \"Development\", \"into\", \"view\"], \"ner_tags\": [0, 0, 2, 1, 1, 0, 0]}\r\n"
     ]
    }
   ],
   "source": [
    "ner_data = [\n",
    "    [('There', 0), ('is', 0), ('no', 0), ('dataset', 0), ('here', 0)], \n",
    "    [('Load', 0), ('the', 0), ('UN', 2), ('Trade', 1), ('Development', 1), ('into', 0), ('view', 0)]\n",
    "]\n",
    "write_ner_json(ner_data, pth=Path('/kaggle/tmp_ner.json'))\n",
    "! cat /kaggle/tmp_ner.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-belief",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def load_ner_datasets(data_files=None):\n",
    "    '''\n",
    "    Load NER data in json files to a `datasets` object.  In addition,\n",
    "    Append the NER ClassLabel for the `ner_tags` feature.\n",
    "    '''\n",
    "    datasets = load_dataset('json', data_files=data_files)\n",
    "    classlabel = get_ner_classlabel()\n",
    "    for split, dataset in datasets.items():\n",
    "        dataset.features['ner_tags'].feature = classlabel\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-hello",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-6d5cd0925189c967/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27499e9f8514412fa40a603e8a06d7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062eaea55cfe43e1983a150e1a6618c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-6d5cd0925189c967/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "\n",
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None)}\n",
      "{'tokens': ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view'], 'ner_tags': [0, 0, 2, 1, 1, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\n",
    "print()\n",
    "print(datasets['valid'].features)\n",
    "print(datasets['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-trailer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_tokenizer(model_checkpoint='distilbert-base-cased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-smooth",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b9a54b306c48e4a5971858f6607154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=411.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1567673a7b468387bcddce71ae31ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a66c864e2a7442883d889e6086e9048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ded4afa02f4370950c128c6ccb8f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'input_ids': [101, 138, 188, 21943, 9930, 1104, 1234, 9026, 1121, 1103, 2286, 6194, 3499, 1113, 6356, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "{'input_ids': [101, 144, 6512, 9436, 24372, 1317, 185, 12937, 2042, 15520, 1114, 8626, 2330, 1447, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(model_checkpoint='distilbert-base-cased')\n",
    "print(\n",
    "    tokenizer(\"A smattering of people descended from the midday boat on Monday.\"))\n",
    "print()\n",
    "print(\n",
    "    tokenizer(\"Giglio boasts several pristine bays with crystal clear water\".split(), is_split_into_words=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-telescope",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'smattering', 'of', 'people', 'descended', 'from', 'the', 'midday', 'boat', 'on', 'Monday.']\n",
      "[None, 0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 10, None]\n",
      "['[CLS]', 'A', 's', '##mat', '##tering', 'of', 'people', 'descended', 'from', 'the', 'mid', '##day', 'boat', 'on', 'Monday', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"A smattering of people descended from the midday boat on Monday.\".split()\n",
    "tokenized_sentence = tokenizer(sentence, is_split_into_words=True)\n",
    "\n",
    "print(sentence)\n",
    "print(tokenized_sentence.word_ids())\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_sentence['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-overall",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_and_align_labels(examples, tokenizer=None, label_all_tokens=True):\n",
    "    '''\n",
    "    Adds a new field called 'labels' that are the NER tags to the tokenized input.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer.\n",
    "        examples (datasets.arrow_dataset.Dataset): Dataset.\n",
    "        label_all_tokens (bool): If True, all sub-tokens are given the same tag as the \n",
    "            first sub-token, otherwise all but the first sub-token are given the tag\n",
    "            -100.\n",
    "    '''\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids_all = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "        word_ids_all.append(word_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs['word_ids'] = word_ids_all\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-mauritius",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b5fe34c99e484789cb54a79e258503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29c7e0a8ec14d6cbdf7e5afc7f44d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6098f1d1216d48968fec37711d659833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c41d539d91433b9ed7de663d482bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"tokens\": [\"There\", \"is\", \"no\", \"dataset\", \"here\"], \"ner_tags\": [0, 0, 0, 0, 0]}\r\n",
      "{\"tokens\": [\"Load\", \"the\", \"UN\", \"Trade\", \"Development\", \"into\", \"view\"], \"ner_tags\": [0, 0, 2, 1, 1, 0, 0]}\r\n",
      "\n",
      "{'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'word_ids': [[None, 0, 1, 2, 3, 3, 4, None], [None, 0, 0, 1, 2, 3, 4, 5, 6, None]]}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83e9403768040de9c3814ba219f58bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc819244c264b278aafc399b5b962a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'input_ids': [[101, 1247, 1110, 1185, 2233, 9388, 1303, 102], [101, 10605, 3556, 1103, 7414, 5820, 3273, 1154, 2458, 102]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 2, 1, 1, 0, 0, -100]], 'ner_tags': [[0, 0, 0, 0, 0], [0, 0, 2, 1, 1, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'tokens': [['There', 'is', 'no', 'dataset', 'here'], ['Load', 'the', 'UN', 'Trade', 'Development', 'into', 'view']], 'word_ids': [[None, 0, 1, 2, 3, 3, 4, None], [None, 0, 0, 1, 2, 3, 4, 5, 6, None]]}\n",
      "{'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=3, names=['O', 'I', 'B'], names_file=None, id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'word_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "datasets = load_ner_datasets(data_files={'train':'/kaggle/tmp_ner.json', 'valid':'/kaggle/tmp_ner.json'})\n",
    "tokenizer = create_tokenizer(model_checkpoint='bert-base-cased')\n",
    "\n",
    "! cat /kaggle/tmp_ner.json\n",
    "\n",
    "print()\n",
    "print(tokenize_and_align_labels(datasets['train'][:], tokenizer, label_all_tokens=True), end='\\n\\n')\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\n",
    "print(tokenized_datasets['valid'][:])\n",
    "print(tokenized_datasets['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-circular",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495b1c5904e04236ba1ee78de7d26c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'_': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'overall_precision': 1.0, 'overall_recall': 1.0, 'overall_f1': 1.0, 'overall_accuracy': 1.0}\n",
      "{'_': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1}, 'overall_precision': 0.0, 'overall_recall': 0.0, 'overall_f1': 0.0, 'overall_accuracy': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric('seqeval')\n",
    "\n",
    "predictions = np.array([['O', 'O', 'B', 'I', 'I', 'O']])\n",
    "references = [['O', 'O', 'B', 'I', 'I', 'O']]\n",
    "print(metric.compute(predictions=predictions, references=references))\n",
    "\n",
    "predictions = [['O', 'O', 'B', 'I', 'I', 'O']]\n",
    "references = [['B', 'I', 'I', 'O', 'O', 'O']]\n",
    "print(metric.compute(predictions=predictions, references=references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-africa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def jaccard_similarity(s1, s2):\n",
    "    l1 = set(s1.split(\" \"))\n",
    "    l2 = set(s2.split(\" \"))\n",
    "    intersection = len(list(l1.intersection(l2)))\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-slave",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity('USGS Frog Counts Data', 'USGA Croc Counts Data') == 1 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-player",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_nonoriginal_outputs(outputs, word_ids):\n",
    "    '''\n",
    "    Remove elements that correspond to special tokens or subtokens,\n",
    "    retaining only those elements that correspond to a word in original\n",
    "    text.\n",
    "    \n",
    "    Args:\n",
    "        outputs (np.array): \n",
    "        \n",
    "    Returns:\n",
    "        outputs (list)\n",
    "    '''\n",
    "    assert len(outputs) == len(word_ids)\n",
    "    idxs = [[word_id.index(i) for i in set(word_id) if i is not None] \n",
    "            for word_id in word_ids]\n",
    "    outputs = [output[idx].tolist() for output, idx in zip(outputs, idxs)]\n",
    "    for output in outputs:\n",
    "        assert -100 not in output\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-buying",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions\n",
      "BEFORE:\n",
      "[[2 1 2 0 2 0 2 2]\n",
      " [0 1 2 1 2 0 0 0]]\n",
      "AFTER:\n",
      "[[1, 0, 2], [1, 2, 2]]\n",
      "============================================================\n",
      "label_ids\n",
      "BEFORE:\n",
      "[[-100    0    0    2    1    2    1 -100]\n",
      " [-100    2    1 -100    0 -100 -100 -100]]\n",
      "AFTER:\n",
      "[[0, 2, 1], [2, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "classlabel = get_ner_classlabel()\n",
    "\n",
    "predictions = np.random.randn(2, 8, classlabel.num_classes)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "label_ids = np.array([[-100, 0, 0,    2, 1,    2,    1, -100],\n",
    "                      [-100, 2, 1, -100, 0, -100, -100, -100]])\n",
    "\n",
    "word_ids = [[None, 0, 0, 1, 2, None],\n",
    "            [None, 0, 1, 1, 2, 2, None]]\n",
    "\n",
    "true_predictions = remove_nonoriginal_outputs(predictions, word_ids)\n",
    "true_label_ids   = remove_nonoriginal_outputs(label_ids,   word_ids)\n",
    "\n",
    "print('predictions')\n",
    "print('BEFORE:')\n",
    "print(predictions)\n",
    "print('AFTER:')\n",
    "print(true_predictions)\n",
    "print(60 * '=')\n",
    "print('label_ids')\n",
    "print('BEFORE:')\n",
    "print(label_ids)\n",
    "print('AFTER:')\n",
    "print(true_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-hurricane",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_metrics(p, metric=None, word_ids=None, label_list=None):\n",
    "    '''\n",
    "    1. Remove predicted and ground-truth class ids of special and sub tokens.\n",
    "    2. Convert class ids to class labels. (int ---> str)\n",
    "    3. Compute metric.\n",
    "    \n",
    "    Args:\n",
    "        p (tuple): 2-tuple consisting of model prediction and ground-truth\n",
    "            labels.  These will contain elements corresponding to special \n",
    "            tokens and sub-tokens.\n",
    "        word_ids (list): Word IDs from the tokenizer's output, indicating\n",
    "            which original word each sub-token belongs to.\n",
    "    '''\n",
    "    predictions, label_ids = p\n",
    "    predictions = predictions.argmax(axis=2)\n",
    "\n",
    "    true_predictions = remove_nonoriginal_outputs(predictions, word_ids)\n",
    "    true_label_ids = remove_nonoriginal_outputs(label_ids, word_ids)\n",
    "#     true_predictions = [[p for p, l, in zip(pred, label) if l != -100] \n",
    "#                         for pred, label in zip(predictions, label_ids)]\n",
    "#     true_label_ids   = [[l for l in label if l != -100] for label in label_ids]\n",
    "    \n",
    "    true_predictions = [[label_list[p] for p in pred] for pred in true_predictions]\n",
    "    true_labels = [[label_list[i] for i in label_id] for label_id in true_label_ids]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-pathology",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1 0 0 2]\n",
      " [1 1 1 0 2 2]]\n",
      "[[2 2 1 0 1 1]\n",
      " [0 2 2 1 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5, 'recall': 0.25, 'f1': 0.3333333333333333, 'accuracy': 0.4}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "max_example_length = 6\n",
    "\n",
    "predictions = np.random.randn(batch_size, max_example_length, classlabel.num_classes)\n",
    "label_ids = np.random.randint(low=0, high=classlabel.num_classes, \n",
    "                              size=(batch_size, max_example_length), dtype=np.int16)\n",
    "word_ids = [[None, 0, 0, 1, 2, None], \n",
    "            [None, 0, 1, None]]\n",
    "\n",
    "print(predictions.argmax(axis=2))\n",
    "print(label_ids)\n",
    "p = (predictions, label_ids)\n",
    "metric = load_metric('seqeval')\n",
    "compute_metrics(p, metric=metric, label_list=classlabel.names, word_ids=word_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-midwest",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NER training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-percentage",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 1 positives + 12 negatives: 100%|██████████| 1/1 [00:00<00:00, 153.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train.  Positive count: 3.  Negative count: 37.\n",
      "Valid.  Positive count: 1.  Negative count: 12.\n"
     ]
    }
   ],
   "source": [
    "train_meta = load_train_meta('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv').iloc[:3]\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/train', train_meta.Id)\n",
    "\n",
    "valid_cutoff = int(.50 * len(train_meta))\n",
    "valid_meta = train_meta.iloc[:valid_cutoff].reset_index(drop=True)\n",
    "train_meta = train_meta.iloc[valid_cutoff:].reset_index(drop=True)\n",
    "\n",
    "classlabel = get_ner_classlabel()\n",
    "train_cnt_pos, train_cnt_neg, train_ner_data = get_ner_data(\n",
    "    papers, df=train_meta, classlabel=classlabel, sentence_definition='section', max_length=300, overlap=100)\n",
    "valid_cnt_pos, valid_cnt_neg, valid_ner_data = get_ner_data(\n",
    "    papers, df=valid_meta, classlabel=classlabel, sentence_definition='section', max_length=300, overlap=100)\n",
    "print(f'Train.  Positive count: {train_cnt_pos}.  Negative count: {train_cnt_neg}.')\n",
    "print(f'Valid.  Positive count: {valid_cnt_pos}.  Negative count: {valid_cnt_neg}.')\n",
    "\n",
    "write_ner_json(train_ner_data, pth='train_ner.json')\n",
    "write_ner_json(valid_ner_data, pth='valid_ner.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-mozambique",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-65580daa9634842c/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8458b6ca9b444fb5b7ce8cb03478e9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0b98f03a874e74b2aff0ccfde6ca8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-65580daa9634842c/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60a7b1fbbcd42408b499b88115f304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acacf1eb4854dc0be699a55f7043544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35c884dac8f4dbbaee9cbcb53524dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training data size: 1 positives + 12 negatives: 100%|██████████| 1/1 [00:16<00:00, 16.87s/it] \n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training data size: 29 positives + 223 negatives: 100%|██████████| 10/10 [00:35<00:00,  3.55s/it] \n",
      "Training data size: 27 positives + 70 negatives: 100%|██████████| 10/10 [00:35<00:00,  3.52s/it] \n",
      "Training data size: 3 positives + 37 negatives: 100%|██████████| 2/2 [00:19<00:00,  9.68s/it]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_ner_datasets(data_files={'train':'train_ner.json', 'valid':'valid_ner.json'})\n",
    "\n",
    "model_checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = create_tokenizer(model_checkpoint)\n",
    "tokenized_datasets = datasets.map(\n",
    "    partial(tokenize_and_align_labels, tokenizer=tokenizer, label_all_tokens=True), batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=classlabel.num_classes)\n",
    "\n",
    "metric = load_metric('seqeval')\n",
    "word_ids = tokenized_datasets['valid']['word_ids']\n",
    "compute_metrics_ = partial(compute_metrics, metric=metric, label_list=classlabel.names, word_ids=word_ids)\n",
    "\n",
    "args = TrainingArguments(output_dir='test_training', num_train_epochs=2, \n",
    "                         learning_rate=2e-5, weight_decay=0.01,\n",
    "                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n",
    "                         save_strategy='epoch', save_total_limit=6)\n",
    "\n",
    "trainer = Trainer(model=model, args=args, \n",
    "                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n",
    "                  data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-justice",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 02:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.550555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.994983</td>\n",
       "      <td>6.556100</td>\n",
       "      <td>1.983000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.815100</td>\n",
       "      <td>0.337894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996912</td>\n",
       "      <td>6.605000</td>\n",
       "      <td>1.968000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.6522814631462097, metrics={'train_runtime': 161.3567, 'train_samples_per_second': 0.037, 'total_flos': 11030692654800.0, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 2319024128, 'train_mem_cpu_peaked_delta': 3832201216})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-dinner",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "drwxr-xr-x 2 root root 4096 Jun  3 23:25 checkpoint-3\r\n",
      "drwxr-xr-x 2 root root 4096 Jun  3 23:27 checkpoint-6\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lrt test_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-profit",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 02:02, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.247300</td>\n",
       "      <td>0.210579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996912</td>\n",
       "      <td>6.605600</td>\n",
       "      <td>1.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.149079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996912</td>\n",
       "      <td>6.389500</td>\n",
       "      <td>2.035000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=0.08589735627174377, metrics={'train_runtime': 151.8757, 'train_samples_per_second': 0.079, 'total_flos': 21942472878144.0, 'epoch': 4.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1893203968, 'train_mem_cpu_peaked_delta': 2585927680})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(output_dir='test_training', num_train_epochs=4, \n",
    "                         learning_rate=2e-5, weight_decay=0.01,\n",
    "                         per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                         evaluation_strategy='epoch', logging_steps=4, report_to='none', \n",
    "                         save_strategy='epoch', save_total_limit=6)\n",
    "\n",
    "trainer = Trainer(model=model, args=args, \n",
    "                  train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['valid'], \n",
    "                  data_collator=data_collator, tokenizer=tokenizer, \n",
    "                  compute_metrics=compute_metrics_)\n",
    "trainer.train(resume_from_checkpoint='/kaggle/working/test_training/checkpoint-6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-lottery",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14907871186733246,\n",
       " 'eval_precision': 0.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_accuracy': 0.996912389038981,\n",
       " 'eval_runtime': 6.311,\n",
       " 'eval_samples_per_second': 2.06,\n",
       " 'epoch': 4.0,\n",
       " 'eval_mem_cpu_alloc_delta': 28672,\n",
       " 'eval_mem_cpu_peaked_delta': 0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-hometown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NER inference\n",
    "\n",
    "**Turn off the Internet here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-investigator",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ner_inference_data(papers, sample_submission, classlabel=None, \n",
    "                           sentence_definition='sentence', max_length=64, overlap=20):\n",
    "    '''\n",
    "    Args:\n",
    "        papers (dict): Each list in this dictionary consists of the section of a paper.\n",
    "        sample_submission (pd.DataFrame): Competition 'sample_submission.csv'.\n",
    "    Returns:\n",
    "        test_rows (list): Each list in this list is of the form: \n",
    "             [('goat', 0), ('win', 0), ...] and represents a sentence.  \n",
    "        paper_length (list): Number of sentences in each paper.\n",
    "    '''\n",
    "    test_rows = [] \n",
    "    paper_length = [] \n",
    "\n",
    "    for paper_id in sample_submission['Id']:\n",
    "        paper = papers[paper_id]\n",
    "\n",
    "        sentences = extract_sentences(paper, sentence_definition=sentence_definition)\n",
    "        sentences = shorten_sentences(sentences, max_length=max_length, overlap=overlap)\n",
    "        sentences = [sentence for sentence in sentences if len(sentence) > 10] \n",
    "        sentences = [sentence for sentence in sentences \n",
    "                     if any(word in sentence.lower() for word in ['data', 'study'])]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_words = sentence.split()\n",
    "            dummy_tags = [classlabel.str2int('O')]*len(sentence_words)\n",
    "            test_rows.append(list(zip(sentence_words, dummy_tags)))\n",
    "\n",
    "        paper_length.append(len(sentences))\n",
    "\n",
    "    print(f'total number of \"sentences\": {len(test_rows)}')\n",
    "    return test_rows, paper_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-country",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of \"sentences\": 182\n",
      "[('Authors', 0), ('the', 0), ('Laboratory', 0), ('for', 0), ('Neuro', 0), ('Imaging', 0), ('at', 0), ('the', 0), ('University', 0), ('of', 0), ('Southern', 0), ('California', 0), ('Finally', 0), ('several', 0), ('publicly', 0), ('available', 0), ('datasets', 0), ('were', 0), ('included', 0), ('we', 0), ('kindly', 0), ('thank', 0), ('the', 0), ('investigative', 0), ('teams', 0), ('and', 0), ('staffs', 0), ('of', 0), ('the', 0), ('Pediatric', 0), ('Imaging', 0), ('Neurocognition', 0), ('and', 0), ('Genetics', 0), ('PING', 0), ('study', 0), ('the', 0), ('Alzheimer', 0), ('s', 0), ('Disease', 0), ('Neuroimaging', 0), ('Initiative', 0), ('ADNI', 0), ('project', 0), ('and', 0), ('the', 0), ('studies', 0), ('who', 0), ('made', 0), ('their', 0), ('data', 0), ('available', 0), ('in', 0), ('dbGaP', 0)]\n",
      "[16, 87, 40, 39]\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test', sample_submission.Id)\n",
    "classlabel = get_ner_classlabel()\n",
    "test_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel, \n",
    "                                                 sentence_definition='section', max_length=300, overlap=100)\n",
    "print(test_rows[1])\n",
    "print(paper_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-profit",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def ner_predict(pth=None, tokenizer=None, model=None, metric=None):\n",
    "    classlabel = get_ner_classlabel()\n",
    "    datasets = load_ner_datasets(data_files={'test':pth})\n",
    "    print('Tokenizing testset...')\n",
    "    tokenized_datasets = datasets.map(\n",
    "        partial(tokenize_and_align_labels,tokenizer=tokenizer, label_all_tokens=True), \n",
    "        batched=True) \n",
    "\n",
    "    print('Creating data collator...')\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    \n",
    "    print('Creating (dummy) training arguments...')\n",
    "    args = TrainingArguments(output_dir='test_ner', num_train_epochs=3, \n",
    "                             learning_rate=2e-5, weight_decay=0.01,\n",
    "                             per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                             evaluation_strategy='epoch', logging_steps=4, report_to='none', \n",
    "                             save_strategy='epoch', save_total_limit=6)\n",
    "\n",
    "    print('Creating trainer...')\n",
    "    word_ids = tokenized_datasets['test']['word_ids']\n",
    "    compute_metrics_ = partial(compute_metrics, metric=metric, label_list=classlabel.names, word_ids=word_ids)\n",
    "    trainer = Trainer(model=model, args=args, \n",
    "                      train_dataset=tokenized_datasets['test'], eval_dataset=tokenized_datasets['test'], \n",
    "                      data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics_)\n",
    "\n",
    "    print('Predicting on test samples...')\n",
    "    predictions, label_ids, _ = trainer.predict(tokenized_datasets['test'])\n",
    "    predictions = predictions.argmax(axis=2)\n",
    "    predictions = remove_nonoriginal_outputs(predictions, word_ids)\n",
    "    label_ids   = remove_nonoriginal_outputs(label_ids, word_ids)\n",
    "    return predictions, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-morgan",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This shows where to look for the cached metric `seqeval`.\n",
    "# metric = load_metric('/root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/seqeval.py')\n",
    "\n",
    "# Exporting the cached metric \n",
    "\n",
    "# %cd /root/.cache\n",
    "# ! zip -r huggingface_cache.zip huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/\n",
    "# %cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-monroe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'test_training/checkpoint-6/'\n",
    "\n",
    "tokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "# metric = load_metric('seqeval')\n",
    "metric = load_metric('/root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/ec5b7242a8c40468d189ca0b2b10612578dbcad311b2a134c99e3ded58a0d6e3/seqeval.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-relaxation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-7e607d25dbe8fe83/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf66b6cb456c4b309c5a427e504dec02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7e607d25dbe8fe83/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "Tokenizing testset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6b88f1d6304c3a86f80fe5b5d5cf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating data collator...\n",
      "Creating (dummy) training arguments...\n",
      "Creating trainer...\n",
      "Predicting on test samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: 11 11 11\n",
      "Sample 1: 27 27 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "samples = ['''Archaeologists estimate the carvings are between 4,000 and 5,000 years old''', \n",
    "           ('''I could see that I was looking at a deer stag upside down, '''\n",
    "            '''and as I continued looking around, more animals appeared on the rock,” he said.''')]\n",
    "test_rows = [list(zip(sample.split(), len(sample.split()) * [0])) for sample in samples]\n",
    "write_ner_json(test_rows, pth='test_ner.json')\n",
    "\n",
    "predictions, label_ids = ner_predict(pth='test_ner.json', tokenizer=tokenizer, model=model, metric=metric)\n",
    "for i in range(len(predictions)):\n",
    "    print(f'Sample {i}:', len(predictions[i]), len(label_ids[i]), len(samples[i].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-inquiry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_paper_dataset_labels(pth, paper_length, predictions):\n",
    "    '''\n",
    "    Args:\n",
    "        pth (Path, str): Path to json file containing NER data.  Each row is \n",
    "            of form: {'tokens': ['Studying', 'human'], 'ner_tags': [0, 0, ...]}.\n",
    "    \n",
    "    Returns:\n",
    "        paper_dataset_labels (list): Each element is a set consisting of labels predicted\n",
    "            by the model.\n",
    "    '''\n",
    "    test_sentences = [json.loads(sample)['tokens'] for sample in open(pth).readlines()]\n",
    "    \n",
    "    paper_dataset_labels = [] # store all dataset labels for each publication\n",
    "    for ipaper in range(len(paper_length)):\n",
    "        istart = sum(paper_length[:ipaper])\n",
    "        iend = istart + paper_length[ipaper]\n",
    "        \n",
    "        labels = set()\n",
    "        for sentence, pred in zip(test_sentences[istart:iend], predictions[istart:iend]):\n",
    "            curr_phrase = ''\n",
    "            for word, tag in zip(sentence, pred):\n",
    "                if tag == 'B': # start a new phrase\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "                    curr_phrase = word\n",
    "                elif tag == 'I' and curr_phrase: # continue the phrase\n",
    "                    curr_phrase += ' ' + word\n",
    "                else: # end last phrase (if any)\n",
    "                    if curr_phrase:\n",
    "                        labels.add(curr_phrase)\n",
    "                        curr_phrase = ''\n",
    "            # check if the label is the suffix of the sentence\n",
    "            if curr_phrase:\n",
    "                labels.add(curr_phrase)\n",
    "                curr_phrase = ''\n",
    "\n",
    "        # record dataset labels for this publication\n",
    "        paper_dataset_labels.append(labels)\n",
    "\n",
    "    return paper_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-worry",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Tigers EcoNAX dataset', 'present all the'}, {'WGS Equality Definitiveness Dataset'}]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['They do not present all the features', \n",
    "             'Despite the pretraining on the Tigers EcoNAX dataset',\n",
    "             'Weirdly there has been lots of studies based on WGS Equality Definitiveness Dataset']\n",
    "paper_length = [2, 1]\n",
    "test_rows = [[(word, 0) for word in sentence.split()] for sentence in sentences]\n",
    "predictions = [['O', 'O', 'O', 'B', 'I', 'I', 'O'],\n",
    "               ['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I'],\n",
    "               ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I']]\n",
    "for i, row in enumerate(test_rows):\n",
    "    assert len(row) == len(predictions[i])\n",
    "\n",
    "write_ner_json(test_rows, pth='test_ner.json')\n",
    "\n",
    "paper_dataset_labels = get_paper_dataset_labels('test_ner.json', paper_length, predictions)\n",
    "print(paper_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-smooth",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_dataset_labels(paper_dataset_labels):\n",
    "    '''\n",
    "    When several labels for a paper are too similar, keep just one of them.\n",
    "    '''\n",
    "    filtered_dataset_labels = []\n",
    "\n",
    "    for labels in paper_dataset_labels:\n",
    "        filtered = []\n",
    "\n",
    "        for label in sorted(labels, key=len):\n",
    "            label = clean_training_text(label, lower=True)\n",
    "            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n",
    "                filtered.append(label)\n",
    "\n",
    "        filtered_dataset_labels.append('|'.join(filtered))\n",
    "    return filtered_dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-diary",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moma artists|housing market|moma artists catalogue',\n",
       " 'deep sea rock salts|rhs fertiliser index']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_dataset_labels = [{'moma artists catalogue', 'moma artists', 'housing market'},\n",
    "                       {'rhs flowers fertiliser index', 'deep sea rock salts', 'rhs fertiliser index'}]\n",
    "\n",
    "filter_dataset_labels(paper_dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-aberdeen",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Literal matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-liverpool",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def create_knowledge_bank(pth):\n",
    "    '''\n",
    "    Args:\n",
    "        pth (str): Path to meta data like 'train.csv', which\n",
    "        needs to have columns: 'dataset_title', 'dataset_label', and 'cleaned_label'.\n",
    "        \n",
    "    Returns:\n",
    "        all_labels (set): All possible strings associated with a dataset from the meta data.\n",
    "    '''\n",
    "    df = load_train_meta(pth, group_id=False)\n",
    "    all_labels = set()\n",
    "    for label_1, label_2, label_3 in df[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n",
    "        all_labels.add(str(label_1).lower())\n",
    "        all_labels.add(str(label_2).lower())\n",
    "        all_labels.add(str(label_3).lower())\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-sussex",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "['2019 ncov complete genome sequences', '2019 ncov genome sequence', '2019 ncov genome sequences', '2019-ncov complete genome sequences', '2019-ncov genome sequence', '2019-ncov genome sequences', 'adni', 'advanced national seismic system (anss) comprehensive catalog (comcat)', 'advanced national seismic system anss comprehensive catalog comcat ', 'advanced national seismic system comprehensive catalog']\n"
     ]
    }
   ],
   "source": [
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "print(len(all_labels))\n",
    "print(sorted(all_labels)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-tennis",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def literal_match(paper, all_labels):\n",
    "    '''\n",
    "    Args:\n",
    "        paper ()\n",
    "    '''\n",
    "    text_1 = '. '.join(section['text'] for section in paper).lower()\n",
    "    text_2 = clean_training_text(text_1, lower=True, total_clean=True)\n",
    "    \n",
    "    labels = set()\n",
    "    for label in all_labels:\n",
    "        if label in text_1 or label in text_2:\n",
    "            labels.add(clean_training_text(label, lower=True, total_clean=True))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-equation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alzheimer s disease neuroimaging initiative adni|adni',\n",
       " 'trends in international mathematics and science study|nces common core of data|common core of data',\n",
       " 'sea lake and overland surges from hurricanes|slosh model|noaa storm surge inundation',\n",
       " 'rural urban continuum codes']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n",
    "\n",
    "pth = Path('/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv')\n",
    "all_labels = create_knowledge_bank(pth)\n",
    "\n",
    "literal_preds = []\n",
    "for paper_id in sample_submission.Id:\n",
    "    paper = papers[paper_id]\n",
    "    literal_preds.append('|'.join(literal_match(paper, all_labels)))\n",
    "    \n",
    "literal_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-preference",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overall prediction for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-swaziland",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def combine_matching_and_model(literal_preds, filtererd_dataset_labels):\n",
    "    '''\n",
    "    For a given sentence, if there's a literal match, use that as the final\n",
    "    prediction for the sentence.  If there isn't a literal match,\n",
    "    use what the model predicts.\n",
    "    '''\n",
    "    final_predictions = []\n",
    "    for literal_match, model_pred in zip(literal_preds, filtered_dataset_labels):\n",
    "        if literal_match:\n",
    "            final_predictions.append(literal_match)\n",
    "        else:\n",
    "            final_predictions.append(model_pred)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-leadership",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mongolian racing cars|reallife headphones',\n",
       " 'hifi dataset|headphones collection data']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_preds = ['mongolian racing cars|reallife headphones', '']\n",
    "filtered_dataset_labels = ['data|dataset', 'hifi dataset|headphones collection data']\n",
    "combine_matching_and_model(literal_preds, filtered_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-unemployment",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing NER inference data...\n",
      "total number of \"sentences\": 182\n",
      "Loading model, tokenizer, and metric...\n",
      "Predicting on each sentence...\n",
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-f677e8a91f0b8278/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42460686e1314bbd9cbbdaa875d59f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f677e8a91f0b8278/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "Tokenizing testset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1307e0b5f03d4da38c3cb742b44dde5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating data collator...\n",
      "Creating (dummy) training arguments...\n",
      "Creating trainer...\n",
      "Predicting on test samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 01:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predicted labels for each article...\n",
      "Keeping just one of labels that are too similar to each other...\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'test_training/checkpoint-6/'\n",
    "model_checkpoint = '/kaggle/input/showusdata-distilbert-base-cased-ner/ner_training_results/checkpoint-72822'\n",
    "\n",
    "print('Preparing NER inference data...')\n",
    "sample_submission = pd.read_csv('/kaggle/input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\n",
    "papers = load_papers('/kaggle/input/coleridgeinitiative-show-us-the-data/test/', sample_submission.Id)\n",
    "test_rows, paper_length = get_ner_inference_data(papers, sample_submission, classlabel=classlabel,\n",
    "                                                 sentence_definition='section', max_length=300, overlap=100)\n",
    "write_ner_json(test_rows, pth='test_ner.json')\n",
    "\n",
    "print('Loading model, tokenizer, and metric...')\n",
    "tokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "metric = load_metric('seqeval')\n",
    "\n",
    "print('Predicting on each sentence...')\n",
    "predictions, label_ids = ner_predict('test_ner.json', tokenizer=tokenizer, model=model, metric=metric)\n",
    "predictions = [[classlabel.int2str(p) for p in pred] for pred in predictions]\n",
    "label_ids   = [[classlabel.int2str(l) for l in label] for label in label_ids]\n",
    "\n",
    "print('Getting predicted labels for each article...')\n",
    "paper_dataset_labels = get_paper_dataset_labels('test_ner.json', paper_length, predictions)\n",
    "\n",
    "print('Keeping just one of labels that are too similar to each other...')\n",
    "filtered_dataset_labels = filter_dataset_labels(paper_dataset_labels)\n",
    "\n",
    "sample_submission['PredictionString'] = filtered_dataset_labels\n",
    "\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-criterion",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,PredictionString\r\n",
      "2100032a-7c33-4bff-97ef-690822c43466,alzheimer s disease neuroimaging initiative adni\r\n",
      "2f392438-e215-4169-bebf-21ac4ff253e1,trends in international mathematics and science study\r\n",
      "3f316b38-1a24-45a9-8d8c-4e05a42257c6,slosh model\r\n",
      "8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60,rural urban continuum codes\r\n"
     ]
    }
   ],
   "source": [
    "! cat submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-circulation",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-pickup",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-36caebbbae12bcf8/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bf28ef38a9437fbb3a66c57d3f587c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-36caebbbae12bcf8/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n",
      "Tokenizing testset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98db95220d574f858da93758cbe5ee50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating data collator...\n",
      "Creating (dummy) training arguments...\n",
      "Creating trainer...\n",
      "Predicting on test samples...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = '/kaggle/input/showusdata-distilbert-base-cased-ner/ner_training_results/checkpoint-72822'\n",
    "pth_train_json = '/kaggle/input/showus-data-ner-jsons/train_ner.json'\n",
    "pth_valid_json = '/kaggle/input/showus-data-ner-jsons/valid_ner.json'\n",
    "\n",
    "\n",
    "classlabel = get_ner_classlabel()\n",
    "\n",
    "ner_data_train = open(pth_train_json).readlines()[:10]\n",
    "ner_data_valid = open(pth_valid_json).readlines()[:10]\n",
    "ner_data_train = [json.loads(sample) for sample in ner_data_train]\n",
    "ner_data_valid = [json.loads(sample) for sample in ner_data_valid]\n",
    "ner_data_train = [list(zip(sample['tokens'], sample['ner_tags'])) for sample in ner_data_train]\n",
    "ner_data_valid = [list(zip(sample['tokens'], sample['ner_tags'])) for sample in ner_data_valid]\n",
    "write_ner_json(ner_data_train, pth='train_ner.json')\n",
    "write_ner_json(ner_data_valid, pth='valid_ner.json')\n",
    "\n",
    "tokenizer = create_tokenizer(model_checkpoint=model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=classlabel.num_classes)\n",
    "metric = load_metric('seqeval')\n",
    "\n",
    "predictions, label_ids = ner_predict(pth='valid_ner.json', tokenizer=tokenizer, model=model, metric=metric)\n",
    "predictions = [[classlabel.int2str(p) for p in pred] for pred in predictions]\n",
    "label_ids   = [[classlabel.int2str(l) for l in label] for label in label_ids]\n",
    "\n",
    "paper_dataset_labels = get_paper_dataset_labels('valid_ner.json', len(predictions) * [1], predictions)\n",
    "gt_paper_dataset_labels = get_paper_dataset_labels('valid_ner.json', len(label_ids) * [1], label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-danger",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:\n",
      "[set(), set(), {'ADNI'}, set(), set(), {'Beginning Postsecondary Students Longitudinal Study'}, {'IBTrACS'}, set(), set(), {'Program for the International Assessment of Adult Competencies'}]\n",
      "Ground-truth labels:\n",
      "[set(), set(), {'ADNI'}, set(), set(), {'Beginning Postsecondary Students Longitudinal Study'}, {'IBTrACS'}, set(), set(), set()]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted labels:')\n",
    "print(paper_dataset_labels)\n",
    "print('Ground-truth labels:')\n",
    "print(gt_paper_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-creature",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_': {'precision': 0.8, 'recall': 1.0, 'f1': 0.888888888888889, 'number': 4},\n",
       " 'overall_precision': 0.8,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 0.888888888888889,\n",
       " 'overall_accuracy': 0.9974747474747475}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predictions, references=label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-edition",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reference\n",
    "- https://www.kaggle.com/tungmphung/pytorch-bert-for-named-entity-recognition/notebook\n",
    "- https://www.kaggle.com/tungmphung/coleridge-matching-bert-ner/notebook\n",
    "- https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
    "- https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin\n",
    "- https://huggingface.co/docs/datasets/loading_metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-cheat",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
