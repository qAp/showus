# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/showus.ipynb (unless otherwise specified).

__all__ = ['load_train_meta', 'load_papers', 'load_sample_text', 'clean_training_text', 'shorten_sentences',
           'find_sublist', 'tag_sentence', 'get_ner_data', 'write_ner_json', 'kaggle_run_ner', 'create_knowledge_bank',
           'literal_match', 'get_ner_inference_data', 'kaggle_run_ner_predict', 'run_inference',
           'get_bert_dataset_labels', 'jaccard_similarity', 'filter_bert_labels', 'combine_matching_and_bert']

# Cell
import os, shutil
from pathlib import Path
import itertools
import re
import json
import random
import numpy as np
import pandas as pd
import torch
import transformers, seqeval
from tqdm import tqdm

# Cell
Path.ls = lambda pth: list(pth.iterdir())

# Cell
def load_train_meta(pth, group_id=True):
    df = pd.read_csv(pth)
    if group_id:
        df = df.groupby('Id').agg({'pub_title': 'first', 'dataset_title': '|'.join,
                                   'dataset_label': '|'.join, 'cleaned_label': '|'.join}).reset_index()
    return df

# Cell
def load_papers(dir_json, paper_ids):
    '''
    Load papers into a dictionary.

    `papers`:
        {''}
    '''

    papers = {}
    for paper_id in paper_ids:
        with open(f'{dir_json}/{paper_id}.json', 'r') as f:
            paper = json.load(f)
            papers[paper_id] = paper
    return papers

# Cell
def load_sample_text(jpth):
    sections = json.loads(jpth.read_text())
    text = '\n'.join(section['text'] for section in sections)
    return text

# Cell
def clean_training_text(txt, lower=False, total_clean=False):
    """
    similar to the default clean_text function but without lowercasing.
    """
    txt = str(txt).lower() if lower else str(txt)
    txt = re.sub('[^A-Za-z0-9]+', ' ', txt).strip()
    if total_clean:
        txt = re.sub(' +', ' ', txt)
    return txt

# Cell
def shorten_sentences(sentences, max_length=64, overlap=20):
    '''
    Args:
        sentences (list): List of sentences.
        max_length (int): Maximum number of words allowed for each sentence.
        overlap (int): If a sentence exceeds `max_length`, we split it to multiple sentences with
            this amount of overlapping.
    '''
    short_sentences = []
    for sentence in sentences:
        words = sentence.split()
        if len(words) > max_length:
            for p in range(0, len(words), max_length - overlap):
                short_sentences.append(' '.join(words[p:p+max_length]))
        else:
            short_sentences.append(sentence)
    return short_sentences

# Cell
def find_sublist(big_list, small_list):
    all_positions = []
    for i in range(len(big_list) - len(small_list) + 1):
        if small_list == big_list[i:i+len(small_list)]:
            all_positions.append(i)

    return all_positions

# Cell
def tag_sentence(sentence, labels):
    '''
    requirement: both sentence and labels are already cleaned
    '''
    sentence_words = sentence.split()

    if labels is not None and any(re.findall(f'\\b{label}\\b', sentence)
                                  for label in labels): # positive sample
        nes = ['O'] * len(sentence_words)
        for label in labels:
            label_words = label.split()

            all_pos = find_sublist(sentence_words, label_words)
            for pos in all_pos:
                nes[pos] = 'B'
                for i in range(pos+1, pos+len(label_words)):
                    nes[i] = 'I'

        return True, list(zip(sentence_words, nes))

    else: # negative sample
        nes = ['O'] * len(sentence_words)
        return False, list(zip(sentence_words, nes))

# Cell
def get_ner_data(papers, df=None, shuffle=True):
    '''
    Args:
        papers (dict): Like that returned by `load_papers`.
        df (pd.DataFrame): Competition's train.csv or a subset of it.
    '''
    cnt_pos, cnt_neg = 0, 0
    ner_data = []

    tqdm._instances.clear()
    pbar = tqdm(total=len(df))
    for i, id, dataset_label in df[['Id', 'dataset_label']].itertuples():
        paper = papers[id]

        labels = dataset_label.split('|')
        labels = [clean_training_text(label) for label in labels]

        sentences = set([clean_training_text(sentence) for section in paper
                     for sentence in section['text'].split('.')])
        sentences = shorten_sentences(sentences)
        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars

        # positive sample
        for sentence in sentences:
            is_positive, tags = tag_sentence(sentence, labels)
            if is_positive:
                cnt_pos += 1
                ner_data.append(tags)
            elif any(word in sentence.lower() for word in ['data', 'study']):
                ner_data.append(tags)
                cnt_neg += 1

        pbar.update(1)
        pbar.set_description(f"Training data size: {cnt_pos} positives + {cnt_neg} negatives")
#         print(f"\rProcessing paper {i:05d} / {len(df)}. Training data size: {cnt_pos} positives + {cnt_neg} negatives",
#               flush=True, end='')

    if shuffle:
        random.shuffle(ner_data)
    return ner_data

# Cell
def write_ner_json(ner_data, pth=Path('train_ner.json')):
    with open(pth, 'w') as f:
        for row in ner_data:
            words, nes = list(zip(*row))
            row_json = {'tokens' : words, 'tags' : nes}
            json.dump(row_json, f)
            f.write('\n')

# Cell
def kaggle_run_ner(model_name_or_path='bert-base-cased',
                   train_file='./train_ner.json', validation_file='./train_ner.json',
                   num_train_epochs=1, per_device_train_batch_size=8, per_device_eval_batch_size=8,
                   save_steps=15000, output_dir='./output', report_to='none', seed=123):
    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \
    --model_name_or_path {model_name_or_path} \
    --train_file {train_file} \
    --validation_file {validation_file} \
    --num_train_epochs {num_train_epochs} \
    --per_device_train_batch_size {per_device_train_batch_size} \
    --per_device_eval_batch_size {per_device_eval_batch_size} \
    --save_steps {save_steps} \
    --output_dir {output_dir} \
    --report_to {report_to} \
    --seed {seed} \
    --do_train

# Cell
def create_knowledge_bank(pth):
    '''
    Args:
        pth (str): Path to meta data like 'train.csv', which
        needs to have columns: 'dataset_title', 'dataset_label', and 'cleaned_label'.

    Returns:
        all_labels (set): All possible strings associated with a dataset from the meta data.
    '''
    df = load_train_meta(pth, group_id=False)
    all_labels = set()
    for label_1, label_2, label_3 in df[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):
        all_labels.add(str(label_1).lower())
        all_labels.add(str(label_2).lower())
        all_labels.add(str(label_3).lower())
    return all_labels

# Cell
def literal_match(paper, all_labels):
    '''
    Args:
        paper ()
    '''
    text_1 = '. '.join(section['text'] for section in paper).lower()
    text_2 = clean_training_text(text_1, lower=True, total_clean=True)

    labels = set()
    for label in all_labels:
        if label in text_1 or label in text_2:
            labels.add(clean_training_text(label, lower=True, total_clean=True))
    return labels

# Cell
def get_ner_inference_data(papers, sample_submission):
    '''
    Args:
        papers (dict): Each list in this dictionary consists of the section of a paper.
        sample_submission (pd.DataFrame): Competition 'sample_submission.csv'.
    Returns:
        test_rows (list): Each dict in this list is of the form:
            {'tokens': ['goat', 'win', ...], 'tags': ['O', 'O', ...]}
            and represents a sentence.
        paper_length (list): Number of sentences in each paper.
    '''
    test_rows = [] # test data in NER format
    paper_length = [] # store the number of sentences each paper has

    for paper_id in sample_submission['Id']:
        # load paper
        paper = papers[paper_id]

        # extract sentences
        sentences = [clean_training_text(sentence) for section in paper
                     for sentence in section['text'].split('.')
                    ]
        sentences = shorten_sentences(sentences) # make sentences short
        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars
        sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]

        # collect all sentences in json
        for sentence in sentences:
            sentence_words = sentence.split()
            dummy_tags = ['O']*len(sentence_words)
            test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})

        # track which sentence belongs to which data point
        paper_length.append(len(sentences))

    print(f'total number of sentences: {len(test_rows)}')
    return test_rows, paper_length

# Cell
def kaggle_run_ner_predict(model_name_or_path='/kaggle/input/coleridge-bert-models/output',
                           train_file='/kaggle/input/coleridge-bert-models/train_ner.json',
                           validation_file='/kaggle/input/coleridge-bert-models/train_ner.json',
                           test_file='./input_data/test_ner_input.json',
                           output_dir='./pred'):
    '''
    Args:
        test_file (Path, str): Path to json file in which each row represents an input
            sample to the model (representing a sentence in this context).  Each row
            is a dictionary of the form:
            {'tokens': ['hi', 'there', ...], 'tags': ['O', 'O', ...]}
        output_dir (Path, str): Path to the directory in which prediction results are saved.
    '''
    os.environ["MODEL_PATH"] = f"{model_name_or_path}"
    os.environ["TRAIN_FILE"] = f"{train_file}"
    os.environ["VALIDATION_FILE"] = f"{validation_file}"
    os.environ["TEST_FILE"] = f"{test_file}"
    os.environ["OUTPUT_DIR"] = f"{output_dir}"

    ! python /kaggle/input/kaggle-ner-utils/kaggle_run_ner.py \
    --model_name_or_path "$MODEL_PATH" \
    --validation_file "$VALIDATION_FILE" \
    --train_file "$TRAIN_FILE" \
    --test_file "$TEST_FILE" \
    --output_dir "$OUTPUT_DIR" \
    --report_to 'none' \
    --seed 123 \
    --do_predict

def run_inference(test_rows, predict_batch=64_000,
                  model_name_or_path='/kaggle/input/coleridge-bert-models/output',
                  train_file='/kaggle/input/coleridge-bert-models/train_ner.json',
                  validation_file='/kaggle/input/coleridge-bert-models/train_ner.json',
                  test_file='./input_data/test_ner_input.json',
                  output_dir='./pred'):
    '''
    '''
    test_file = Path(test_file)
    test_file.parent.mkdir(exist_ok=True, parents=True)

    bert_outputs = []
    for batch_begin in range(0, len(test_rows), predict_batch):
        # write data rows to input file
        with open(test_file, 'w') as f:
            for row in test_rows[batch_begin:batch_begin + predict_batch]:
                json.dump(row, f)
                f.write('\n')

        # remove output dir
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)

        # do predict
        kaggle_run_ner_predict(
            model_name_or_path=model_name_or_path,
            train_file=train_file, validation_file=validation_file, test_file=test_file,
            output_dir=output_dir)

        # read predictions
        with open(f'{output_dir}/test_predictions.txt') as f:
            this_preds = f.read().split('\n')[:-1]
            bert_outputs += [pred.split() for pred in this_preds]
    return bert_outputs

# Cell
def get_bert_dataset_labels(test_rows, paper_length, bert_outputs):
    '''
    Returns:
        bert_dataset_labels (list): Each element is a set consisting of labels predicted
            by the model.
    '''
    test_sentences = [row['tokens'] for row in test_rows]

    bert_dataset_labels = [] # store all dataset labels for each publication

    for length in paper_length:
        labels = set()
        for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):
            curr_phrase = ''
            for word, tag in zip(sentence, pred):
                if tag == 'B': # start a new phrase
                    if curr_phrase:
                        labels.add(curr_phrase)
                        curr_phrase = ''
                    curr_phrase = word
                elif tag == 'I' and curr_phrase: # continue the phrase
                    curr_phrase += ' ' + word
                else: # end last phrase (if any)
                    if curr_phrase:
                        labels.add(curr_phrase)
                        curr_phrase = ''
            # check if the label is the suffix of the sentence
            if curr_phrase:
                labels.add(curr_phrase)
                curr_phrase = ''

        # record dataset labels for this publication
        bert_dataset_labels.append(labels)

        del test_sentences[:length], bert_outputs[:length]

    return bert_dataset_labels

# Cell
def jaccard_similarity(s1, s2):
    l1 = set(s1.split(" "))
    l2 = set(s2.split(" "))
    intersection = len(list(l1.intersection(l2)))
    union = (len(l1) + len(l2)) - intersection
    return float(intersection) / union

# Cell
def filter_bert_labels(bert_dataset_labels):
    '''
    When several labels for a paper are too similar, keep just one of them.
    '''
    filtered_bert_labels = []

    for labels in bert_dataset_labels:
        filtered = []

        for label in sorted(labels, key=len):
            label = clean_training_text(label, lower=True)
            if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):
                filtered.append(label)

        filtered_bert_labels.append('|'.join(filtered))
    return filtered_bert_labels

# Cell
def combine_matching_and_bert(literal_preds, filtererd_bert_labels):
    final_predictions = []
    for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):
        if literal_match:
            final_predictions.append(literal_match)
        else:
            final_predictions.append(bert_pred)
    return final_predictions